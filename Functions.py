# -*- coding: utf-8 -*-
"""Functions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JSEnzCMmsd-d-e6aF_ksIt15VY10UunZ
"""

import torch
from torch_geometric.data import Data
from torch_geometric.datasets import Planetoid, Amazon, Coauthor
import numpy as np
from tqdm import tqdm
import random
import arc
from arc import models
from arc import methods
from arc import black_boxes
from arc import others
from arc import coverage

def load_regression_dataset(dataset_name, num_nodes):

    edge_list_file = f'{dataset_name}_edge_list.txt'
    features_file = f'{dataset_name}_features.txt'
    labels_file = f'{dataset_name}_labels.txt'
    with open(edge_list_file, 'r') as f:
        edges = [list(map(int, line.strip().split())) for line in f]

    edges_tensor = torch.tensor(edges, dtype=torch.long)

    edge_index = edges_tensor.t().contiguous()

    features = torch.tensor([list(map(float, line.strip().split())) for line in open(features_file)], dtype=torch.float)

    labels = torch.tensor([float(line.strip()) for line in open(labels_file)], dtype=torch.float)

    valid_edges_mask = (edge_index >= 0).all(dim=0) & (edge_index < num_nodes).all(dim=0)

    filtered_edge_index = edge_index[:, valid_edges_mask]

    data = Data(x=features, edge_index=filtered_edge_index, y=labels)

    return data



def load_classification_dataset(dataset_name):
    name = dataset_name.lower()
    if name in ['cora', 'citeseer', 'pubmed']:
        dataset = Planetoid(root=f'./data/{name}', name=name.capitalize())
    elif name in ['computers', 'photos']:
        dataset = Amazon(root=f'./data/{name}', name=name.capitalize())
    elif name in ['cs', 'physics']:
        dataset = Coauthor(root=f'./data/{name}', name=name.capitalize())
    else:
        raise ValueError(f"Dataset {name} is not recognized or not available in PyTorch Geometric.")

    return dataset

def cond_run(cond_conf,x_test,score_inv_fn,alpha):
        lbs_kernel = np.zeros((len(x_test),))
        ubs_kernel= np.zeros((len(x_test),))
        i = 0
        for x_t in tqdm(x_test):
            res = cond_conf.predict(1-alpha, x_t, score_inv_fn)
            lbs_kernel[i] = res[0]
            ubs_kernel[i] = res[1]
            i += 1
        return lbs_kernel,ubs_kernel

def indicator_matrix(scalar_values, disc):
        scalar_values = np.array(scalar_values)
        intervals = [(disc[i], disc[i + 1]) for i in range(len(disc) - 1)]
        matrix = np.zeros((scalar_values.shape[0], len(intervals)))
        for i in range(scalar_values.shape[0]):
          value = scalar_values[i,0]
          for j,(a,b) in enumerate(intervals):
            if (a<= value <b):
              matrix[i,j] = 1

        return matrix
#Into group according to the dimension of interest
def phi_fn_groups(x_12,dimension_interest,disc):
      if len(x_12.shape) == 2:
        a=x_12[:x_12.shape[0],dimension_interest]
        a = a.reshape(-1,1)
        return indicator_matrix(a, disc)
      else:
        a = x_12.reshape(-1,1)
        a = a[dimension_interest]
        a = a.reshape(-1,1)
        return indicator_matrix(a, disc)

def get_marginalcoverage_condi(ubs_1,lbs_1,x_test,y_test):
      j=0
      for i in range(len(x_test)):
        if((y_test[i]<=ubs_1[i]) & (y_test[i]>=lbs_1[i])):
          j+=1

      return j/len(x_test)

def get_marginalcoverage_split(x_test,y_test):
  j=0
  x_hat = get_xhat(x_test)
  for i in range(len(x_test)):
    if((y_test[i]>=x_hat[i]-q )& (y_test[i]<= x_hat[i]+q)):
      j+=1

  return j/len(x_test)

def evaluate_predictions(S, X, y):
    marg_coverage = np.mean([y[i] in S[i] for i in range(len(y))])
    wsc_coverage = coverage.wsc_unbiased(X, y, S)
    print('Marginal coverage:       {:2.3%}'.format(marg_coverage))
    print('WS conditional coverage: {:2.3%}'.format(wsc_coverage))


def get_xhat(x_vector,cali_mask,test_mask,dimension,fn):
         if(x_vector.shape[1]!=1):
            xhat_1= np.ones((x_vector.shape[0], 1))
            for j in range(x_vector.shape[0]):
              for i in range((cali_mask+test_mask).sum()):
                if(np.array_equal(x_vector[j],fn[i,0:dimension])):
                  xhat_1[j]=fn[i,dimension]
            return xhat_1
         else:
            for i in range((cali_mask+test_mask).sum()):
              if(np.array_equal(x_vector.squeeze(),fn[i,0:dimension])):
                return fn[i,dimension]
def get_prediction_sets(alpha,emb3_Cali,y_pred_Cali,emb3_Test,y_pred_Test,n,m,cali_mask,test_mask,y):
    n=cali_mask.sum()
    m=test_mask.sum()
    emb3_Cali = emb3_Cali.softmax(dim=-1)
    numpy_emb3_Cali = emb3_Cali.detach().cpu().numpy()
    numpy_y_pred_Cali = y_pred_Cali.detach().cpu().numpy()
    numpy_y_labels_Cali = y[cali_mask].detach().cpu().numpy()
    numpy_y_labels_Test = y[test_mask].detach().cpu().numpy()
    numpy_y_labels_column = np.array(numpy_y_labels_Cali).reshape(-1, 1)
    cal_smx_tensor = torch.FloatTensor(emb3_Cali)
    cal_smx = cal_smx_tensor.detach().cpu().numpy()
    cal_pi = cal_smx.argsort(1)[:, ::-1]
    cal_srt = np.take_along_axis(cal_smx,cal_pi,axis=1).cumsum(axis=1)
    cal_scores = np.take_along_axis(cal_srt, cal_pi.argsort(axis=1), axis=1)[
      range(n), numpy_y_labels_Cali
    ]
    qhat = np.quantile(
      cal_scores, np.ceil((n + 1) * (1 - alpha)) / n, interpolation="higher"
    )
    emb3_Test = emb3_Test.softmax(dim=-1)
    numpy_emb3_Test = emb3_Test.detach().cpu().numpy()
    test_smx = numpy_emb3_Test
    test_pi = test_smx.argsort(1)[:, ::-1]
    test_srt = np.take_along_axis(test_smx,test_pi,axis=1).cumsum(axis=1)

    test_scores = np.take_along_axis(test_srt, test_pi.argsort(axis=1), axis=1)[
      range(m), numpy_y_labels_Test
    ]


    prediction_sets = np.take_along_axis(test_srt <= qhat, test_pi.argsort(axis=1), axis=1)
    return prediction_sets

def phi_n(x_2):

  if(len(x_2.shape)==2):

          return x_2
  else:

        return  x_2.reshape(1,-1)
def get_srt(emb3):
  smx_tensor = torch.FloatTensor(emb3)
  smx = smx_tensor.detach().cpu().numpy()
  pi = smx.argsort(1)[:, ::-1]
  srt = np.take_along_axis(smx,pi,axis=1).cumsum(axis=1)
  return srt


def get_pi(emb3):
    smx_tensor = torch.FloatTensor(emb3)
    smx = smx_tensor.detach().cpu().numpy()
    pi = smx.argsort(1)[:, ::-1]
    return pi
def create_mask(size, indices):
    mask = torch.zeros(size, dtype=torch.bool)
    mask[indices] = True
    return mask
def split_data(num_nodes, train_percentage, test_percentage):
    all_node_indices = list(range(num_nodes))
    random.shuffle(all_node_indices)

    num_train_nodes = int(train_percentage * num_nodes)
    num_testandcali_nodes = num_nodes - num_train_nodes

    train_node_indices = all_node_indices[:num_train_nodes]
    testandcali_node_indices = all_node_indices[num_train_nodes:]

    test_node_indices = testandcali_node_indices[:int(test_percentage * num_testandcali_nodes)]
    cali_node_indices = testandcali_node_indices[int(test_percentage * num_testandcali_nodes):]

    train_mask = create_mask(num_nodes, train_node_indices)
    test_mask = create_mask(num_nodes, test_node_indices)
    cali_mask = create_mask(num_nodes, cali_node_indices)

    return train_mask, test_mask, cali_mask

def evaluate_predictions(S, X, y):
        marg_coverage = np.mean([y[i] in S[i] for i in range(len(y))])
        wsc_coverage = coverage.wsc_unbiased(X, y, S)
        print('Marginal coverage:       {:2.3%}'.format(marg_coverage))
        print('WS conditional coverage: {:2.3%}'.format(wsc_coverage))

def indices_of_nonzero_entries(binary_matrix):
    # Using list comprehension to find the indices of non-zero entries for each row
    indices_matrix = [np.flatnonzero(row) for row in binary_matrix]
    return indices_matrix