{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNcYT+XswOA24txNFEQaq+1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caltdreamer/GNN/blob/main/Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1NgqDuLuZ-K"
      },
      "outputs": [],
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install -q torch-geometric\n",
        "!pip install ogb\n",
        "!pip install umap-learn\n",
        "!pip install conditionalconformal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GraphSAGE\n",
        "from torch_geometric.data import NeighborSampler\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.datasets import NELL\n",
        "from torch_geometric.datasets import CitationFull\n",
        "from torch_geometric.datasets import CoraFull\n",
        "from torch_geometric.datasets import Coauthor\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.nn import BatchNorm\n",
        "from conditionalconformal.condconf import setup_cvx_problem_calib\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import cvxpy as cp\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os.path as osp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import collections\n",
        "import random\n",
        "import networkx as nx\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from conditionalconformal.synthetic_data import generate_cqr_data, indicator_matrix\n",
        "from conditionalconformal import CondConf\n",
        "\n",
        "from scipy.stats import norm\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "QAdn5TtHuoqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CoraFull(root='./data')\n",
        "#dataset = Planetoid(root='./data', name='CiteSeer',split = 'public')\n",
        "#dataset = Coauthor(root = './data',name ='CS' )\n",
        "data = dataset[0]\n",
        "print(data)\n",
        "print(data.x)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "x = data.x.to(device)\n",
        "y = data.y.squeeze().to(device)\n"
      ],
      "metadata": {
        "id": "3kptpNgIu6Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Randomly choose train and calitest set\n",
        "marginal_condi = []\n",
        "marginal_split = []\n",
        "prediction_split = []\n",
        "prediction_condi=[]\n",
        "test_index=[]\n",
        "train_index=[]\n",
        "calib_index=[]\n",
        "for c in range(10):\n",
        "      train_percentage = 0.6\n",
        "      test_percentage = 0.4\n",
        "\n",
        "\n",
        "\n",
        "      num_nodes = data.num_nodes\n",
        "      all_node_indices = list(range(num_nodes))\n",
        "      random.shuffle(all_node_indices)\n",
        "      num_train_nodes = int(train_percentage * num_nodes)\n",
        "      num_testandcali_nodes = num_nodes - num_train_nodes\n",
        "      train_node_indices = all_node_indices[:num_train_nodes]\n",
        "      testandcali_node_indices = all_node_indices[num_train_nodes:]\n",
        "\n",
        "      #print(testandcali_node_indices)\n",
        "      test_node_indices = testandcali_node_indices[:int(test_percentage * num_testandcali_nodes)]\n",
        "      cali_node_indices = testandcali_node_indices[int(test_percentage * num_testandcali_nodes):]\n",
        "\n",
        "      #print(test_node_indices)\n",
        "\n",
        "      train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "      train_mask[train_node_indices] = True\n",
        "      train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "      train_mask[train_node_indices] = True\n",
        "\n",
        "      testandcali_mask = torch.zeros(num_nodes,dtype=torch.bool)\n",
        "      testandcali_mask[testandcali_node_indices] = True\n",
        "\n",
        "      test_mask = torch.zeros(num_nodes,dtype=torch.bool)\n",
        "      test_mask[test_node_indices] = True\n",
        "\n",
        "\n",
        "      cali_mask = torch.zeros(num_nodes,dtype=torch.bool)\n",
        "      cali_mask[cali_node_indices] = True\n",
        "      #***************************************************\n",
        "      train_index.append(train_mask)\n",
        "      calib_index.append(cali_mask)\n",
        "      test_index.append(test_mask)\n",
        "      #***************************************************\n",
        "      train_loader = NeighborSampler(\n",
        "        data.edge_index, node_idx=train_mask,\n",
        "        sizes=[5,5,5], batch_size=32, shuffle=False,\n",
        "      )\n",
        "      #***************************************************\n",
        "      class SAGE(torch.nn.Module):\n",
        "        def __init__(self,in_channels,hidden_channels,out_channels,num_layers = 3):\n",
        "            super(SAGE,self).__init__()\n",
        "\n",
        "            self.numlayers = num_layers\n",
        "            self.convs = torch.nn.ModuleList()\n",
        "            self.convs.append(SAGEConv(in_channels,hidden_channels))\n",
        "            for i in range(num_layers - 2):\n",
        "                self.convs.append(SAGEConv(hidden_channels,hidden_channels))\n",
        "            self.convs.append(SAGEConv(hidden_channels,out_channels))\n",
        "        def reset_parameters(self):\n",
        "            for conv in self.convs:\n",
        "                conv.reset_parameters()\n",
        "        def forward(self,x,adjs):\n",
        "            for i ,(edge_index,_,size) in enumerate(adjs):\n",
        "                xs = []\n",
        "                x_target = x[:size[1]]#x should be a list prioritizing target nodes\n",
        "                x = self.convs[i]((x,x_target),edge_index)\n",
        "                if i != self.numlayers -1:\n",
        "                    x = F.relu(x)\n",
        "                    x = F.dropout(x,p=0.5,training=self.training)\n",
        "                xs.append(x)\n",
        "                if i == 0:\n",
        "                    x_all = torch.cat(xs, dim=0)\n",
        "                    layer_1_embeddings = x_all\n",
        "                elif i == 1:\n",
        "                    x_all = torch.cat(xs, dim=0)\n",
        "                    layer_2_embeddings = x_all\n",
        "                elif i == 2:\n",
        "                    x_all = torch.cat(xs, dim=0)\n",
        "                    layer_3_embeddings = x_all\n",
        "            return layer_1_embeddings, layer_2_embeddings, layer_3_embeddings\n",
        "\n",
        "        def inference(self, x,adjs):\n",
        "            for i ,(edge_index,_,size) in enumerate(adjs):\n",
        "                xs = []\n",
        "                x_target = x[:size[1]]#x should be a list prioritizing target nodes\n",
        "                x = self.convs[i]((x,x_target),edge_index)\n",
        "                if i != self.numlayers -1:\n",
        "                    x = F.relu(x)\n",
        "                    x = F.dropout(x,p=0.5,training=self.training)\n",
        "                xs.append(x)\n",
        "                if i == 0:\n",
        "                    x_all = torch.cat(xs, dim=0)\n",
        "                    layer_1_embeddings = x_all\n",
        "                elif i == 1:\n",
        "                    x_all = torch.cat(xs, dim=0)\n",
        "                    layer_2_embeddings = x_all\n",
        "                elif i == 2:\n",
        "                    x_all = torch.cat(xs, dim=0)\n",
        "                    layer_3_embeddings = x_all\n",
        "            return layer_1_embeddings, layer_2_embeddings, layer_3_embeddings\n",
        "      #***************************************************\n",
        "      model = SAGE(dataset.num_features, 128, dataset.num_classes, num_layers=3)\n",
        "\n",
        "      model.reset_parameters()\n",
        "      #We only input the initializing variables\n",
        "      model = model.to(device)\n",
        "      #***************************************************\n",
        "      def train(epoch):\n",
        "        model.train()\n",
        "        total_loss = total_correct =0\n",
        "\n",
        "        for batch_size,n_id,adjs in train_loader:\n",
        "            adjs = [adj.to(device) for adj in adjs]\n",
        "            optimizer.zero_grad()\n",
        "            l1_emb, l2_emb, l3_emb = model(x[n_id], adjs)\n",
        "            out = l3_emb.log_softmax(dim=-1)\n",
        "            loss = F.nll_loss(out,y[n_id[:batch_size]])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += float(loss)\n",
        "            total_correct += int(out.argmax(dim=-1).eq(y[n_id[:batch_size]]).sum())\n",
        "\n",
        "\n",
        "        loss = total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "        approx_acc = total_correct / int(train_mask.sum())\n",
        "\n",
        "        return loss, approx_acc\n",
        "      #***************************************************\n",
        "      optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)\n",
        "\n",
        "      for epoch in range(1,10):\n",
        "        loss,acc = train(epoch)\n",
        "        print(f'Epoch {epoch:02d}, Loss: {loss:.4f}, Approx. Train: {acc:.4f}')\n",
        "      #***************************************************\n",
        "\n",
        "      CaliGraph_loader = NeighborSampler(data.edge_index,node_idx = cali_mask\n",
        "      , sizes = [5,5,5],batch_size = 10000,shuffle=False)\n",
        "\n",
        "      TestGraph_loader  = NeighborSampler(data.edge_index,node_idx = test_mask\n",
        "      , sizes = [5,5,5],batch_size = 10000,shuffle=False)\n",
        "      #***************************************************\n",
        "      #function used to output final layer embedding and predicted labels\n",
        "      def inference_1(subgraph_loader):\n",
        "        model.eval()\n",
        "        total_loss = total_correct = 0\n",
        "        for batch_size,n_id,adjs in subgraph_loader:\n",
        "\n",
        "            emb1,emb2,emb3 = model(x[n_id],adjs)\n",
        "            out = emb3.log_softmax(dim=-1)\n",
        "            y_pred = out.argmax(dim=-1,keepdim =True)\n",
        "            c = x[n_id][:batch_size]\n",
        "            fn = torch.cat((c, emb3.softmax(dim=-1)), dim=1)\n",
        "            return fn,emb3, y_pred\n",
        "      #***************************************************\n",
        "      #Use this one\n",
        "\n",
        "      fn_cali,emb3_Cali,y_pred_Cali = inference_1(CaliGraph_loader)\n",
        "      emb3_Cali = emb3_Cali.softmax(dim=-1)\n",
        "      numpy_emb3_Cali = emb3_Cali.detach().cpu().numpy()\n",
        "      print(numpy_emb3_Cali[0:10])\n",
        "      #row_sums = np.sum(numpy_emb3_Cali, axis=1).reshape(-1, 1)\n",
        "      #emb3_Cali = numpy_emb3_Cali/row_sums\n",
        "\n",
        "\n",
        "      numpy_y_pred_Cali = y_pred_Cali.detach().cpu().numpy()\n",
        "      numpy_y_labels_Cali = y[cali_mask].detach().cpu().numpy()\n",
        "      numpy_y_labels_Test = y[test_mask].detach().cpu().numpy()\n",
        "      numpy_y_labels_column = np.array(numpy_y_labels_Cali).reshape(-1, 1)\n",
        "      #accuracy of our model for calibration nodes\n",
        "      acc = (numpy_y_pred_Cali == numpy_y_labels_column).sum()/cali_mask.sum()\n",
        "\n",
        "      alpha = 0.05\n",
        "      n=cali_mask.sum()\n",
        "      m=test_mask.sum()\n",
        "\n",
        "      #Using APS score function\n",
        "      cal_smx_tensor = torch.FloatTensor(emb3_Cali)\n",
        "      cal_smx = cal_smx_tensor.detach().cpu().numpy()\n",
        "      cal_pi = cal_smx.argsort(1)[:, ::-1]\n",
        "      cal_srt = np.take_along_axis(cal_smx,cal_pi,axis=1).cumsum(axis=1)\n",
        "      cal_scores = np.take_along_axis(cal_srt, cal_pi.argsort(axis=1), axis=1)[\n",
        "        range(n), numpy_y_labels_Cali\n",
        "      ]\n",
        "\n",
        "      qhat = np.quantile(\n",
        "        cal_scores, np.ceil((n + 1) * (1 - alpha)) / n, interpolation=\"higher\"\n",
        "      )\n",
        "\n",
        "      #Use this one\n",
        "\n",
        "\n",
        "      fn_test,emb3_Test,y_pred_Test = inference_1(TestGraph_loader)\n",
        "      emb3_Test = emb3_Test.softmax(dim=-1)\n",
        "      numpy_emb3_Test = emb3_Test.detach().cpu().numpy()\n",
        "      test_smx = numpy_emb3_Test\n",
        "      #row_sums = np.sum(numpy_emb3_Test, axis=1).reshape(-1, 1)\n",
        "      #test_smx = numpy_emb3_Test/row_sums\n",
        "      test_pi = test_smx.argsort(1)[:, ::-1]\n",
        "      test_srt = np.take_along_axis(test_smx,test_pi,axis=1).cumsum(axis=1)\n",
        "\n",
        "      test_scores = np.take_along_axis(test_srt, test_pi.argsort(axis=1), axis=1)[\n",
        "        range(m), numpy_y_labels_Test\n",
        "      ]\n",
        "\n",
        "\n",
        "      prediction_sets = np.take_along_axis(test_srt <= qhat, test_pi.argsort(axis=1), axis=1)\n",
        "\n",
        "      #***************************************************\n",
        "      print(prediction_sets)\n",
        "      empirical = prediction_sets[\n",
        "        np.arange(prediction_sets.shape[0]), numpy_y_labels_Test\n",
        "      ].mean()\n",
        "\n",
        "      print(f\"The empirical marginal coverage is: {empirical}\")\n",
        "      #***************************************************\n",
        "      #Now use CondConf to predict the interval\n",
        "      ### our conditional calibration method cover the test points\n",
        "      def computeCoverages(XCal, scoresCal, XTest, scoresTest, alpha):\n",
        "\n",
        "        coveragesCond = np.zeros(len(XTest))\n",
        "        for i in tqdm(range(len(XTest))):\n",
        "            prob = setup_cvx_problem_calib(1-alpha,None,\n",
        "                                                  np.concatenate((scoresCal,np.array([scoresTest[i]]))), np.vstack((XCal,XTest[i,:])),{})\n",
        "            prob.solve()\n",
        "            #print(prob.constraints[2].shape)\n",
        "            coveragesCond[i] = scoresTest[i] <= XTest[i,:]@prob.constraints[2].dual_value\n",
        "\n",
        "        return coveragesCond\n",
        "\n",
        "      coveragesCond = computeCoverages(numpy_emb3_Cali, cal_scores,numpy_emb3_Test, test_scores, alpha = 0.05)\n",
        "      print(f\"The conditional coverage is: {coveragesCond.mean()}\")\n",
        "\n",
        "      #print(f\"The conditional coverage is: {coveragesCond.mean()}\")\n",
        "      marginal_condi.append(coveragesCond.mean())\n",
        "      marginal_split.append(empirical)\n",
        "      prediction_split.append(prediction_sets)\n",
        "      prediction_condi.append(coveragesCond)\n",
        "      #***************************************************\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZckdHKpNu6XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def serialize_ndarrays(list_of_ndarrays):\n",
        "    return '\\n\\n'.join('\\n'.join(' '.join(str(cell) for cell in row) for row in arr) if arr.ndim > 1 else ' '.join(str(cell) for cell in arr) for arr in list_of_ndarrays)\n",
        "\n",
        "# Assuming your list of tensors is named list_of_tensors\n",
        "\n",
        "list_of_numpy_arrays_train = [tensor.numpy() for tensor in train_index if tensor.requires_grad == False]\n",
        "list_of_numpy_arrays_calib = [tensor.numpy() for tensor in calib_index if tensor.requires_grad == False]\n",
        "list_of_numpy_arrays_test = [tensor.numpy() for tensor in test_index if tensor.requires_grad == False]\n",
        "\n",
        "# Assuming your list of ndarray of booleans is named list_of_boolean_arrays\n",
        "train_index_int = [arr.astype(int) for arr in list_of_numpy_arrays_train]\n",
        "calib_index_int = [arr.astype(int) for arr in list_of_numpy_arrays_calib]\n",
        "test_index_int = [arr.astype(int) for arr in list_of_numpy_arrays_test]\n",
        "\n",
        "train_index_string = serialize_ndarrays(train_index)\n",
        "calib_index_string = serialize_ndarrays(calib_index)\n",
        "test_index_string = serialize_ndarrays(test_index)\n",
        "prediction_set_string =serialize_ndarrays(prediction_sets)\n",
        "prediction_condi_string = serialize_ndarrays(prediction_condi)\n",
        "#marginal_condi_string = serialize_ndarrays(marginal_condi)\n",
        "#marginal_split_string = serialize_ndarrays(marginal_split)\n",
        "\n",
        "file_path = '/content/drive/My Drive/Classification_Chicago_10runs.txt'\n",
        "separator = \"\\n\" + \"-\" * 50 + \"\\n\"  # Creates a line of dashes as a separator.\n",
        "\n",
        "with open(file_path, 'w') as f:\n",
        "     f.write(train_index_string + separator)\n",
        "     #f.write(calib_index_string + separator)\n",
        "    # f.write(test_index_string + separator)\n",
        "   # f.write(marginal_condi_string + separator)\n",
        "   # f.write(marginal_split_string + separator)\n",
        "     f.write(prediction_set_string + separator)\n",
        "     f.write(prediction_condi_string)\n"
      ],
      "metadata": {
        "id": "bp-HooeztU31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "numbers_string_condi = ' '.join(str(number) for number in marginal_condi)\n",
        "numbers_string_split = ' '.join(str(number) for number in marginal_split)\n",
        "with open(file_path, 'w') as f:\n",
        "     f.write(numbers_string_condi + separator)\n",
        "     #f.write(calib_index_string + separator)\n",
        "    # f.write(test_index_string + separator)\n",
        "   # f.write(marginal_condi_string + separator)\n",
        "   # f.write(marginal_split_string + separator)\n",
        "     f.write(numbers_string_split + separator)\n",
        "\n"
      ],
      "metadata": {
        "id": "_DI1CiucM7t4"
      },
      "execution_count": 48,
      "outputs": []
    }
  ]
}