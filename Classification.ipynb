{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN3nihv5JuvAfGQKCc3uTf8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caltdreamer/GNN/blob/main/Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1NgqDuLuZ-K"
      },
      "outputs": [],
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install -q torch-geometric\n",
        "!pip install ogb\n",
        "!pip install umap-learn\n",
        "!pip install conditionalconformal\n",
        "!git clone https://github.com/msesia/arc.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GraphSAGE\n",
        "from torch_geometric.data import NeighborSampler\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.datasets import NELL\n",
        "from torch_geometric.datasets import CitationFull\n",
        "from torch_geometric.datasets import CoraFull\n",
        "from torch_geometric.datasets import Coauthor\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.nn import BatchNorm\n",
        "from conditionalconformal.condconf import setup_cvx_problem_calib\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import cvxpy as cp\n",
        "import arc\n",
        "from arc import models\n",
        "from arc import methods\n",
        "from arc import black_boxes\n",
        "from arc import others\n",
        "from arc import coverage\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os.path as osp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import collections\n",
        "import random\n",
        "import networkx as nx\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import arc\n",
        "from arc import models\n",
        "from arc import methods\n",
        "from arc import black_boxes\n",
        "from arc import others\n",
        "from arc import coverage\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from conditionalconformal.synthetic_data import generate_cqr_data, indicator_matrix\n",
        "from conditionalconformal import CondConf\n",
        "\n",
        "from scipy.stats import norm\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "QAdn5TtHuoqU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset = CoraFull(root='./data')\n",
        "dataset = Planetoid(root='./data', name='Cora',split = 'public')\n",
        "#dataset = Coauthor(root = './data',name ='Physics' )\n",
        "data = dataset[0]\n",
        "original_x = data.x\n",
        "for data in dataset:\n",
        "    data.x = data.x / np.linalg.norm(data.x, axis=1, keepdims=True)\n",
        "\n",
        "# Now, the feature vectors in the 'dataset' have been normalized with respect L2 norm.\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "x = data.x.to(device)\n",
        "y = data.y.squeeze().to(device)\n"
      ],
      "metadata": {
        "id": "3kptpNgIu6Zf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.x)"
      ],
      "metadata": {
        "id": "S3gKxPOJfu43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Randomly choose train and calitest set\n",
        "marginal_condi = []\n",
        "marginal_split = []\n",
        "prediction_split = []\n",
        "prediction_condi=[]\n",
        "test_index=[]\n",
        "train_index=[]\n",
        "calib_index=[]\n",
        "train_mask_list=[]\n",
        "calib_mask_list=[]\n",
        "test_mask_list=[]\n",
        "\n",
        "train_percentage = 0.3\n",
        "test_percentage = 0.5\n",
        "\n",
        "for c in range(5):\n",
        "\n",
        "    num_nodes = data.num_nodes\n",
        "    all_node_indices = list(range(num_nodes))\n",
        "    random.shuffle(all_node_indices)\n",
        "    num_train_nodes = int(train_percentage * num_nodes)\n",
        "    num_testandcali_nodes = num_nodes - num_train_nodes\n",
        "    train_node_indices = all_node_indices[:num_train_nodes]\n",
        "    testandcali_node_indices = all_node_indices[num_train_nodes:]\n",
        "\n",
        "    #print(testandcali_node_indices)\n",
        "    test_node_indices = testandcali_node_indices[:int(test_percentage * num_testandcali_nodes)]\n",
        "    cali_node_indices = testandcali_node_indices[int(test_percentage * num_testandcali_nodes):]\n",
        "\n",
        "    #print(test_node_indices)\n",
        "\n",
        "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    train_mask[train_node_indices] = True\n",
        "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    train_mask[train_node_indices] = True\n",
        "\n",
        "    testandcali_mask = torch.zeros(num_nodes,dtype=torch.bool)\n",
        "    testandcali_mask[testandcali_node_indices] = True\n",
        "\n",
        "    test_mask = torch.zeros(num_nodes,dtype=torch.bool)\n",
        "    test_mask[test_node_indices] = True\n",
        "\n",
        "\n",
        "    cali_mask = torch.zeros(num_nodes,dtype=torch.bool)\n",
        "    cali_mask[cali_node_indices] = True\n",
        "    #***************************************************\n",
        "    train_index.append(train_mask)\n",
        "    calib_index.append(cali_mask)\n",
        "    test_index.append(test_mask)\n",
        "    #***************************************************\n",
        "    train_loader = NeighborSampler(\n",
        "      data.edge_index, node_idx=train_mask,\n",
        "      sizes=[2,2,2], batch_size=32, shuffle=False,\n",
        "    )\n",
        "    #***************************************************\n",
        "    class SAGE(torch.nn.Module):\n",
        "      def __init__(self,in_channels,hidden_channels,out_channels,num_layers = 3):\n",
        "          super(SAGE,self).__init__()\n",
        "\n",
        "          self.numlayers = num_layers\n",
        "          self.convs = torch.nn.ModuleList()\n",
        "          self.convs.append(SAGEConv(in_channels,hidden_channels))\n",
        "          for i in range(num_layers - 2):\n",
        "              self.convs.append(SAGEConv(hidden_channels,hidden_channels))\n",
        "          self.convs.append(SAGEConv(hidden_channels,out_channels))\n",
        "      def reset_parameters(self):\n",
        "          for conv in self.convs:\n",
        "              conv.reset_parameters()\n",
        "      def forward(self,x,adjs):\n",
        "          for i ,(edge_index,_,size) in enumerate(adjs):\n",
        "              xs = []\n",
        "              x_target = x[:size[1]]#x should be a list prioritizing target nodes\n",
        "              x = self.convs[i]((x,x_target),edge_index)\n",
        "              if i != self.numlayers -1:\n",
        "                  x = F.relu(x)\n",
        "                  x = F.dropout(x,p=0.5,training=self.training)\n",
        "              xs.append(x)\n",
        "              if i == 0:\n",
        "                  x_all = torch.cat(xs, dim=0)\n",
        "                  layer_1_embeddings = x_all\n",
        "              elif i == 1:\n",
        "                  x_all = torch.cat(xs, dim=0)\n",
        "                  layer_2_embeddings = x_all\n",
        "              elif i == 2:\n",
        "                  x_all = torch.cat(xs, dim=0)\n",
        "                  layer_3_embeddings = x_all\n",
        "          return layer_1_embeddings, layer_2_embeddings, layer_3_embeddings\n",
        "    \"\"\"(In situation when we aim to use all neighborhood information)\n",
        "      def inference(self, x,adjs):\n",
        "          for i ,(edge_index,_,size) in enumerate(adjs):\n",
        "              xs = []\n",
        "              x_target = x[:size[1]]#x should be a list prioritizing target nodes\n",
        "              x = self.convs[i]((x,x_target),edge_index)\n",
        "              if i != self.numlayers -1:\n",
        "                  x = F.relu(x)\n",
        "                  x = F.dropout(x,p=0.5,training=self.training)\n",
        "              xs.append(x)\n",
        "              if i == 0:\n",
        "                  x_all = torch.cat(xs, dim=0)\n",
        "                  layer_1_embeddings = x_all\n",
        "              elif i == 1:\n",
        "                  x_all = torch.cat(xs, dim=0)\n",
        "                  layer_2_embeddings = x_all\n",
        "              elif i == 2:\n",
        "                  x_all = torch.cat(xs, dim=0)\n",
        "                  layer_3_embeddings = x_all\n",
        "          return layer_1_embeddings, layer_2_embeddings, layer_3_embeddings\n",
        "    \"\"\"\n",
        "    #***************************************************\n",
        "    model = SAGE(dataset.num_features, 128, dataset.num_classes, num_layers=3)\n",
        "\n",
        "    model.reset_parameters()\n",
        "    #We only input the initializing variables\n",
        "    model = model.to(device)\n",
        "    #***************************************************\n",
        "    def train(epoch):\n",
        "      model.train()\n",
        "      total_loss = total_correct =0\n",
        "\n",
        "      for batch_size,n_id,adjs in train_loader:\n",
        "          adjs = [adj.to(device) for adj in adjs]\n",
        "          optimizer.zero_grad()\n",
        "          l1_emb, l2_emb, l3_emb = model(x[n_id], adjs)\n",
        "          out = l3_emb.log_softmax(dim=-1)\n",
        "          loss = F.nll_loss(out,y[n_id[:batch_size]])\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          total_loss += float(loss)\n",
        "          total_correct += int(out.argmax(dim=-1).eq(y[n_id[:batch_size]]).sum())\n",
        "\n",
        "\n",
        "      loss = total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "      approx_acc = total_correct / int(train_mask.sum())\n",
        "\n",
        "      return loss, approx_acc\n",
        "    #***************************************************\n",
        "    optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)\n",
        "\n",
        "    for epoch in range(1,20):\n",
        "      loss,acc = train(epoch)\n",
        "      print(f'Epoch {epoch:02d}, Loss: {loss:.4f}, Approx. Train: {acc:.4f}')\n",
        "    #***************************************************\n",
        "\n",
        "    CaliGraph_loader = NeighborSampler(data.edge_index,node_idx = cali_mask\n",
        "    , sizes = [2,2,2],batch_size = 100000,shuffle=False)\n",
        "\n",
        "    TestGraph_loader  = NeighborSampler(data.edge_index,node_idx = test_mask\n",
        "    , sizes = [2,2,2],batch_size = 100000,shuffle=False)\n",
        "    #***************************************************\n",
        "    #function used to output final layer embedding and predicted labels\n",
        "    def inference_1(subgraph_loader):\n",
        "      model.eval()\n",
        "      total_loss = total_correct = 0\n",
        "      for batch_size,n_id,adjs in subgraph_loader:\n",
        "\n",
        "          emb1,emb2,emb3 = model(x[n_id],adjs)\n",
        "          out = emb3.log_softmax(dim=-1)\n",
        "          y_pred = out.argmax(dim=-1,keepdim =True)\n",
        "          c = x[n_id][:batch_size]\n",
        "          fn = torch.cat((c, emb3.softmax(dim=-1)), dim=1)\n",
        "          return fn,emb3, y_pred\n",
        "    #***************************************************\n",
        "    #Use this one\n",
        "\n",
        "    fn_cali,emb3_Cali,y_pred_Cali = inference_1(CaliGraph_loader)\n",
        "    emb3_Cali = emb3_Cali.softmax(dim=-1)\n",
        "    numpy_emb3_Cali = emb3_Cali.detach().cpu().numpy()\n",
        "    #print(numpy_emb3_Cali[0:10])\n",
        "    #row_sums = np.sum(numpy_emb3_Cali, axis=1).reshape(-1, 1)\n",
        "    #emb3_Cali = numpy_emb3_Cali/row_sums\n",
        "\n",
        "\n",
        "    numpy_y_pred_Cali = y_pred_Cali.detach().cpu().numpy()\n",
        "    numpy_y_labels_Cali = y[cali_mask].detach().cpu().numpy()\n",
        "    numpy_y_labels_Test = y[test_mask].detach().cpu().numpy()\n",
        "    numpy_y_labels_column = np.array(numpy_y_labels_Cali).reshape(-1, 1)\n",
        "    #accuracy of our model for calibration nodes\n",
        "    #acc = (numpy_y_pred_Cali == numpy_y_labels_column).sum()/cali_mask.sum()\n",
        "\n",
        "    alpha = 0.05\n",
        "    n=cali_mask.sum()\n",
        "    m=test_mask.sum()\n",
        "\n",
        "    #Using APS score function\n",
        "    cal_smx_tensor = torch.FloatTensor(emb3_Cali)\n",
        "    cal_smx = cal_smx_tensor.detach().cpu().numpy()\n",
        "    cal_pi = cal_smx.argsort(1)[:, ::-1]\n",
        "    cal_srt = np.take_along_axis(cal_smx,cal_pi,axis=1).cumsum(axis=1)\n",
        "    cal_scores = np.take_along_axis(cal_srt, cal_pi.argsort(axis=1), axis=1)[\n",
        "      range(n), numpy_y_labels_Cali\n",
        "    ]\n",
        "\n",
        "    qhat = np.quantile(\n",
        "      cal_scores, np.ceil((n + 1) * (1 - alpha)) / n, interpolation=\"higher\"\n",
        "    )\n",
        "\n",
        "\n",
        "    fn_test,emb3_Test,y_pred_Test = inference_1(TestGraph_loader)\n",
        "    emb3_Test = emb3_Test.softmax(dim=-1)\n",
        "    numpy_emb3_Test = emb3_Test.detach().cpu().numpy()\n",
        "    test_smx = numpy_emb3_Test\n",
        "    test_pi = test_smx.argsort(1)[:, ::-1]\n",
        "    test_srt = np.take_along_axis(test_smx,test_pi,axis=1).cumsum(axis=1)\n",
        "\n",
        "    test_scores = np.take_along_axis(test_srt, test_pi.argsort(axis=1), axis=1)[\n",
        "      range(m), numpy_y_labels_Test\n",
        "    ]\n",
        "\n",
        "\n",
        "    prediction_sets = np.take_along_axis(test_srt <= qhat, test_pi.argsort(axis=1), axis=1)\n",
        "\n",
        "    #***************************************************\n",
        "    #print(prediction_sets)\n",
        "    empirical = prediction_sets[\n",
        "      np.arange(prediction_sets.shape[0]), numpy_y_labels_Test\n",
        "    ].mean()\n",
        "\n",
        "    print(f\"The empirical marginal coverage is: {empirical}\")\n",
        "    #***************************************************\n",
        "    #Now use CondConf to predict the interval\n",
        "    # our conditional calibration method cover the test points\n",
        "    #Important:we intentionally put the feature representation as the predicted probability to get the balanced coevrgae over every categories\n",
        "    #To implement the alogorithm ,we should bear in mind the characteristics we care, in this case our goal is to reweight acoording to the predicted probabilities\n",
        "    \"\"\"\n",
        "    #This auxiliary function aims to quickly calculate the covergae rate but not the specific prediction sets\n",
        "\n",
        "    def computeCoverages(XCal, scoresCal, XTest, scoresTest, alpha):\n",
        "\n",
        "      coveragesCond = np.zeros(len(XTest))\n",
        "      for i in tqdm(range(len(XTest))):\n",
        "          prob = setup_cvx_problem_calib(1-alpha,None,\n",
        "                                                np.concatenate((scoresCal,np.array([scoresTest[i]]))), np.vstack((XCal,XTest[i,:])),{})\n",
        "          prob.solve()\n",
        "          #print(prob.constraints[2].shape)\n",
        "          coveragesCond[i] = scoresTest[i] <= XTest[i,:]@prob.constraints[2].dual_value\n",
        "\n",
        "      return coveragesCond\n",
        "\n",
        "    coveragesCond = computeCoverages(numpy_emb3_Cali, cal_scores,numpy_emb3_Test, test_scores, alpha = 0.05)\n",
        "    \"\"\"\n",
        "\n",
        "    #***************************************************\n",
        "    #This cell defines fucntion that computes the ubs and lbs using the condconf by implementing the predicted probability feature represenation\n",
        "    #fn has shape of [n_calib+n_test,data.num_features+1]\n",
        "    #From Now the feature vector becomes the predicted probabilities\n",
        "    x_caliandtest = np.vstack((numpy_emb3_Cali,numpy_emb3_Test))\n",
        "    #x_caliandtest = np.hstack((x_caliandtest, np.ones((x_caliandtest.shape[0], 1))))\n",
        "    fn = np.vstack((fn_cali.detach().cpu().numpy(),fn_test.detach().cpu().numpy()))\n",
        "    num_features = data.num_features\n",
        "    n_calib = int(cali_mask.sum())\n",
        "    n_test = int(test_mask.sum())\n",
        "    #matrix_1 represents class matrix\n",
        "\n",
        "    num_classes = dataset.num_classes\n",
        "    matrix_1 = np.zeros((n_calib+n_test, num_classes),dtype=int)\n",
        "\n",
        "    for i in range(dataset.num_classes):\n",
        "        matrix_1[:, i] = i\n",
        "\n",
        "    caliandtest_srt = np.vstack((cal_srt, test_srt))\n",
        "    caliandtest_pi = np.vstack((cal_pi,test_pi))\n",
        "\n",
        "    #for each column we compute S(Xi,y) for all possible y categories(calibration and test)*****because we will use these in optimization problem\n",
        "\n",
        "    test_scores_foreachclass=np.zeros((n_calib+n_test, dataset.num_classes))\n",
        "\n",
        "    for i in range(dataset.num_classes):\n",
        "        test_scores = (np.take_along_axis(caliandtest_srt , caliandtest_pi.argsort(axis=1), axis=1)[\n",
        "        range(n_calib+n_test), matrix_1[:,i]])\n",
        "        test_scores_foreachclass[:,i]=test_scores\n",
        "\n",
        "    x_calib = x_caliandtest[:n_calib]\n",
        "    y_calib = y[cali_mask].detach().cpu().numpy()\n",
        "    x_test = x_caliandtest[-n_test:]\n",
        "    y_test = y[test_mask].detach().cpu().numpy()\n",
        "    alpha = 0.05\n",
        "    #This function input packs of vectors and output the score of that x\n",
        "    #I'll Rewrite it tomorrow for higher efficiency\n",
        "\n",
        "    def score_fn(x_1,y):\n",
        "      if(x_1.shape[1]!=1):\n",
        "        #print('!=1')\n",
        "        scores = np.ones((x_1.shape[0],1))\n",
        "        for i in range(x_1.shape[0]):\n",
        "          scores[i]=test_scores_foreachclass[i,y[i]]\n",
        "        return scores.squeeze()\n",
        "      else:\n",
        "        #print('=1')\n",
        "        for i in range(n_calib+n_test):\n",
        "          if(np.array_equal(x_1,x_caliandtest[i])):\n",
        "            return test_scores_foreachclass[i,y]\n",
        "\n",
        "    def phi_n(x_2):\n",
        "\n",
        "      if(len(x_2.shape)==2):\n",
        "\n",
        "              return x_2\n",
        "      else:\n",
        "\n",
        "            return  x_2.reshape(1,dataset.num_classes)\n",
        "\n",
        "\n",
        "    #given x_3 and s, output the prediction interval\n",
        "    def score_inv_fn(s,x_3):\n",
        "\n",
        "      for i in range(n_calib+n_test):\n",
        "\n",
        "        if(np.array_equal(x_3.squeeze(),x_caliandtest[i])):\n",
        "\n",
        "          return np.take_along_axis(caliandtest_srt[i].reshape(1,dataset.num_classes) <= s,caliandtest_pi[i].reshape(1,dataset.num_classes).argsort(axis=1),axis=1)\n",
        "    def check_coverage(prediction_set):\n",
        "          j=0\n",
        "          for i in range(n_test):\n",
        "            a = prediction_set[i].squeeze()\n",
        "            if(a[y[test_mask][i]]== True):\n",
        "              j=j+1\n",
        "          return j/n_test\n",
        "\n",
        "\n",
        "\n",
        "    infinite_params = {}\n",
        "    cond_conf = CondConf(score_fn, phi_n, infinite_params)\n",
        "    cond_conf.setup_problem(x_calib, y_calib)\n",
        "\n",
        "    prediction_sets_test=[]\n",
        "    for x_t in tqdm(x_test):\n",
        "\n",
        "        res = cond_conf.predict(1-alpha, x_t, score_inv_fn)\n",
        "        prediction_sets_test.append(res)\n",
        "    print(f\"The empirical marginal coverage using condition calibration is: {check_coverage(prediction_sets_test)}\")\n",
        "\n",
        "    prediction_sets_test = np.vstack(prediction_sets_test)\n",
        "    def evaluate_predictions(S, X, y):\n",
        "        marg_coverage = np.mean([y[i] in S[i] for i in range(len(y))])\n",
        "        wsc_coverage = coverage.wsc_unbiased(X, y, S)\n",
        "        print('Marginal coverage:       {:2.3%}'.format(marg_coverage))\n",
        "        print('WS conditional coverage: {:2.3%}'.format(wsc_coverage))\n",
        "    def indices_of_nonzero_entries(binary_matrix):\n",
        "        # Using list comprehension to find the indices of non-zero entries for each row\n",
        "        indices_matrix = [np.flatnonzero(row) for row in binary_matrix]\n",
        "        return indices_matrix\n",
        "\n",
        "    int_matrix =prediction_sets.astype(int)\n",
        "    indices_matrix = indices_of_nonzero_entries(int_matrix)\n",
        "    print(f\"The empirical marginal and WS conditional coverage using split conformal is: \")\n",
        "    print(evaluate_predictions(indices_matrix,original_x[test_mask].detach().cpu().numpy(),y[test_mask].detach().cpu().numpy()))\n",
        "    int_matrix =prediction_sets_test.astype(int)\n",
        "    indices_matrix = indices_of_nonzero_entries(int_matrix)\n",
        "    print(f\"The empirical marginal and WS conditional coverage using conditional calibration is: \")\n",
        "    print(evaluate_predictions(indices_matrix,original_x[test_mask].detach().cpu().numpy(),y[test_mask].detach().cpu().numpy()))\n",
        "\n",
        "    marginal_split.append(empirical)\n",
        "    marginal_split.append(check_coverage(prediction_sets_test))\n",
        "    prediction_split.append(prediction_sets)\n",
        "    prediction_condi.append(prediction_sets_test)\n",
        "    test_mask_list.append(test_mask.numpy().astype(int))\n"
      ],
      "metadata": {
        "id": "ZckdHKpNu6XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(empirical)\n",
        "print(check_coverage(prediction_sets_test))\n",
        "print(prediction_sets_test)"
      ],
      "metadata": {
        "id": "qDGCFyUGB0d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Store our data\n",
        "def serialize_ndarrays(list_of_ndarrays):\n",
        "    return '\\n\\n'.join('\\n'.join(' '.join(str(cell) for cell in row) for row in arr) if arr.ndim > 1 else ' '.join(str(cell) for cell in arr) for arr in list_of_ndarrays)\n",
        "\n",
        "\n",
        "\n",
        "list_of_numpy_arrays_train = [tensor.numpy() for tensor in train_index if tensor.requires_grad == False]\n",
        "list_of_numpy_arrays_calib = [tensor.numpy() for tensor in calib_index if tensor.requires_grad == False]\n",
        "list_of_numpy_arrays_test = [tensor.numpy() for tensor in test_index if tensor.requires_grad == False]\n",
        "test_index_int = [arr.astype(int) for arr in list_of_numpy_arrays_test]\n",
        "test_index_string = serialize_ndarrays(test_index)\n",
        "prediction_set_string =serialize_ndarrays(prediction_sets)\n",
        "prediction_condi_string = serialize_ndarrays(prediction_condi)\n",
        "marginal_condi_string = serialize_ndarrays(marginal_condi)\n",
        "marginal_split_string = serialize_ndarrays(marginal_split)\n",
        "\n",
        "file_path = '/content/drive/My Drive/Classification_Chicago_10runs.txt'\n",
        "separator = \"\\n\" + \"-\" * 50 + \"\\n\"  # Creates a line of dashes as a separator.\n",
        "\n",
        "with open(file_path, 'w') as f:\n",
        "     f.write(test_index_string + separator)\n",
        "     #f.write(calib_index_string + separator)\n",
        "    # f.write(test_index_string + separator)\n",
        "   # f.write(marginal_condi_string + separator)\n",
        "   # f.write(marginal_split_string + separator)\n",
        "     f.write(prediction_set_string + separator)\n",
        "     f.write(prediction_condi_string)\n",
        "\n"
      ],
      "metadata": {
        "id": "bp-HooeztU31"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}