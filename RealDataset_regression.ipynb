{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caltdreamer/GNN/blob/main/RealDataset_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcX285LF1Ngv"
      },
      "outputs": [],
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install -q torch-geometric\n",
        "!pip install umap-learn\n",
        "!git clone https://github.com/msesia/arc.git\n",
        "!pip install conditionalconformal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5QiD-WUE1sVs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GraphSAGE\n",
        "from torch_geometric.data import NeighborSampler\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.datasets import NELL\n",
        "from torch_geometric.datasets import CitationFull\n",
        "from torch_geometric.datasets import CoraFull\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.data import Data\n",
        "from tqdm import tqdm\n",
        "from torch_geometric.nn import SAGEConv, BatchNorm\n",
        "import arc\n",
        "from arc import models\n",
        "from arc import methods\n",
        "from arc import black_boxes\n",
        "from arc import others\n",
        "from arc import coverage\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os.path as osp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import collections\n",
        "import random\n",
        "import networkx as nx\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "from numpy.ma.core import maximum\n",
        "from conditionalconformal.synthetic_data import generate_cqr_data, indicator_matrix\n",
        "from conditionalconformal import CondConf\n",
        "\n",
        "from scipy.stats import norm\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import gaussian_kde\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "kTzaCTmx9hO3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Read the file\n",
        "with open('ChicagoSketch_edge_list.txt', 'r') as f:\n",
        "    edges = [list(map(int, line.strip().split())) for line in f]\n",
        "edges_tensor = torch.tensor(edges, dtype=torch.long)\n",
        "edge_index = edges_tensor.t().contiguous()\n",
        "features = torch.tensor([list(map(float, line.strip().split())) for line in open('ChicagoSketch_features.txt')], dtype=torch.float)\n",
        "labels = torch.tensor([float(line.strip()) for line in open('ChicagoSketch_labels.txt')], dtype=torch.float)\n",
        "num_nodes = 2176\n",
        "valid_edges_mask = (edge_index >= 0).all(dim=0) & (edge_index < num_nodes).all(dim=0)\n",
        "filtered_edge_index = edge_index[:, valid_edges_mask]\n",
        "\n",
        "data = Data(x=features, edge_index=filtered_edge_index, y=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "5BzXJhtqJeDS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3ad7498-c9e7-4fb8-c8ae-ca583e8e2145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[2176, 4], edge_index=[2, 15099], y=[2176])\n"
          ]
        }
      ],
      "source": [
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "0XDhvq1R3PS8"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "x = data.x.to(device)\n",
        "y = data.y.squeeze().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using PCA and then kernel density estimation\n",
        "data_4d = data.x\n",
        "scaler = StandardScaler()\n",
        "data_4d_std = scaler.fit_transform(data_4d)\n",
        "pca = PCA(n_components=1)\n",
        "data_reduced = pca.fit_transform(data_4d_std)\n",
        "kde = gaussian_kde(data_reduced.T)\n",
        "\n",
        "x_d = np.linspace(data_reduced.min(), data_reduced.max(), 1000)\n",
        "kde_values = kde(x_d)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(x_d, kde_values, label='KDE')\n",
        "plt.scatter(data_reduced, np.full_like(data_reduced, -0.01), alpha=0.6, color='red', label='Data points')  # Overlay the data points\n",
        "plt.title('KDE of the First Principal Component')\n",
        "plt.legend()\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Density')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "MDmzNixEmZCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZnPeom21sOc"
      },
      "outputs": [],
      "source": [
        "#Plot our feature versus the label\n",
        "x_first_dim = features[:, ].numpy()\n",
        "y_numpy = labels.numpy()\n",
        "plt.scatter(x_first_dim[:,0], y_numpy, alpha=0.6, edgecolors=\"w\", linewidth=0.5)\n",
        "plt.xlabel(\"Some Dimension of X\")\n",
        "plt.ylabel(\"Label (Y)\")\n",
        "plt.title(\"First Dimension of X vs. Label Y\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ibCIOu12_UJ"
      },
      "outputs": [],
      "source": [
        "marginal_condi = []\n",
        "marginal_split = []\n",
        "ubs_condi = []\n",
        "lbs_condi = []\n",
        "ubs_split =[]\n",
        "lbs_split = []\n",
        "q = []\n",
        "ubs_condi_kernel=[]\n",
        "lbs_condi_kernel=[]\n",
        "test_mask_lists = []\n",
        "\n",
        "\n",
        "train_percentage = 0.5\n",
        "test_percentage = 0.5\n",
        "#Run the experiment\n",
        "for c in range(5):\n",
        "    #Randomly choose train and calitest set\n",
        "    marginal_condi = []\n",
        "    marginal_split = []\n",
        "    ubs_condi = []\n",
        "    lbs_condi = []\n",
        "    ubs_split =[]\n",
        "    lbs_split = []\n",
        "    q = []\n",
        "    ubs_condi_shifts=[]\n",
        "    lbs_condi_shifts=[]\n",
        "    test_mask_lists = []\n",
        "\n",
        "    num_nodes = data.num_nodes\n",
        "    all_node_indices = list(range(num_nodes))\n",
        "    random.shuffle(all_node_indices)\n",
        "    num_train_nodes = int(train_percentage * num_nodes)\n",
        "    num_testandcali_nodes = num_nodes - num_train_nodes\n",
        "    train_node_indices = all_node_indices[:num_train_nodes]\n",
        "    testandcali_node_indices = all_node_indices[num_train_nodes:]\n",
        "\n",
        "    test_node_indices = testandcali_node_indices[:int(test_percentage * num_testandcali_nodes)]\n",
        "    cali_node_indices = testandcali_node_indices[int(test_percentage * num_testandcali_nodes):]\n",
        "\n",
        "\n",
        "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    train_mask[train_node_indices] = True\n",
        "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    train_mask[train_node_indices] = True\n",
        "\n",
        "    testandcali_mask = torch.zeros(num_nodes,dtype=torch.bool)\n",
        "    testandcali_mask[testandcali_node_indices] = True\n",
        "\n",
        "    test_mask = torch.zeros(num_nodes,dtype=torch.bool)\n",
        "    test_mask[test_node_indices] = True\n",
        "\n",
        "\n",
        "    cali_mask = torch.zeros(num_nodes,dtype=torch.bool)\n",
        "    cali_mask[cali_node_indices] = True\n",
        "    test_mask_lists.append(test_mask)\n",
        "\n",
        "    #*********************************************\n",
        "    n_train =train_mask.sum()\n",
        "    n_calib = test_mask.sum()\n",
        "    n_test = cali_mask.sum()\n",
        "\n",
        "\n",
        "    #*********************************************\n",
        "    #Use same Notation as the synthetic data\n",
        "    x_train_final, y_train_final, x_calib, y_calib, x_test, y_test= x[train_mask].detach().cpu().numpy(), y[train_mask].detach().cpu().numpy(), x[cali_mask].detach().cpu().numpy(), y[cali_mask].detach().cpu().numpy(), x[test_mask].detach().cpu().numpy(), y[test_mask].detach().cpu().numpy()\n",
        "\n",
        "    x_all = np.concatenate((x_train_final, x_calib, x_test), axis=0)\n",
        "    y_all = np.concatenate((y_train_final,y_calib,y_test),axis = 0)\n",
        "    #*********************************************\n",
        "    train_loader = NeighborSampler(\n",
        "        data.edge_index, node_idx=(train_mask),\n",
        "        sizes=[3, 3, 3], batch_size=32, shuffle=False,\n",
        "    )\n",
        "    #*********************************************\n",
        "    class SAGERegressor(torch.nn.Module):\n",
        "        def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3):\n",
        "            super(SAGERegressor, self).__init__()\n",
        "\n",
        "            self.num_layers = num_layers\n",
        "            self.convs = torch.nn.ModuleList()\n",
        "            self.batch_norms = torch.nn.ModuleList()\n",
        "            self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
        "            self.batch_norms.append(BatchNorm(hidden_channels))\n",
        "\n",
        "            for i in range(num_layers - 2):\n",
        "                self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "                self.batch_norms.append(BatchNorm(hidden_channels))\n",
        "\n",
        "            self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
        "            self.batch_norms.append(BatchNorm(out_channels))\n",
        "            self.linear1 = torch.nn.Linear(out_channels, hidden_channels)\n",
        "            self.linear2 = torch.nn.Linear(hidden_channels, 1)\n",
        "\n",
        "        def reset_parameters(self):\n",
        "\n",
        "            for conv in self.convs:\n",
        "                conv.reset_parameters()\n",
        "            for bn in self.batch_norms:\n",
        "                bn.reset_parameters()\n",
        "            self.linear1.reset_parameters()\n",
        "            self.linear2.reset_parameters()\n",
        "\n",
        "        def forward(self, x, adjs):\n",
        "            for i, (edge_index, _, size) in enumerate(adjs):\n",
        "                x_target = x[:size[1]]\n",
        "                x = self.convs[i]((x, x_target), edge_index)\n",
        "\n",
        "                x = self.batch_norms[i](x)\n",
        "                if i != self.num_layers - 1:\n",
        "                    x = F.leaky_relu(x, 0.2)\n",
        "                    x = F.dropout(x, p=0.1, training=self.training)\n",
        "\n",
        "            x = F.leaky_relu(self.linear1(x), 0.2)\n",
        "            x = self.linear2(x)\n",
        "\n",
        "            return x\n",
        "    #*********************************************\n",
        "    #regression task\n",
        "    model = SAGERegressor(data.num_features, 3, 1, num_layers=3)\n",
        "    model.reset_parameters()\n",
        "    model = model.to(device)\n",
        "    #print(model)\n",
        "    #*********************************************\n",
        "    model.reset_parameters()\n",
        "    #*********************************************\n",
        "    optimizer = torch.optim.Adam(model.parameters(),lr = 0.002)\n",
        "    def train(epoch):\n",
        "        model.train()\n",
        "        i=0\n",
        "        for batch_size,n_id,adjs in train_loader:\n",
        "\n",
        "            adjs = [adj.to(device) for adj in adjs]\n",
        "            optimizer.zero_grad()\n",
        "            # l1_emb, l2_emb, l3_emb = model(x[n_id], adjs)\n",
        "            l3_emb = model(x[n_id], adjs)\n",
        "            loss = F.mse_loss(l3_emb,y[n_id][:batch_size].unsqueeze(1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            i+=loss\n",
        "\n",
        "        return i\n",
        "    for epoch in range(1,20):\n",
        "        loss = train(epoch)\n",
        "    #*********************************************\n",
        "    Calib_loader = NeighborSampler(data.edge_index,node_idx = cali_mask\n",
        "      , sizes = [3,3,3],batch_size = int((test_mask+cali_mask).sum()),shuffle=False)\n",
        "\n",
        "    Test_loader = NeighborSampler(data.edge_index,node_idx = test_mask,sizes = [3,3,3],batch_size = int((test_mask+cali_mask).sum()),shuffle=False)\n",
        "    #*********************************************\n",
        "    #function the takes graph loader to the third layer embedding and the fn which is\n",
        "    #a matrix in the form [x,predicted y]\n",
        "    def inference_1(subgraph_loader):\n",
        "        model.eval()\n",
        "        for batch_size,n_id,adjs in subgraph_loader:\n",
        "\n",
        "            #emb1,emb2,emb3 = model(x[n_id],adjs)\n",
        "            emb3 = model(x[n_id],adjs)\n",
        "\n",
        "            c = x[n_id][:batch_size]\n",
        "            fn = torch.cat((c, emb3), dim=1)\n",
        "\n",
        "\n",
        "        return emb3,fn\n",
        "    #*********************************************\n",
        "\n",
        "    emb3_Calib,fn_Calib = inference_1(Calib_loader)\n",
        "    emb3_Test,fn_Test =  inference_1(Test_loader)\n",
        "    fn_Calib = fn_Calib.detach().cpu().numpy()\n",
        "    fn_Test = fn_Test.detach().cpu().numpy()\n",
        "    fn = np.vstack((fn_Calib,fn_Test))\n",
        "    prediction_dimension = data.num_features\n",
        "    #*********************************************\n",
        "    dimension = data.num_features\n",
        "    #x is input vectors with each row representing one data and output is predicted label vector\n",
        "    def get_xhat(x_vector):\n",
        "      if(x_vector.shape[1]!=1):\n",
        "            xhat_1= np.ones((x_vector.shape[0], 1))\n",
        "            for j in range(x_vector.shape[0]):\n",
        "              for i in range((cali_mask+test_mask).sum()):\n",
        "                if(np.array_equal(x_vector[j],fn[i,0:dimension])):\n",
        "                  xhat_1[j]=fn[i,dimension]\n",
        "            return xhat_1\n",
        "      else:\n",
        "            for i in range((cali_mask+test_mask).sum()):\n",
        "              if(np.array_equal(x_vector.squeeze(),fn[i,0:dimension])):\n",
        "                return fn[i,dimension]\n",
        "    #*********************************************\n",
        "    #Two sided confidence interval\n",
        "    score_fn = lambda x, y : np.abs(y - get_xhat(x).squeeze())\n",
        "    score_inv_fn = lambda s, x : [get_xhat(x)-s, get_xhat(x) + s]\n",
        "    score_inv_fn_lb = lambda s, x : [get_xhat(x) + s, np.inf]\n",
        "    #*********************************************\n",
        "    def indicator_matrix(scalar_values, disc):\n",
        "        scalar_values = np.array(scalar_values)\n",
        "        intervals = [(disc[i], disc[i + 1]) for i in range(len(disc) - 1)]\n",
        "        matrix = np.zeros((scalar_values.shape[0], len(intervals)))\n",
        "        for i in range(scalar_values.shape[0]):\n",
        "          value = scalar_values[i,0]\n",
        "          for j,(a,b) in enumerate(intervals):\n",
        "            if (a<= value <b):\n",
        "              matrix[i,j] = 1\n",
        "\n",
        "        return matrix\n",
        "    #*********************************************\n",
        "    #Consider covariate shift of one of the dimension\n",
        "    dimension_interest = 2\n",
        "    #*********************************************\n",
        "    minimum = data.x[:,dimension_interest].min()\n",
        "    maximum = data.x[:,dimension_interest].max()\n",
        "    #disc depends on the minimum and maximum of the second dimension\n",
        "    #We next consider multiple types of covariate shifts and the last one is without any shifts\n",
        "    disc = np.arange(math.ceil(minimum)-1, math.ceil(maximum),0.5)\n",
        "    def phi_fn_groups(x_12):\n",
        "      if len(x_12.shape) == 2:\n",
        "        a=x_12[:x_12.shape[0],dimension_interest]\n",
        "        a = a.reshape(-1,1)\n",
        "        return indicator_matrix(a, disc)\n",
        "      else:\n",
        "        a = x_12.reshape(-1,1)\n",
        "        a = a[dimension_interest]\n",
        "        a = a.reshape(-1,1)\n",
        "        return indicator_matrix(a, disc)\n",
        "\n",
        "    #The gaussian density tilts with mean and variance\n",
        "    eval_locs = [-1.5, -0.5]\n",
        "    eval_scale = 0.2\n",
        "\n",
        "    other_locs = [0, 1,1.5]\n",
        "    other_scale = 1\n",
        "\n",
        "    def phi_fn_shifts(x_13):\n",
        "        if(len(x_13.shape) == 2):\n",
        "          a = x_13[:x_13.shape[0],dimension_interest]\n",
        "\n",
        "          shifts = [norm.pdf(a, loc=loc, scale=eval_scale).reshape(-1,1)\n",
        "                        for loc in eval_locs]\n",
        "          shifts.extend([norm.pdf(a, loc=loc, scale=other_scale).reshape(-1,1)\n",
        "                        for loc in other_locs])\n",
        "          shifts.append(np.ones((a.shape[0], 1)))\n",
        "          return np.concatenate(shifts, axis=1)\n",
        "\n",
        "        else:\n",
        "          a = x_13.reshape(-1,1)\n",
        "          a = a[dimension_interest]\n",
        "          shifts = [norm.pdf(a, loc=loc, scale=eval_scale).reshape(-1,1)\n",
        "                        for loc in eval_locs]\n",
        "          shifts.extend([norm.pdf(a, loc=loc, scale=other_scale).reshape(-1,1)\n",
        "                        for loc in other_locs])\n",
        "          shifts.append(np.ones((a.shape[0], 1)))\n",
        "          return np.concatenate(shifts, axis=1)\n",
        "    def intercept(x_14):\n",
        "      if(len(x_14.shape)==2):\n",
        "        return x_14\n",
        "      else:\n",
        "        return x_14.reshape(1,data.num_features)\n",
        "\n",
        "    def phi_kernel_estimation(x_15):\n",
        "      if(len(x_15.shape) == 2):\n",
        "        ones_vector = np.ones((x_15.shape[0],1))\n",
        "        new_data_point_std = x_15.reshape(x_15.shape[0],data.num_features)\n",
        "        projected_vector = pca.transform(new_data_point_std)\n",
        "        return np.column_stack((kde(projected_vector.T).reshape(x_15.shape[0],1),ones_vector))\n",
        "\n",
        "      else:\n",
        "        ones_vector = np.ones((1,1))\n",
        "        new_data_point_std = x_15.reshape(1,data.num_features)\n",
        "        projected_vector = pca.transform(new_data_point_std)\n",
        "\n",
        "        return np.column_stack((kde(projected_vector).reshape(1,1),ones_vector))\n",
        "\n",
        "\n",
        "\n",
        "    #*********************************************\n",
        "    #Set up the problem by kernel density estimation\n",
        "    infinite_params = {}\n",
        "    cond_conf = CondConf(score_fn, phi_kernel_estimation, infinite_params)\n",
        "    cond_conf.setup_problem(x_calib, y_calib)\n",
        "\n",
        "    n_test = len(x_test)\n",
        "    alpha = 0.05\n",
        "    lbs_kernel = np.zeros((n_test,))\n",
        "    ubs_kernel= np.zeros((n_test,))\n",
        "    i = 0\n",
        "    for x_t in tqdm(x_test):\n",
        "        res = cond_conf.predict(1-alpha, x_t, score_inv_fn)\n",
        "        lbs_kernel[i] = res[0]\n",
        "        ubs_kernel[i] = res[1]\n",
        "        i += 1\n",
        "    #*********************************************\n",
        "    #Set up the problem by groups\n",
        "    \"\"\"\n",
        "    infinite_params = {}\n",
        "    #cond_conf = CondConf(score_fn, phi_fn_shifts, infinite_params)\n",
        "    #cond_conf.setup_problem(x_calib, y_calib)\n",
        "\n",
        "    n_test = len(x_test)\n",
        "    alpha = 0.05\n",
        "    lbs_groups = np.zeros((n_test,))\n",
        "    ubs_groups = np.zeros((n_test,))\n",
        "    i = 0\n",
        "    for x_t in tqdm(x_test):\n",
        "        res = cond_conf.predict(1-alpha, x_t, score_inv_fn)\n",
        "        lbs_groups[i] = res[0]\n",
        "        ubs_groups[i] = res[1]\n",
        "      i += 1\n",
        "    \"\"\"\n",
        "\n",
        "    def get_marginalcoverage_condi(ubs_1,lbs_1):\n",
        "      j=0\n",
        "      for i in range(len(x_test)):\n",
        "        if((y_test[i]<=ubs_1[i]) & (y_test[i]>=lbs_1[i])):\n",
        "          j+=1\n",
        "\n",
        "      return j/len(x_test)\n",
        "    def get_marginalcoverage_split():\n",
        "      j=0\n",
        "      x_hat = get_xhat(x_test)\n",
        "      for i in range(len(x_test)):\n",
        "        if((y_test[i]>=x_hat[i]-q )& (y_test[i]<= x_hat[i]+q)):\n",
        "          j+=1\n",
        "\n",
        "      return j/len(x_test)\n",
        "    def evaluate_predictions(S, X, y):\n",
        "        marg_coverage = np.mean([y[i] in S[i] for i in range(len(y))])\n",
        "        wsc_coverage = coverage.wsc_unbiased(X, y, S)\n",
        "        print('Marginal coverage:       {:2.3%}'.format(marg_coverage))\n",
        "        print('WS conditional coverage: {:2.3%}'.format(wsc_coverage))\n",
        "\n",
        "    q = np.quantile(np.abs(get_xhat(x_calib) - y_calib.reshape(-1,1)),\n",
        "                    np.ceil((len(x_calib) + 1) * (0.95)) / len(x_calib),)\n",
        "\n",
        "    marginal_coverage_split = get_marginalcoverage_split()\n",
        "    marginal_coverage_condi_kernel = get_marginalcoverage_condi(ubs_kernel,lbs_kernel)\n",
        "    marginal_condi.append(marginal_coverage_condi_kernel)\n",
        "    marginal_split.append(marginal_coverage_split)\n",
        "    ubs_condi_kernel.append(ubs_kernel)\n",
        "    lbs_condi_kernel.append(lbs_kernel)\n",
        "    ubs_split.append(get_xhat(x_test)+q)\n",
        "    lbs_split.append(get_xhat(x_test)-q)\n",
        "    #Check the marginal and WS conditional coverage\n",
        "    ubs = (get_xhat(x_test)+q).squeeze()\n",
        "    lbs =(get_xhat(x_test)-q).squeeze()\n",
        "    test_indices = test_mask\n",
        "    intervals = [pd.Interval(left=lb, right=ub, closed='right') for lb, ub in zip(lbs, ubs)]\n",
        "    print(f\"The empirical marginal and WS conditional coverage using split conformal is: \")\n",
        "    print(evaluate_predictions(intervals,x[test_indices].detach().cpu().numpy(),y[test_indices].detach().cpu().numpy()))\n",
        "\n",
        "\n",
        "    ubs = ubs_kernel\n",
        "    lbs = lbs_kernel\n",
        "    test_indices = test_mask\n",
        "    intervals = [pd.Interval(left=lb, right=ub, closed='right') for lb, ub in zip(lbs, ubs)]\n",
        "    print(f\"The empirical marginal and WS conditional coverage using kernel density is: \")\n",
        "    print(evaluate_predictions(intervals,x[test_indices].detach().cpu().numpy(),y[test_indices].detach().cpu().numpy()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_vLnnJAex58"
      },
      "outputs": [],
      "source": [
        "#Plot the first result of the dimension of interest of covariates\n",
        "x_test = x[test_mask_lists[0]]\n",
        "y_test = y[test_mask_lists[0]]\n",
        "ubs = ubs_condi_kernel[0]\n",
        "lbs = lbs_condi_kernel[0]\n",
        "dimension_interest = 2\n",
        "\n",
        "cp = sns.color_palette()\n",
        "sns.set(font=\"DejaVu Sans\")\n",
        "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
        "fig = plt.figure()\n",
        "fig.set_size_inches(17.5, 6)\n",
        "sort_order = np.argsort(x_test[0:n_test,dimension_interest])\n",
        "x_test_s = x_test[0:n_test,dimension_interest][sort_order]\n",
        "y_test_s = y_test[sort_order]\n",
        "y_test_hat = get_xhat(x_test[sort_order])\n",
        "lb = lbs[sort_order]\n",
        "ub = ubs[sort_order]\n",
        "\n",
        "ax1 = fig.add_subplot(1, 3, 1)\n",
        "ax1.plot(x_test_s, y_test_s, '.', alpha=0.2)\n",
        "ax1.plot(x_test_s, y_test_hat, lw=1, color='k')\n",
        "ax1.plot(x_test_s, y_test_hat + q, color=cp[0], lw=2)\n",
        "ax1.plot(x_test_s, y_test_hat - q, color=cp[0], lw=2)\n",
        "\n",
        "ax1.set_ylim(-5,10)\n",
        "ax1.tick_params(axis='both', which='major', labelsize=14)\n",
        "ax1.set_xlabel(\"$X$\", fontsize=16, labelpad=10)\n",
        "ax1.set_ylabel(\"$Y$\", fontsize=16, labelpad=10)\n",
        "ax1.set_title(\"Split Conformal\", fontsize=18, pad=12)\n",
        "ax1.fill_between(x_test_s.flatten(), y_test_hat.flatten() - q, y_test_hat.flatten() + q,\n",
        "                 color=cp[3], alpha=0.4, label='split prediction interval')\n",
        "\n",
        "\n",
        "ax2 = fig.add_subplot(1, 3, 2, sharex = ax1, sharey = ax1)\n",
        "ax2.plot(x_test_s, y_test_s, '.', alpha=0.2)\n",
        "ax2.plot(x_test_s, y_test_hat, color='k', lw=1)\n",
        "ax2.plot(x_test_s, ub, color=cp[1], lw=2)\n",
        "ax2.plot(x_test_s, lb, color=cp[1], lw=2)\n",
        "ax2.fill_between(x_test_s.flatten(), lb,\n",
        "                 ub,\n",
        "                 color=cp[3], alpha=0.4, label='conditional calibration')\n",
        "ax2.tick_params(axis='both', which='major', direction='out', labelsize=14)\n",
        "ax2.set_xlabel(\"$X$\", fontsize=16, labelpad=10)\n",
        "ax2.set_ylabel(\"$Y$\", fontsize=16, labelpad=10)\n",
        "ax2.set_title(\"Conditional Calibration\", fontsize=18, pad=12)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "169b0XbkEQL5ZbJK9htGLsQE3ZnQH2-Gy",
      "authorship_tag": "ABX9TyNR8PKooy8sunjLXAkCtB7/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}