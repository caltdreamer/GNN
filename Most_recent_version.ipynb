{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caltdreamer/GNN/blob/main/Most_recent_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcX285LF1Ngv"
      },
      "outputs": [],
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install -q torch-geometric\n",
        "!pip install umap-learn\n",
        "!git clone https://github.com/msesia/arc.git\n",
        "!pip install conditionalconformal\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7BhvS6Ou9Gm"
      },
      "outputs": [],
      "source": [
        "!pip install -r /content/conformalized-gnn-master/requirements.txt\n",
        "%cd /content/conformalized-gnn-master\n",
        "!mkdir -p /content/conformalized-gnn-master/model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GraphSAGE\n",
        "from torch_geometric.data import NeighborSampler\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.datasets import NELL\n",
        "from torch_geometric.datasets import CitationFull\n",
        "from torch_geometric.datasets import CoraFull\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.data import Data\n",
        "from tqdm import tqdm\n",
        "from torch_geometric.nn import SAGEConv, BatchNorm\n",
        "import arc\n",
        "from arc import models\n",
        "from arc import methods\n",
        "from arc import black_boxes\n",
        "from arc import others\n",
        "from arc import coverage\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os.path as osp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import collections\n",
        "import random\n",
        "import networkx as nx\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "from numpy.ma.core import maximum\n",
        "from conditionalconformal.synthetic_data import generate_cqr_data, indicator_matrix\n",
        "from conditionalconformal import CondConf\n",
        "\n",
        "from scipy.stats import norm\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import gaussian_kde\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ],
      "metadata": {
        "id": "0sPBFV-BCQxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the results"
      ],
      "metadata": {
        "id": "C3U0Jxai96q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_weighted_average_coverage(coverages, X_test, f):\n",
        "\n",
        "    weighted_coverages = [coverages[i] * f(X_test[i]) for i in range(len(X_test))]\n",
        "\n",
        "\n",
        "    total_weight = sum(f(X_test[i]) for i in range(len(X_test)))\n",
        "\n",
        "\n",
        "    weighted_average_coverage = sum(weighted_coverages) / total_weight\n",
        "\n",
        "    return weighted_average_coverage\n",
        "def create_intervals(min_value, max_value, n):\n",
        "    interval_length = (max_value - min_value) / n\n",
        "\n",
        "    # Create the intervals\n",
        "    intervals = [(min_value + i * interval_length, min_value + (i + 1) * interval_length) for i in range(n)]\n",
        "\n",
        "    return intervals"
      ],
      "metadata": {
        "id": "09mZtskjSXJn"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Regression_SplitwithCondi_pca=rank_results.txt'\n",
        "\n",
        "with open(file_path, 'rb') as file:\n",
        "    results = pickle.load(file)\n",
        "\n",
        "datasets = ['Anaheim','ChicagoSketch','county_election_2016','county_education_2012','county_income_2012','county_unemployment_2012','twitch_PTBR']\n",
        "datasets = ['Anaheim']\n",
        "dataset_results = {dataset: {'method_split_conformal': {}} for dataset in datasets}\n",
        "num_nodes_dataset = my_dict = {\n",
        "    'Anaheim': 914,\n",
        "    'ChicagoSketch': 2176 ,\n",
        "    'county_election_2016': 3234,\n",
        "    'county_education_2012': 3234,\n",
        "    'county_income_2012': 3234,\n",
        "    'county_unemployment_2012':3234,\n",
        "    'twitch_PTBR':1912\n",
        "}\n",
        "weighted_coverage = {dataset: {'Weighted_coverage_density': []} for dataset in datasets}\n",
        "for dataset in datasets:\n",
        "    print(dataset)\n",
        "    with open(f'/content/conformalized-gnn-master/dataset_regression/{dataset}_edge_list.txt', 'r') as f:\n",
        "      edges = [list(map(int, line.strip().split())) for line in f]\n",
        "    edges_tensor = torch.tensor(edges, dtype=torch.long)\n",
        "    edge_index = edges_tensor.t().contiguous()\n",
        "    features = torch.tensor([list(map(float, line.strip().split())) for line in open(f'/content/conformalized-gnn-master/dataset_regression/{dataset}_features.txt')], dtype=torch.float)\n",
        "    labels = torch.tensor([float(line.strip()) for line in open(f'/content/conformalized-gnn-master/dataset_regression/{dataset}_labels.txt')], dtype=torch.float)\n",
        "    num_nodes = num_nodes_dataset.get(f'{dataset}')\n",
        "    valid_edges_mask = (edge_index >= 0).all(dim=0) & (edge_index < num_nodes).all(dim=0)\n",
        "    filtered_edge_index = edge_index[:, valid_edges_mask]\n",
        "\n",
        "    data = Data(x=features, edge_index=filtered_edge_index, y=labels)\n",
        "    rank = np.linalg.matrix_rank(data.x.detach().cpu().numpy())\n",
        "    print(data)\n",
        "    data_4d = data.x\n",
        "    scaler = StandardScaler()\n",
        "    data_4d_std = scaler.fit_transform(data_4d)\n",
        "    pca = PCA(n_components=rank)\n",
        "    data_reduced = pca.fit_transform(data_4d_std)\n",
        "    kde = gaussian_kde(data_reduced.T)\n",
        "\n",
        "    total_coverage = 0\n",
        "    for o in range(30):\n",
        "\n",
        "      coverages = results.get(f'{dataset}').get(f'Iteration_{o}').get('coverage_flag')\n",
        "      X_test = results.get(f'{dataset}').get(f'Iteration_{o}').get('current_test_features')\n",
        "      X_test_transformed = pca.transform(X_test)\n",
        "      #print(kde(X_test_transformed[0]))\n",
        "      # Apply KDE to the PCA-transformed test points\n",
        "      weighted_coverages = [coverages[i] * kde(X_test_transformed[i:i+1]) for i in range(len(X_test_transformed))]\n",
        "      total_weight = sum(kde(X_test_transformed[i:i+1]) for i in range(len(X_test_transformed)))\n",
        "\n",
        "      weighted_average_coverage = sum(weighted_coverages) / total_weight\n",
        "      print(weighted_average_coverage)\n",
        "      total_coverage += weighted_average_coverage\n",
        "    weighted_coverage[dataset]['Weighted_coverage_density'].append(total_coverage/30)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "QSLtmCfSSRwh",
        "outputId": "2be9f54d-27e4-40ef-d6d9-bbcdabed18ae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anaheim\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/conformalized-gnn-master/dataset_regression/Anaheim_edge_list.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-180d121f9f5e>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'/content/conformalized-gnn-master/dataset_regression/{dataset}_edge_list.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m       \u001b[0medges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0medges_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/conformalized-gnn-master/dataset_regression/Anaheim_edge_list.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "datasets = ['Anaheim','ChicagoSketch','county_election_2016','county_education_2012','county_income_2012','county_unemployment_2012','twitch_PTBR']\n",
        "dataset_results = {dataset: {'method_split_conformal': {}} for dataset in datasets}\n",
        "num_nodes_dataset = my_dict = {\n",
        "    'Anaheim': 914,\n",
        "    'ChicagoSketch': 2176 ,\n",
        "    'county_election_2016': 3234,\n",
        "    'county_education_2012': 3234,\n",
        "    'county_income_2012': 3234,\n",
        "    'county_unemployment_2012':3234,\n",
        "    'twitch_PTBR':1912\n",
        "}\n",
        "\n",
        "weighted_coverage = {dataset: {'Weighted_coverage_density': {}} for dataset in datasets}\n",
        "for dataset in datasets:\n",
        "    print(dataset)\n",
        "    with open(f'/content/conformalized-gnn-master/dataset_regression/{dataset}_edge_list.txt', 'r') as f:\n",
        "      edges = [list(map(int, line.strip().split())) for line in f]\n",
        "    edges_tensor = torch.tensor(edges, dtype=torch.long)\n",
        "    edge_index = edges_tensor.t().contiguous()\n",
        "    features = torch.tensor([list(map(float, line.strip().split())) for line in open(f'/content/conformalized-gnn-master/dataset_regression/{dataset}_features.txt')], dtype=torch.float)\n",
        "    labels = torch.tensor([float(line.strip()) for line in open(f'/content/conformalized-gnn-master/dataset_regression/{dataset}_labels.txt')], dtype=torch.float)\n",
        "    num_nodes = num_nodes_dataset.get(f'{dataset}')\n",
        "    valid_edges_mask = (edge_index >= 0).all(dim=0) & (edge_index < num_nodes).all(dim=0)\n",
        "    filtered_edge_index = edge_index[:, valid_edges_mask]\n",
        "\n",
        "    data = Data(x=features, edge_index=filtered_edge_index, y=labels)\n",
        "    rank = np.linalg.matrix_rank(data.x.detach().cpu().numpy())\n",
        "    print(data)\n",
        "    data_4d = data.x\n",
        "    scaler = StandardScaler()\n",
        "    data_4d_std = scaler.fit_transform(data_4d)\n",
        "    pca = PCA(n_components=rank)\n",
        "    data_reduced = pca.fit_transform(data_4d_std)\n",
        "    kde = gaussian_kde(data_reduced.T)\n",
        "\n",
        "    covariates_np = data.x.detach().cpu().numpy()\n",
        "\n",
        "    min_value = np.min(covariates_np[:, 2])\n",
        "    max_value = np.max(covariates_np[:, 2])\n",
        "    def indicator_function(x, interval):\n",
        "        return 1 if interval[0] <= x < interval[1] else 0\n",
        "    intervals = create_intervals(min_value,max_value,10)\n",
        "\n",
        "    interval_coverage = {i: [] for i in range(10)}\n",
        "    for o in range(30):\n",
        "        coverages = results[dataset]['Iteration_{}'.format(o)]['coverage_flag']\n",
        "        X_test = results[dataset]['Iteration_{}'.format(o)]['current_test_features']\n",
        "\n",
        "        for b in range(10):\n",
        "            weighted_coverages = [coverages[i] * indicator_function(X_test[i:i+1][0][2], intervals[b]) for i in range(len(X_test))]\n",
        "            total_weight = sum(indicator_function(X_test[i:i+1][0][2], intervals[b]) for i in range(len(X_test)))\n",
        "\n",
        "\n",
        "            weighted_average_coverage = sum(weighted_coverages) / total_weight if total_weight != 0 else 0\n",
        "            interval_coverage[b].append(weighted_average_coverage)\n",
        "\n",
        "    for b in range(10):\n",
        "        weighted_coverage[dataset]['Weighted_coverage_density'][intervals[b]] = np.mean(interval_coverage[b])\n",
        "\n"
      ],
      "metadata": {
        "id": "PBqCGmmBhOr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data in datasets:\n",
        "    weighted_cov = weighted_coverage[data]['Weighted_coverage_density']\n",
        "    marginal_cov = np.mean([results[data]['Iteration_{}'.format(i)]['Marginal_Coverage'] for i in range(30)])\n",
        "    conditional_cov = np.mean([results[data]['Iteration_{}'.format(i)]['conditional_coverage'] for i in range(30)])\n",
        "    length = conditional_cov = np.mean([results[data]['Iteration_{}'.format(i)]['Average length'] for i in range(30)])\n",
        "\n",
        "    print(f\"{data}:\\n\"\n",
        "          f\"  - Marginal coverage: {marginal_cov}\\n\"\n",
        "          f\"  - Weighted coverage: {weighted_cov}\\n\"\n",
        "          f\"  - Conditional coverage: {conditional_cov}\")\n"
      ],
      "metadata": {
        "id": "GgsmsWKyZVVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the results\n",
        "for dataset in datasets:\n",
        "    print(f\"{dataset}:\")\n",
        "    for interval, avg_coverage in weighted_coverage[dataset]['Weighted_coverage_density'].items():\n",
        "        print(f\"  Interval {interval}: Average Weighted Coverage = {avg_coverage}\")"
      ],
      "metadata": {
        "id": "18hQJJNXli-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTzaCTmx9hO3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e52646f2-ef55-4246-a254-60dae7c5898e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anaheim dataset has original feature dimension = 4\n",
            "Anaheim dataset has feature rank = 4\n",
            "ChicagoSketch dataset has original feature dimension = 4\n",
            "ChicagoSketch dataset has feature rank = 3\n",
            "county_election_2016 dataset has original feature dimension = 6\n",
            "county_election_2016 dataset has feature rank = 6\n",
            "county_education_2012 dataset has original feature dimension = 6\n",
            "county_education_2012 dataset has feature rank = 6\n",
            "county_income_2012 dataset has original feature dimension = 6\n",
            "county_income_2012 dataset has feature rank = 6\n",
            "county_unemployment_2012 dataset has original feature dimension = 6\n",
            "county_unemployment_2012 dataset has feature rank = 6\n",
            "twitch_PTBR dataset has original feature dimension = 3170\n",
            "twitch_PTBR dataset has feature rank = 1046\n"
          ]
        }
      ],
      "source": [
        "dataset_names = datasets = ['Anaheim','ChicagoSketch','county_election_2016','county_education_2012','county_income_2012','county_unemployment_2012','twitch_PTBR']\n",
        "num_nodes_dataset = my_dict = {\n",
        "    'Anaheim': 914,\n",
        "    'ChicagoSketch': 2176 ,\n",
        "    'county_election_2016': 3234,\n",
        "    'county_education_2012': 3234,\n",
        "    'county_income_2012': 3234,\n",
        "    'county_unemployment_2012':3234,\n",
        "    'twitch_PTBR':1912\n",
        "}\n",
        "for dataset in dataset_names:\n",
        "\n",
        "    with open(f'/content/conformalized-gnn-master/dataset_regression/{dataset}_edge_list.txt', 'r') as f:\n",
        "        edges = [list(map(int, line.strip().split())) for line in f]\n",
        "    edges_tensor = torch.tensor(edges, dtype=torch.long)\n",
        "    edge_index = edges_tensor.t().contiguous()\n",
        "    features = torch.tensor([list(map(float, line.strip().split())) for line in open(f'/content/conformalized-gnn-master/dataset_regression/{dataset}_features.txt')], dtype=torch.float)\n",
        "    labels = torch.tensor([float(line.strip()) for line in open(f'/content/conformalized-gnn-master/dataset_regression/{dataset}_labels.txt')], dtype=torch.float)\n",
        "    num_nodes = num_nodes_dataset.get(f'{dataset}')\n",
        "    valid_edges_mask = (edge_index >= 0).all(dim=0) & (edge_index < num_nodes).all(dim=0)\n",
        "    filtered_edge_index = edge_index[:, valid_edges_mask]\n",
        "\n",
        "    data = Data(x=features, edge_index=filtered_edge_index, y=labels)\n",
        "    rank = np.linalg.matrix_rank(data.x.detach().cpu().numpy())\n",
        "    print(f'{dataset} dataset has original feature dimension = {data.num_node_features}')\n",
        "    print(f'{dataset} dataset has feature rank = {rank}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using our methods"
      ],
      "metadata": {
        "id": "Jh-7bu3E-fLh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ibCIOu12_UJ"
      },
      "outputs": [],
      "source": [
        "datasets = ['Anaheim','ChicagoSketch','county_election_2016','county_education_2012','county_income_2012','county_unemployment_2012']\n",
        "dataset_results = {dataset: {'method_split_conformal': {}, 'method_condition_calibrated': {}} for dataset in datasets}\n",
        "num_nodes_dataset = my_dict = {\n",
        "    'Anaheim': 914,\n",
        "    'ChicagoSketch': 2176 ,\n",
        "    'county_election_2016': 3234,\n",
        "    'county_education_2012': 3234,\n",
        "    'county_income_2012': 3234,\n",
        "    'county_unemployment_2012':3234,\n",
        "    'twitch_PTBR':1912\n",
        "}\n",
        "for dataset in datasets:\n",
        "    print(dataset)\n",
        "    with open(f'/content/conformalized-gnn-master/dataset_regression/{dataset}_edge_list.txt', 'r') as f:\n",
        "      edges = [list(map(int, line.strip().split())) for line in f]\n",
        "    edges_tensor = torch.tensor(edges, dtype=torch.long)\n",
        "    edge_index = edges_tensor.t().contiguous()\n",
        "    features = torch.tensor([list(map(float, line.strip().split())) for line in open(f'/content/conformalized-gnn-master/dataset_regression/{dataset}_features.txt')], dtype=torch.float)\n",
        "    labels = torch.tensor([float(line.strip()) for line in open(f'/content/conformalized-gnn-master/dataset_regression/{dataset}_labels.txt')], dtype=torch.float)\n",
        "    num_nodes = num_nodes_dataset.get(f'{dataset}')\n",
        "    valid_edges_mask = (edge_index >= 0).all(dim=0) & (edge_index < num_nodes).all(dim=0)\n",
        "    filtered_edge_index = edge_index[:, valid_edges_mask]\n",
        "\n",
        "    data = Data(x=features, edge_index=filtered_edge_index, y=labels)\n",
        "    rank = np.linalg.matrix_rank(data.x.detach().cpu().numpy())\n",
        "\n",
        "    data_4d = data.x\n",
        "    scaler = StandardScaler()\n",
        "    data_4d_std = scaler.fit_transform(data_4d)\n",
        "    pca = PCA(n_components=rank)\n",
        "    data_reduced = pca.fit_transform(data_4d_std)\n",
        "    kde = gaussian_kde(data_reduced.T)\n",
        "    print(f'dataset name {dataset}')\n",
        "    print(data)\n",
        "\n",
        "    for c in range(30):\n",
        "\n",
        "      device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "      x = data.x.to(device)\n",
        "      y = data.y.squeeze().to(device)\n",
        "      train_percentage = 0.4\n",
        "      test_percentage = 0.5\n",
        "      split_marginal = []\n",
        "      split_wsc_condition = []\n",
        "      split_length = []\n",
        "      condition_calibrated_marginal = []\n",
        "      condition_calibrated_wsc_condition = []\n",
        "      condition_calibrated_length = []\n",
        "\n",
        "\n",
        "      #Randomly choose train and calitest set\n",
        "\n",
        "      num_nodes = data.num_nodes\n",
        "      all_node_indices = list(range(num_nodes))\n",
        "      random.shuffle(all_node_indices)\n",
        "      num_train_nodes = int(train_percentage * num_nodes)\n",
        "      num_testandcali_nodes = num_nodes - num_train_nodes\n",
        "      train_node_indices = all_node_indices[:num_train_nodes]\n",
        "      testandcali_node_indices = all_node_indices[num_train_nodes:]\n",
        "\n",
        "      test_node_indices = testandcali_node_indices[:int(test_percentage * num_testandcali_nodes)]\n",
        "      cali_node_indices = testandcali_node_indices[int(test_percentage * num_testandcali_nodes):]\n",
        "\n",
        "\n",
        "      train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "      train_mask[train_node_indices] = True\n",
        "      train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "      train_mask[train_node_indices] = True\n",
        "\n",
        "      testandcali_mask = torch.zeros(num_nodes,dtype=torch.bool)\n",
        "      testandcali_mask[testandcali_node_indices] = True\n",
        "\n",
        "      test_mask = torch.zeros(num_nodes,dtype=torch.bool)\n",
        "      test_mask[test_node_indices] = True\n",
        "\n",
        "\n",
        "      cali_mask = torch.zeros(num_nodes,dtype=torch.bool)\n",
        "      cali_mask[cali_node_indices] = True\n",
        "      #test_mask_lists.append(test_mask)\n",
        "\n",
        "      #*********************************************\n",
        "      n_train =train_mask.sum()\n",
        "      n_calib = test_mask.sum()\n",
        "      n_test = cali_mask.sum()\n",
        "\n",
        "\n",
        "      #*********************************************\n",
        "      #Use same Notation as the synthetic data\n",
        "      x_train_final, y_train_final, x_calib, y_calib, x_test, y_test= x[train_mask].detach().cpu().numpy(), y[train_mask].detach().cpu().numpy(), x[cali_mask].detach().cpu().numpy(), y[cali_mask].detach().cpu().numpy(), x[test_mask].detach().cpu().numpy(), y[test_mask].detach().cpu().numpy()\n",
        "\n",
        "      x_all = np.concatenate((x_train_final, x_calib, x_test), axis=0)\n",
        "      y_all = np.concatenate((y_train_final,y_calib,y_test),axis = 0)\n",
        "      #*********************************************\n",
        "      train_loader = NeighborSampler(\n",
        "          data.edge_index, node_idx=(train_mask),\n",
        "          sizes=[3, 3, 3], batch_size=32, shuffle=False,\n",
        "      )\n",
        "      #*********************************************\n",
        "      class SAGERegressor(torch.nn.Module):\n",
        "          def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3):\n",
        "              super(SAGERegressor, self).__init__()\n",
        "\n",
        "              self.num_layers = num_layers\n",
        "              self.convs = torch.nn.ModuleList()\n",
        "              self.batch_norms = torch.nn.ModuleList()\n",
        "              self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
        "              self.batch_norms.append(BatchNorm(hidden_channels))\n",
        "\n",
        "              for i in range(num_layers - 2):\n",
        "                  self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "                  self.batch_norms.append(BatchNorm(hidden_channels))\n",
        "\n",
        "              self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
        "              self.batch_norms.append(BatchNorm(out_channels))\n",
        "              self.linear1 = torch.nn.Linear(out_channels, hidden_channels)\n",
        "              self.linear2 = torch.nn.Linear(hidden_channels, 1)\n",
        "\n",
        "          def reset_parameters(self):\n",
        "\n",
        "              for conv in self.convs:\n",
        "                  conv.reset_parameters()\n",
        "              for bn in self.batch_norms:\n",
        "                  bn.reset_parameters()\n",
        "              self.linear1.reset_parameters()\n",
        "              self.linear2.reset_parameters()\n",
        "\n",
        "          def forward(self, x, adjs):\n",
        "              for i, (edge_index, _, size) in enumerate(adjs):\n",
        "                  x_target = x[:size[1]]\n",
        "                  x = self.convs[i]((x, x_target), edge_index)\n",
        "\n",
        "                  x = self.batch_norms[i](x)\n",
        "                  if i != self.num_layers - 1:\n",
        "                      x = F.leaky_relu(x, 0.2)\n",
        "                      x = F.dropout(x, p=0.1, training=self.training)\n",
        "\n",
        "              x = F.leaky_relu(self.linear1(x), 0.2)\n",
        "              x = self.linear2(x)\n",
        "\n",
        "              return x\n",
        "      #*********************************************\n",
        "      #regression task\n",
        "      model = SAGERegressor(data.num_features, 3, 1, num_layers=3)\n",
        "      model.reset_parameters()\n",
        "      model = model.to(device)\n",
        "      #print(model)\n",
        "      #*********************************************\n",
        "      model.reset_parameters()\n",
        "      #*********************************************\n",
        "      optimizer = torch.optim.Adam(model.parameters(),lr = 0.002)\n",
        "      def train(epoch):\n",
        "          model.train()\n",
        "          i=0\n",
        "          for batch_size,n_id,adjs in train_loader:\n",
        "\n",
        "              adjs = [adj.to(device) for adj in adjs]\n",
        "              optimizer.zero_grad()\n",
        "              # l1_emb, l2_emb, l3_emb = model(x[n_id], adjs)\n",
        "              l3_emb = model(x[n_id], adjs)\n",
        "              loss = F.mse_loss(l3_emb,y[n_id][:batch_size].unsqueeze(1))\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              i+=loss\n",
        "\n",
        "          return i\n",
        "      for epoch in range(1,20):\n",
        "          loss = train(epoch)\n",
        "      #*********************************************\n",
        "      Calib_loader = NeighborSampler(data.edge_index,node_idx = cali_mask\n",
        "        , sizes = [3,3,3],batch_size = int((test_mask+cali_mask).sum()),shuffle=False)\n",
        "\n",
        "      Test_loader = NeighborSampler(data.edge_index,node_idx = test_mask,sizes = [3,3,3],batch_size = int((test_mask+cali_mask).sum()),shuffle=False)\n",
        "      #*********************************************\n",
        "      #function the takes graph loader to the third layer embedding and the fn which is\n",
        "      #a matrix in the form [x,predicted y]\n",
        "      def inference_1(subgraph_loader):\n",
        "          model.eval()\n",
        "          for batch_size,n_id,adjs in subgraph_loader:\n",
        "\n",
        "              #emb1,emb2,emb3 = model(x[n_id],adjs)\n",
        "              emb3 = model(x[n_id],adjs)\n",
        "\n",
        "              c = x[n_id][:batch_size]\n",
        "              fn = torch.cat((c, emb3), dim=1)\n",
        "\n",
        "\n",
        "          return emb3,fn\n",
        "      #*********************************************\n",
        "\n",
        "      emb3_Calib,fn_Calib = inference_1(Calib_loader)\n",
        "      emb3_Test,fn_Test =  inference_1(Test_loader)\n",
        "      fn_Calib = fn_Calib.detach().cpu().numpy()\n",
        "      fn_Test = fn_Test.detach().cpu().numpy()\n",
        "      fn = np.vstack((fn_Calib,fn_Test))\n",
        "      prediction_dimension = data.num_features\n",
        "      #*********************************************\n",
        "      dimension = data.num_features\n",
        "      #x is input vectors with each row representing one data and output is predicted label vector\n",
        "      def get_xhat(x_vector):\n",
        "        if(x_vector.shape[1]!=1):\n",
        "              xhat_1= np.ones((x_vector.shape[0], 1))\n",
        "              for j in range(x_vector.shape[0]):\n",
        "                for i in range((cali_mask+test_mask).sum()):\n",
        "                  if(np.array_equal(x_vector[j],fn[i,0:dimension])):\n",
        "                    xhat_1[j]=fn[i,dimension]\n",
        "              return xhat_1\n",
        "        else:\n",
        "              for i in range((cali_mask+test_mask).sum()):\n",
        "                if(np.array_equal(x_vector.squeeze(),fn[i,0:dimension])):\n",
        "                  return fn[i,dimension]\n",
        "      #*********************************************\n",
        "      #Two sided confidence interval\n",
        "      score_fn = lambda x, y : np.abs(y - get_xhat(x).squeeze())\n",
        "      score_inv_fn = lambda s, x : [get_xhat(x)-s, get_xhat(x) + s]\n",
        "      score_inv_fn_lb = lambda s, x : [get_xhat(x) + s, np.inf]\n",
        "      #*********************************************\n",
        "      def indicator_matrix(scalar_values, disc):\n",
        "          scalar_values = np.array(scalar_values)\n",
        "          intervals = [(disc[i], disc[i + 1]) for i in range(len(disc) - 1)]\n",
        "          matrix = np.zeros((scalar_values.shape[0], len(intervals)))\n",
        "          for i in range(scalar_values.shape[0]):\n",
        "            value = scalar_values[i,0]\n",
        "            for j,(a,b) in enumerate(intervals):\n",
        "              if (a<= value <b):\n",
        "                matrix[i,j] = 1\n",
        "\n",
        "          return matrix\n",
        "      #*********************************************\n",
        "      #Consider covariate shift of one of the dimension\n",
        "      dimension_interest = 2\n",
        "      #*********************************************\n",
        "      minimum = data.x[:,dimension_interest].min()\n",
        "      maximum = data.x[:,dimension_interest].max()\n",
        "      #disc depends on the minimum and maximum of the second dimension\n",
        "      #We next consider multiple types of covariate shifts and the last one is without any shifts\n",
        "      disc = np.arange(math.ceil(minimum)-1, math.ceil(maximum),0.5)\n",
        "      def phi_fn_groups(x_12):\n",
        "        if len(x_12.shape) == 2:\n",
        "          a=x_12[:x_12.shape[0],dimension_interest]\n",
        "          a = a.reshape(-1,1)\n",
        "          return indicator_matrix(a, disc)\n",
        "        else:\n",
        "          a = x_12.reshape(-1,1)\n",
        "          a = a[dimension_interest]\n",
        "          a = a.reshape(-1,1)\n",
        "          return indicator_matrix(a, disc)\n",
        "\n",
        "      #The gaussian density tilts with mean and variance\n",
        "      eval_locs = [-1.5, -0.5]\n",
        "      eval_scale = 0.2\n",
        "\n",
        "      other_locs = [0, 1,1.5]\n",
        "      other_scale = 1\n",
        "\n",
        "      def phi_fn_shifts(x_13):\n",
        "          if(len(x_13.shape) == 2):\n",
        "            a = x_13[:x_13.shape[0],dimension_interest]\n",
        "\n",
        "            shifts = [norm.pdf(a, loc=loc, scale=eval_scale).reshape(-1,1)\n",
        "                          for loc in eval_locs]\n",
        "            shifts.extend([norm.pdf(a, loc=loc, scale=other_scale).reshape(-1,1)\n",
        "                          for loc in other_locs])\n",
        "            shifts.append(np.ones((a.shape[0], 1)))\n",
        "            return np.concatenate(shifts, axis=1)\n",
        "\n",
        "          else:\n",
        "            a = x_13.reshape(-1,1)\n",
        "            a = a[dimension_interest]\n",
        "            shifts = [norm.pdf(a, loc=loc, scale=eval_scale).reshape(-1,1)\n",
        "                          for loc in eval_locs]\n",
        "            shifts.extend([norm.pdf(a, loc=loc, scale=other_scale).reshape(-1,1)\n",
        "                          for loc in other_locs])\n",
        "            shifts.append(np.ones((a.shape[0], 1)))\n",
        "            return np.concatenate(shifts, axis=1)\n",
        "      def intercept(x_14):\n",
        "        if(len(x_14.shape)==2):\n",
        "          return x_14\n",
        "        else:\n",
        "          return x_14.reshape(1,data.num_features)\n",
        "\n",
        "      def phi_kernel_estimation(x_15):\n",
        "        if(len(x_15.shape) == 2):\n",
        "          ones_vector = np.ones((x_15.shape[0],1))\n",
        "          new_data_point_std = x_15.reshape(x_15.shape[0],data.num_features)\n",
        "          projected_vector = pca.transform(new_data_point_std)\n",
        "          return np.column_stack((kde(projected_vector.T).reshape(x_15.shape[0],1),ones_vector))\n",
        "\n",
        "        else:\n",
        "          ones_vector = np.ones((1,1))\n",
        "          new_data_point_std = x_15.reshape(1,data.num_features)\n",
        "          projected_vector = pca.transform(new_data_point_std)\n",
        "\n",
        "          return np.column_stack((kde(projected_vector).reshape(1,1),ones_vector))\n",
        "\n",
        "\n",
        "\n",
        "      #*********************************************\n",
        "      #Set up the problem by kernel density estimation\n",
        "      infinite_params = {}\n",
        "      cond_conf = CondConf(score_fn, phi_kernel_estimation, infinite_params)\n",
        "      cond_conf.setup_problem(x_calib, y_calib)\n",
        "\n",
        "      n_test = len(x_test)\n",
        "      alpha = 0.05\n",
        "      lbs_kernel = np.zeros((n_test,))\n",
        "      ubs_kernel= np.zeros((n_test,))\n",
        "      i = 0\n",
        "      for x_t in tqdm(x_test):\n",
        "          res = cond_conf.predict(1-alpha, x_t, score_inv_fn)\n",
        "          lbs_kernel[i] = res[0]\n",
        "          ubs_kernel[i] = res[1]\n",
        "          i += 1\n",
        "      #*********************************************\n",
        "      #Set up the problem by groups\n",
        "      \"\"\"\n",
        "      infinite_params = {}\n",
        "      #cond_conf = CondConf(score_fn, phi_fn_shifts, infinite_params)\n",
        "      #cond_conf.setup_problem(x_calib, y_calib)\n",
        "\n",
        "      n_test = len(x_test)\n",
        "      alpha = 0.05\n",
        "      lbs_groups = np.zeros((n_test,))\n",
        "      ubs_groups = np.zeros((n_test,))\n",
        "      i = 0\n",
        "      for x_t in tqdm(x_test):\n",
        "          res = cond_conf.predict(1-alpha, x_t, score_inv_fn)\n",
        "          lbs_groups[i] = res[0]\n",
        "          ubs_groups[i] = res[1]\n",
        "        i += 1\n",
        "      \"\"\"\n",
        "\n",
        "      def get_marginalcoverage_condi(ubs_1,lbs_1):\n",
        "        j=0\n",
        "        for i in range(len(x_test)):\n",
        "          if((y_test[i]<=ubs_1[i]) & (y_test[i]>=lbs_1[i])):\n",
        "            j+=1\n",
        "\n",
        "        return j/len(x_test)\n",
        "      def get_marginalcoverage_split():\n",
        "        j=0\n",
        "        x_hat = get_xhat(x_test)\n",
        "        for i in range(len(x_test)):\n",
        "          if((y_test[i]>=x_hat[i]-q )& (y_test[i]<= x_hat[i]+q)):\n",
        "            j+=1\n",
        "\n",
        "        return j/len(x_test)\n",
        "      def evaluate_predictions(S, X, y):\n",
        "          marg_coverage = np.mean([y[i] in S[i] for i in range(len(y))])\n",
        "          wsc_coverage = coverage.wsc_unbiased(X, y, S,delta=0.1,M=1000)\n",
        "          # print('Marginal coverage:       {:2.3%}'.format(marg_coverage))\n",
        "          #print('WS conditional coverage: {:2.3%}'.format(wsc_coverage))\n",
        "          return wsc_coverage\n",
        "      q = np.quantile(np.abs(get_xhat(x_calib) - y_calib.reshape(-1,1)),\n",
        "                      np.ceil((len(x_calib) + 1) * (0.95)) / len(x_calib),)\n",
        "      \"\"\"\n",
        "      marginal_coverage_split = get_marginalcoverage_split()\n",
        "      marginal_coverage_condi_kernel = get_marginalcoverage_condi(ubs_kernel,lbs_kernel)\n",
        "      marginal_condi.append(marginal_coverage_condi_kernel)\n",
        "      marginal_split.append(marginal_coverage_split)\n",
        "      ubs_condi_kernel.append(ubs_kernel)\n",
        "      lbs_condi_kernel.append(lbs_kernel)\n",
        "      ubs_split.append(get_xhat(x_test)+q)\n",
        "      lbs_split.append(get_xhat(x_test)-q)\n",
        "      \"\"\"\n",
        "      #Check the marginal and WS conditional coverage\n",
        "      ubs = (get_xhat(x_test)+q).squeeze()\n",
        "      lbs =(get_xhat(x_test)-q).squeeze()\n",
        "      test_indices = test_mask\n",
        "      intervals = [pd.Interval(left=lb, right=ub, closed='right') for lb, ub in zip(lbs, ubs)]\n",
        "      #print(f\"The empirical marginal and WS conditional coverage using split conformal is: \")\n",
        "      #print(evaluate_predictions(intervals,x[test_indices].detach().cpu().numpy(),y[test_indices].detach().cpu().numpy()))\n",
        "      marg_coverage = np.mean([y[test_indices].detach().cpu().numpy()[i] in intervals[i] for i in range(len(y[test_indices].detach().cpu().numpy()))])\n",
        "      wsc_coverage = evaluate_predictions(intervals,x[test_indices].detach().cpu().numpy(),y[test_indices].detach().cpu().numpy())\n",
        "      split_wsc_condition.append(evaluate_predictions(intervals,x[test_indices].detach().cpu().numpy(),y[test_indices].detach().cpu().numpy()))\n",
        "      split_length.append(2*q)\n",
        "      current_test_features = x[test_mask].detach().cpu().numpy()\n",
        "      coverage_flag = [y[test_indices].detach().cpu().numpy()[i] in intervals[i] for i in range(len(y[test_indices].detach().cpu().numpy()))]\n",
        "\n",
        "      dataset_results[dataset].get('method_split_conformal')[f'Iteration_{c}'] = {\n",
        "          'Marginal_Coverage': marg_coverage,\n",
        "          'Conditional_Coverage': wsc_coverage,\n",
        "          'test_mask':test_mask,\n",
        "          'coverage_flag':coverage_flag,\n",
        "          'Average length':2*q,\n",
        "          'ubs':ubs,\n",
        "          'lbs':lbs}\n",
        "\n",
        "      ubs = ubs_kernel\n",
        "      lbs = lbs_kernel\n",
        "      test_indices = test_mask\n",
        "      intervals = [pd.Interval(left=lb, right=ub, closed='right') for lb, ub in zip(lbs, ubs)]\n",
        "      #print(f\"The empirical marginal and WS conditional coverage using kernel density is: \")\n",
        "      #print(evaluate_predictions(intervals,x[test_indices].detach().cpu().numpy(),y[test_indices].detach().cpu().numpy()))\n",
        "      #condition_calibrated_marginal.append(get_marginalcoverage_condi(ubs,lbs))\n",
        "      #condition_calibrated_wsc_condition.append(evaluate_predictions(intervals,x[test_indices].detach().cpu().numpy(),y[test_indices].detach().cpu().numpy()))\n",
        "      #condition_calibrated_length.append(ubs-lbs)\n",
        "      marginal_coverage = np.mean([y[test_indices].detach().cpu().numpy()[i] in intervals[i] for i in range(len(y[test_indices].detach().cpu().numpy()))])\n",
        "      wsc_coverage = evaluate_predictions(intervals,x[test_indices].detach().cpu().numpy(),y[test_indices].detach().cpu().numpy())\n",
        "      coverage_flag = [y[test_indices].detach().cpu().numpy()[i] in intervals[i] for i in range(len(y[test_indices].detach().cpu().numpy()))]\n",
        "      average_length = np.mean(ubs-lbs)\n",
        "      dataset_results[dataset].get('method_condition_calibrated')[f'Iteration_{c}'] = {\n",
        "          'Marginal_Coverage': marg_coverage,\n",
        "          'Conditional_Coverage': wsc_coverage,\n",
        "          'test_mask':test_mask,\n",
        "          'coverage_flag':coverage_flag,\n",
        "          'Average length':average_length,\n",
        "          'ubs':ubs,\n",
        "          'lbs':lbs}\n",
        "      #print(c)\n",
        "import pickle\n",
        "file_path = '/content/drive/MyDrive/Regression_SplitwithCondi_pca=rank_results.txt'\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump(dataset_results, file)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run their regression model and collect the results"
      ],
      "metadata": {
        "id": "Z-BzngmJ-llt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_D7v0UiJfx2K"
      },
      "outputs": [],
      "source": [
        "datasets = ['Anaheim','ChicagoSketch','county_election_2016','county_education_2012','county_income_2012','county_unemployment_2012','twitch_PTBR']\n",
        "dataset_results = {dataset: {} for dataset in datasets}\n",
        "for dataset_name in datasets:\n",
        "      marginal_coverage_all = []\n",
        "      conditional_coverage_all = []\n",
        "      for z in range(50):\n",
        "          !python train.py --model GraphSAGE \\\n",
        "                      --dataset {dataset_name} \\\n",
        "                      --device cuda \\\n",
        "                      --alpha 0.05 \\\n",
        "                      --num_runs 1 \\\n",
        "                      --conf_correct_model QR \\\n",
        "                      --data_seed {z+100}\n",
        "\n",
        "          import pickle\n",
        "          file_path = '/content/conformalized-gnn-master/data_splits.pkl'\n",
        "          with open(file_path, 'rb') as file:\n",
        "            data_splits = pickle.load(file)\n",
        "          file_path = f'/content/conformalized-gnn-master/pred/{dataset_name}_GraphSAGE_QR_alpha_0.05.pkl'\n",
        "          with open(file_path, 'rb') as file:\n",
        "            results1 = pickle.load(file)\n",
        "          file_path = '/content/conformalized-gnn-master/labelsandfeatures.pkl'\n",
        "          with open(file_path, 'rb') as file:\n",
        "            data_labels = pickle.load(file)\n",
        "\n",
        "          #print(data_labels.get('labels')[~results1[0].get('conf_gnn').get('Raw')[4][0]])\n",
        "          current_data_labels = data_labels.get('labels')\n",
        "          current_data_features = data_labels.get('features')\n",
        "\n",
        "          current_calib_test_mask = data_labels.get('calib_test_mask')\n",
        "          #print(current_calib_test_mask.shape)\n",
        "          current_test_mask_outof_calibtest = ~current_calib_test_mask\n",
        "          current_test_features = current_data_features[current_calib_test_mask][~results1[0].get('gnn').get('CQR')[4][0]]\n",
        "          current_test_labels = current_data_labels[~results1[0].get('gnn').get('CQR')[4][0]]\n",
        "          current_lbs = results1[0].get('gnn').get('CQR')[2][0][0]\n",
        "          current_ubs = results1[0].get('gnn').get('CQR')[2][0][1]\n",
        "          average_length = np.mean(current_ubs - current_lbs)\n",
        "          intervals = [pd.Interval(left=lb, right=ub, closed='right') for lb, ub in zip(current_lbs, current_ubs)]\n",
        "          marg_coverage = np.mean([current_test_labels[i] in intervals[i] for i in range(len(current_test_labels))])\n",
        "          coverage_flag = [current_test_labels[i] in intervals[i] for i in range(len(current_test_labels))]\n",
        "          #wsc_coverage = coverage.wsc_unbiased(current_test_features, current_test_labels, intervals,delta=0.05,M=5000)\n",
        "          dataset_results[dataset_name][f'Iteration_{z}'] = {\n",
        "            'Marginal_Coverage': marg_coverage,\n",
        "            'current_test_features':current_test_features,\n",
        "            'coverage_flag':coverage_flag,\n",
        "            'Average length':average_length,\n",
        "            'current_data_features':current_data_features,\n",
        "            'current_calib_test_mask':current_calib_test_mask,\n",
        "            'current_test_mask_outof_calibtest':current_test_mask_outof_calibtest,\n",
        "            'current_test_labels':current_test_labels}\n",
        "\n",
        "\n",
        "# Save the entire dictionary to a pickle file\n",
        "file_path = '/content/drive/MyDrive/Regression_GNN_results.txt'\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump(dataset_results, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run their classification model and collect the results"
      ],
      "metadata": {
        "id": "T1mUMFt4-tb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "datasets = ['Cora_ML_CF','DBLP_CF','CiteSeer_CF','PubMed_CF','Amazon-Computers','Amazon-Photo','Coauthor-CS','Coauthor-Physics']\n",
        "dataset_results = {dataset: {} for dataset in datasets}\n",
        "for dataset_name in datasets:\n",
        "      marginal_coverage_all = []\n",
        "      conditional_coverage_all = []\n",
        "      for z in range(30):\n",
        "          !python train.py --model GraphSAGE \\\n",
        "                      --dataset {dataset_name} \\\n",
        "                      --device cuda \\\n",
        "                      --alpha 0.05 \\\n",
        "                      --num_runs 1 \\\n",
        "                      --conf_correct_model Calibrate \\\n",
        "                      --calibrator TS \\\n",
        "                      --data_seed {z+200}\n",
        "\n",
        "          import pickle\n",
        "          file_path = '/content/conformalized-gnn-master/data_splits.pkl'\n",
        "          with open(file_path, 'rb') as file:\n",
        "            data_splits = pickle.load(file)\n",
        "          file_path = f'/content/conformalized-gnn-master/pred/{dataset_name}_GraphSAGE_TS_alpha_0.05.pkl'\n",
        "          with open(file_path, 'rb') as file:\n",
        "            results1 = pickle.load(file)\n",
        "          file_path = '/content/conformalized-gnn-master/labelsandfeatures.pkl'\n",
        "          with open(file_path, 'rb') as file:\n",
        "            data_labels = pickle.load(file)\n",
        "          import numpy as np\n",
        "          #print(data_labels.get('labels')[~results1[0].get('conf_gnn').get('Raw')[4][0]])\n",
        "          current_data_labels = data_labels.get('labels')\n",
        "          current_data_features = data_labels.get('features')\n",
        "          current_calib_test_mask = data_labels.get('calib_test_mask')\n",
        "          #print(current_calib_test_mask.shape)\n",
        "          current_test_mask_outof_calibtest = ~current_calib_test_mask\n",
        "          current_test_features = current_data_features[current_calib_test_mask][~results1[0].get('gnn').get('APS')[4][0]]\n",
        "          current_test_labels = current_data_labels[~results1[0].get('gnn').get('APS')[4][0]]\n",
        "          current_predictions = results1[0].get('gnn').get('APS')[2][0]\n",
        "          true_indices_per_row = [list(np.where(row)[0]) for row in current_predictions]\n",
        "          avg_interval_length = np.mean([len(row) for row in true_indices_per_row])\n",
        "          marg_coverage = np.mean([current_test_labels[i] in true_indices_per_row[i] for i in range(len(current_test_labels))])\n",
        "          coverage_flag = [current_test_labels[i] in true_indices_per_row[i] for i in range(len(current_test_labels))]\n",
        "          #print([current_test_labels[i] in true_indices_per_row[i] for i in range(len(current_test_labels))])\n",
        "          wsc_coverage = coverage.wsc_unbiased(current_test_features, current_test_labels, true_indices_per_row,delta=0.1,M=1000)\n",
        "          dataset_results[dataset_name][f'Iteration_{z}'] = {\n",
        "            'Marginal_Coverage': marg_coverage,\n",
        "            'Conditional_Coverage':wsc_coverage,\n",
        "            'Average_Interval_Length': avg_interval_length,\n",
        "            'current_test_features':current_test_features,\n",
        "            'coverage_flag':coverage_flag,\n",
        "            'current_data_features':current_data_features,\n",
        "            'current_calib_test_mask':current_calib_test_mask,\n",
        "            'current_test_mask_outof_calibtest':current_test_mask_outof_calibtest}\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Classification_GNN_results.txt'\n",
        "\n",
        "# Save the final results to a file\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump(dataset_results, file)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "dBIg8tocWo8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/Regression_CFGNN_results.txt'\n",
        "with open(file_path, 'rb') as file:\n",
        "    loaded_results = pickle.load(file)\n",
        "\n",
        "for dataset_name in dataset_results:\n",
        "    marginal_coverage_sum = 0\n",
        "    conditional_coverage_sum = 0\n",
        "    count_valid_conditional = 0\n",
        "\n",
        "    for i in range(30):\n",
        "        iteration_key = f'Iteration_{i}'\n",
        "        marginal_coverage_sum += dataset_results[dataset_name][iteration_key]['Marginal_Coverage']\n",
        "\n",
        "        conditional_coverage = dataset_results[dataset_name][iteration_key]['Conditional_Coverage']\n",
        "        if not np.isnan(conditional_coverage):  # Check if conditional coverage is not nan\n",
        "            conditional_coverage_sum += conditional_coverage\n",
        "            count_valid_conditional += 1\n",
        "\n",
        "    avg_marginal_coverage = marginal_coverage_sum / 30\n",
        "    avg_conditional_coverage = conditional_coverage_sum / count_valid_conditional if count_valid_conditional > 0 else 'nan'\n",
        "\n",
        "    print(f'{dataset_name}:')\n",
        "    print(f'  Average Marginal Coverage: {avg_marginal_coverage}')\n",
        "    print(f'  Average Conditional Coverage: {avg_conditional_coverage}\\n')"
      ],
      "metadata": {
        "id": "O9YiCkSeFYw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "possible plot to see coverage"
      ],
      "metadata": {
        "id": "LHH1eCcl_Fg9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_vLnnJAex58"
      },
      "outputs": [],
      "source": [
        "#Plot the first result of the dimension of interest of covariates\n",
        "x_test = x[test_mask_lists[0]]\n",
        "y_test = y[test_mask_lists[0]]\n",
        "ubs = ubs_condi_kernel[4]\n",
        "lbs = lbs_condi_kernel[4]\n",
        "dimension_interest = 1\n",
        "\n",
        "cp = sns.color_palette()\n",
        "sns.set(font=\"DejaVu Sans\")\n",
        "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
        "fig = plt.figure()\n",
        "fig.set_size_inches(17.5, 6)\n",
        "sort_order = np.argsort(x_test[0:n_test,dimension_interest])\n",
        "x_test_s = x_test[0:n_test,dimension_interest][sort_order]\n",
        "y_test_s = y_test[sort_order]\n",
        "y_test_hat = get_xhat(x_test[sort_order])\n",
        "lb = lbs[sort_order]\n",
        "ub = ubs[sort_order]\n",
        "\n",
        "ax1 = fig.add_subplot(1, 3, 1)\n",
        "ax1.plot(x_test_s, y_test_s, '.', alpha=0.2)\n",
        "ax1.plot(x_test_s, y_test_hat, lw=1, color='k')\n",
        "ax1.plot(x_test_s, y_test_hat + q, color=cp[0], lw=2)\n",
        "ax1.plot(x_test_s, y_test_hat - q, color=cp[0], lw=2)\n",
        "\n",
        "ax1.set_ylim(-5,10)\n",
        "ax1.tick_params(axis='both', which='major', labelsize=14)\n",
        "ax1.set_xlabel(\"$X$\", fontsize=16, labelpad=10)\n",
        "ax1.set_ylabel(\"$Y$\", fontsize=16, labelpad=10)\n",
        "ax1.set_title(\"Split Conformal\", fontsize=18, pad=12)\n",
        "ax1.fill_between(x_test_s.flatten(), y_test_hat.flatten() - q, y_test_hat.flatten() + q,\n",
        "                 color=cp[3], alpha=0.4, label='split prediction interval')\n",
        "\n",
        "\n",
        "ax2 = fig.add_subplot(1, 3, 2, sharex = ax1, sharey = ax1)\n",
        "ax2.plot(x_test_s, y_test_s, '.', alpha=0.2)\n",
        "ax2.plot(x_test_s, y_test_hat, color='k', lw=1)\n",
        "ax2.plot(x_test_s, ub, color=cp[1], lw=2)\n",
        "ax2.plot(x_test_s, lb, color=cp[1], lw=2)\n",
        "ax2.fill_between(x_test_s.flatten(), lb,\n",
        "                 ub,\n",
        "                 color=cp[3], alpha=0.4, label='conditional calibration')\n",
        "ax2.tick_params(axis='both', which='major', direction='out', labelsize=14)\n",
        "ax2.set_xlabel(\"$X$\", fontsize=16, labelpad=10)\n",
        "ax2.set_ylabel(\"$Y$\", fontsize=16, labelpad=10)\n",
        "ax2.set_title(\"Conditional Calibration\", fontsize=18, pad=12)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1CTFhngUMO_SD_uyLw2orgHrOtUVwZN_z",
      "authorship_tag": "ABX9TyPGbL+8E3xo6Q2GdVHszXzx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}