{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7EDPYKn3oxZXC1Hmnr38W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caltdreamer/GNN/blob/main/Conditional_Coverage_GraphSage_classification_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install conditionalconformal"
      ],
      "metadata": {
        "id": "oRpHmcRj8Ygh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa7_gM6R_Hh9"
      },
      "outputs": [],
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install -q torch-geometric\n",
        "!pip install ogb\n",
        "!pip install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GraphSAGE\n",
        "from torch_geometric.data import NeighborSampler\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.datasets import NELL\n",
        "from torch_geometric.datasets import CitationFull\n",
        "from torch_geometric.datasets import CoraFull\n",
        "from torch_geometric.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os.path as osp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import collections\n",
        "import random\n",
        "import networkx as nx\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
      ],
      "metadata": {
        "id": "9ngMu5UK__-0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CoraFull(root='./data')\n",
        "dataset = Planetoid(root='./data', name='Cora',split = 'public')\n",
        "\n"
      ],
      "metadata": {
        "id": "3IHffzL5__8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset[0]\n",
        "print(data)\n",
        "print(data.x)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "x = data.x.to(device)\n",
        "y = data.y.squeeze().to(device)\n"
      ],
      "metadata": {
        "id": "mtWZAZnr__6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)"
      ],
      "metadata": {
        "id": "dFmguIYvl4vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Randomly choose train and calitest set\n",
        "train_percentage = 0.6\n",
        "test_percentage = 0.5\n",
        "\n",
        "\n",
        "\n",
        "num_nodes = data.num_nodes\n",
        "all_node_indices = list(range(num_nodes))\n",
        "random.shuffle(all_node_indices)\n",
        "num_train_nodes = int(train_percentage * num_nodes)\n",
        "num_testandcali_nodes = num_nodes - num_train_nodes\n",
        "train_node_indices = all_node_indices[:num_train_nodes]\n",
        "testandcali_node_indices = all_node_indices[num_train_nodes:]\n",
        "\n",
        "print(testandcali_node_indices)\n",
        "test_node_indices = testandcali_node_indices[:int(test_percentage * num_testandcali_nodes)]\n",
        "cali_node_indices = testandcali_node_indices[int(test_percentage * num_testandcali_nodes):]\n",
        "\n",
        "print(test_node_indices)\n",
        "\n",
        "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "train_mask[train_node_indices] = True\n",
        "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "train_mask[train_node_indices] = True\n",
        "\n",
        "testandcali_mask = torch.zeros(num_nodes,dtype=torch.bool)\n",
        "testandcali_mask[testandcali_node_indices] = True\n",
        "\n",
        "test_mask = torch.zeros(num_nodes,dtype=torch.bool)\n",
        "test_mask[test_node_indices] = True\n",
        "\n",
        "\n",
        "cali_mask = torch.zeros(num_nodes,dtype=torch.bool)\n",
        "cali_mask[cali_node_indices] = True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(train_mask.sum())\n",
        "print(test_mask.sum())\n",
        "print(cali_mask.sum())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(test_node_indices)\n"
      ],
      "metadata": {
        "id": "Biv-BpbR__4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = NeighborSampler(\n",
        "    data.edge_index, node_idx=train_mask,\n",
        "    sizes=[5,5,5], batch_size=64, shuffle=False,\n",
        ")"
      ],
      "metadata": {
        "id": "0redTlMj__1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SAGE(torch.nn.Module):\n",
        "    def __init__(self,in_channels,hidden_channels,out_channels,num_layers = 3):\n",
        "        super(SAGE,self).__init__()\n",
        "\n",
        "        self.numlayers = num_layers\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(SAGEConv(in_channels,hidden_channels))\n",
        "        for i in range(num_layers - 2):\n",
        "            self.convs.append(SAGEConv(hidden_channels,hidden_channels))\n",
        "        self.convs.append(SAGEConv(hidden_channels,out_channels))\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "    def forward(self,x,adjs):\n",
        "        for i ,(edge_index,_,size) in enumerate(adjs):\n",
        "            xs = []\n",
        "            x_target = x[:size[1]]#x should be a list prioritizing target nodes\n",
        "            x = self.convs[i]((x,x_target),edge_index)\n",
        "            if i != self.numlayers -1:\n",
        "                x = F.relu(x)\n",
        "                x = F.dropout(x,p=0.5,training=self.training)\n",
        "            xs.append(x)\n",
        "            if i == 0:\n",
        "                x_all = torch.cat(xs, dim=0)\n",
        "                layer_1_embeddings = x_all\n",
        "            elif i == 1:\n",
        "                x_all = torch.cat(xs, dim=0)\n",
        "                layer_2_embeddings = x_all\n",
        "            elif i == 2:\n",
        "                x_all = torch.cat(xs, dim=0)\n",
        "                layer_3_embeddings = x_all\n",
        "        return layer_1_embeddings, layer_2_embeddings, layer_3_embeddings\n",
        "\n",
        "    def inference(self, x,adjs):\n",
        "        for i ,(edge_index,_,size) in enumerate(adjs):\n",
        "            xs = []\n",
        "            x_target = x[:size[1]]#x should be a list prioritizing target nodes\n",
        "            x = self.convs[i]((x,x_target),edge_index)\n",
        "            if i != self.numlayers -1:\n",
        "                x = F.relu(x)\n",
        "                x = F.dropout(x,p=0.5,training=self.training)\n",
        "            xs.append(x)\n",
        "            if i == 0:\n",
        "                x_all = torch.cat(xs, dim=0)\n",
        "                layer_1_embeddings = x_all\n",
        "            elif i == 1:\n",
        "                x_all = torch.cat(xs, dim=0)\n",
        "                layer_2_embeddings = x_all\n",
        "            elif i == 2:\n",
        "                x_all = torch.cat(xs, dim=0)\n",
        "                layer_3_embeddings = x_all\n",
        "        return layer_1_embeddings, layer_2_embeddings, layer_3_embeddings\n"
      ],
      "metadata": {
        "id": "DAx3shN2__zu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SAGE(dataset.num_features, 64, dataset.num_classes, num_layers=3)\n",
        "\n",
        "model.reset_parameters()\n",
        "#We only input the initializing variables\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "v2-thI7s__xd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "CD9d_m6-P9kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = total_correct =0\n",
        "\n",
        "    for batch_size,n_id,adjs in train_loader:\n",
        "        adjs = [adj.to(device) for adj in adjs]\n",
        "        optimizer.zero_grad()\n",
        "        l1_emb, l2_emb, l3_emb = model(x[n_id], adjs)\n",
        "        out = l3_emb.log_softmax(dim=-1)\n",
        "        loss = F.nll_loss(out,y[n_id[:batch_size]])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += float(loss)\n",
        "        total_correct += int(out.argmax(dim=-1).eq(y[n_id[:batch_size]]).sum())\n",
        "\n",
        "\n",
        "    loss = total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "    approx_acc = total_correct / int(train_mask.sum())\n",
        "\n",
        "    return loss, approx_acc"
      ],
      "metadata": {
        "id": "6Bpagr0j__sx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)\n",
        "\n",
        "for epoch in range(1,20):\n",
        "    loss,acc = train(epoch)\n",
        "    print(f'Epoch {epoch:02d}, Loss: {loss:.4f}, Approx. Train: {acc:.4f}')"
      ],
      "metadata": {
        "id": "GmK6nyQG__qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CaliGraph_loader = NeighborSampler(data.edge_index,node_idx = cali_mask\n",
        "  , sizes = [5,5,5],batch_size = 5000,shuffle=False)"
      ],
      "metadata": {
        "id": "Auta1Ox8BPWC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TestGraph_loader  = NeighborSampler(data.edge_index,node_idx = test_mask\n",
        "  , sizes = [5,5,5],batch_size = 5000,shuffle=False)"
      ],
      "metadata": {
        "id": "_wPOWrbgBPTo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function used to output final layer embedding and predicted labels\n",
        "def inference_1(subgraph_loader):\n",
        "    model.eval()\n",
        "    total_loss = total_correct = 0\n",
        "    for batch_size,n_id,adjs in subgraph_loader:\n",
        "\n",
        "        emb1,emb2,emb3 = model(x[n_id],adjs)\n",
        "        out = emb3.log_softmax(dim=-1)\n",
        "        y_pred = out.argmax(dim=-1,keepdim =True)\n",
        "\n",
        "\n",
        "\n",
        "    return emb3, y_pred\n"
      ],
      "metadata": {
        "id": "tja9Ss-GBPRk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Cali_labels_idx = []\n",
        "\n",
        "for i,(batch_size,n_id,adjs) in enumerate(CaliGraph_loader):\n",
        "\n",
        "   print(batch_size)\n",
        "   print(n_id)\n",
        "   Cali_labels_idx = n_id\n",
        "print(Cali_labels_idx)"
      ],
      "metadata": {
        "id": "BYz2Yt4FBPPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use this one\n",
        "\n",
        "emb3_Cali,y_pred_Cali = inference_1(CaliGraph_loader)\n",
        "emb3_Cali = emb3_Cali.softmax(dim=-1)\n",
        "#numpy_emb3_Cali = emb3_Cali.detach().cpu().numpy()\n",
        "#row_sums = np.sum(numpy_emb3_Cali, axis=1).reshape(-1, 1)\n",
        "#emb3_Cali = numpy_emb3_Cali/row_sums\n",
        "\n",
        "\n",
        "numpy_y_pred_Cali = y_pred_Cali.detach().cpu().numpy()\n",
        "numpy_y_labels_Cali = y[cali_mask].detach().cpu().numpy()\n",
        "numpy_y_labels_Test = y[test_mask].detach().cpu().numpy()\n",
        "numpy_y_labels_column = np.array(numpy_y_labels_Cali).reshape(-1, 1)\n",
        "#accuracy of our model for calibration nodes\n",
        "acc = (numpy_y_pred_Cali == numpy_y_labels_column).sum()/cali_mask.sum()\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "Tt4iaY47GCY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.05\n",
        "n=cali_mask.sum()\n",
        "m=test_mask.sum()\n",
        "\n",
        "#Using APS score function\n",
        "cal_smx_tensor = torch.FloatTensor(emb3_Cali)\n",
        "cal_smx = cal_smx_tensor.detach().cpu().numpy()\n",
        "cal_pi = cal_smx.argsort(1)[:, ::-1]\n",
        "cal_srt = np.take_along_axis(cal_smx,cal_pi,axis=1).cumsum(axis=1)\n",
        "cal_scores = np.take_along_axis(cal_srt, cal_pi.argsort(axis=1), axis=1)[\n",
        "    range(n), numpy_y_labels_Cali\n",
        "]\n",
        "\n",
        "qhat = np.quantile(\n",
        "    cal_scores, np.ceil((n + 1) * (1 - alpha)) / n, interpolation=\"higher\"\n",
        ")"
      ],
      "metadata": {
        "id": "w_HuoW1vBPIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use this one\n",
        "\n",
        "\n",
        "emb3_Test,y_pred_Test = inference_1(TestGraph_loader)\n",
        "emb3_Test = emb3_Test.softmax(dim=-1)\n",
        "numpy_emb3_Test = emb3_Test.detach().cpu().numpy()\n",
        "test_smx = numpy_emb3_Test\n",
        "#row_sums = np.sum(numpy_emb3_Test, axis=1).reshape(-1, 1)\n",
        "#test_smx = numpy_emb3_Test/row_sums\n",
        "\n",
        "test_pi = test_smx.argsort(1)[:, ::-1]\n",
        "test_srt = np.take_along_axis(test_smx,test_pi,axis=1).cumsum(axis=1)\n",
        "\n",
        "test_scores = np.take_along_axis(test_srt, test_pi.argsort(axis=1), axis=1)[\n",
        "    range(m), numpy_y_labels_Test\n",
        "]\n",
        "\n",
        "qhat = np.quantile(\n",
        "    test_scores, np.ceil((m + 1) * (1 - alpha)) / m, interpolation=\"higher\")\n",
        "\n",
        "prediction_sets = np.take_along_axis(test_srt <= qhat, test_pi.argsort(axis=1), axis=1)"
      ],
      "metadata": {
        "id": "v348DrJUGPYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "empirical = prediction_sets[\n",
        "    np.arange(prediction_sets.shape[0]), numpy_y_labels_Test\n",
        "].mean()\n",
        "\n",
        "print(f\"The empirical coverage is: {empirical}\")"
      ],
      "metadata": {
        "id": "Vxdzf3dsBkcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we cosnider group condition covergae by self-implementing the optimization probelm, which turns to be not efficient"
      ],
      "metadata": {
        "id": "dT38GVdcZqkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#matrix_1 represents class matrix\n",
        "\n",
        "matrix_1 = np.zeros((test_mask.sum(), dataset.num_classes), dtype=int)\n",
        "\n",
        "for i in range(dataset.num_classes):\n",
        "    matrix_1[:, i] = i\n",
        "\n",
        "\n",
        "print(matrix_1)\n",
        "\n",
        "\n",
        "\n",
        "#for each column we compute S(Xi,y) for all possible y categories*****because we will use these in optimization problem\n",
        "test_scores_foreachclass=np.zeros((test_mask.sum(), dataset.num_classes))\n",
        "\n",
        "\n",
        "for i in range(dataset.num_classes):\n",
        "    test_scores = np.take_along_axis(test_srt, test_pi.argsort(axis=1), axis=1)[\n",
        "    range(m), matrix_1[:,i]]\n",
        "    test_scores_foreachclass[:,i]=test_scores\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IInDB5kqyYe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#d represents how many groups we're considering\n",
        "#simulation only\n",
        "d = 5\n",
        "\n",
        "matrix = np.zeros((n + m, d), dtype=int)\n",
        "for i in range(n + m):\n",
        "    random_col = np.random.randint(0, 5)\n",
        "    matrix[i, random_col] = 1\n",
        "\n",
        "\n",
        "\n",
        "print(matrix)"
      ],
      "metadata": {
        "id": "5y22KMQoyYc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import cvxpy as cp\n",
        "import numpy as np\n",
        "\n",
        "scores_Cali = cal_scores\n",
        "scores_Test = test_scores\n",
        "column_vector_scores_Cali = scores_Cali.reshape(-1, 1)\n",
        "column_vector_scores_Test = scores_Test.reshape(-1, 1)\n",
        "\n",
        "column_vector_scores_combined = np.concatenate(\n",
        "    (column_vector_scores_Cali, column_vector_scores_Test), axis=0\n",
        ")\n",
        "\n",
        "\n",
        "#Define the feature vector for calibration and test nodes\n",
        "X_Cali = x[cali_mask]\n",
        "X_Test = x[test_mask]\n",
        "y_Cali = y[cali_mask]\n",
        "y_Test = y[test_mask]\n",
        "\n",
        "\n",
        "\n",
        "#list of beta matrix of cardinality test_mask.sum()\n",
        "#for each matrix inside this list, we have the beta value each row represents argmin with the input S(Xn+i-1,y) for different y\n",
        "Beta_matrix = []\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hYvDZYO8m9aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Beta_matrix = []\n",
        "for i in range(test_mask.sum()):\n",
        "  beta_Xi = np.zeros((dataset.num_classes,d))\n",
        "  for j in range(dataset.num_classes):\n",
        "\n",
        "\n",
        "\n",
        "    beta = cp.Variable(d)\n",
        "    def ell_alpha(y, z):\n",
        "        return cp.maximum((y - z) * alpha, (z - y) * (1 - alpha))\n",
        "    objective = cp.Minimize(\n",
        "        (sum([ell_alpha(matrix[i] @ beta, column_vector_scores_combined[i]) for i in range(n)])\n",
        "        + ell_alpha(matrix[n] @ beta, test_scores_foreachclass[i,j])\n",
        "    )/(n+1))\n",
        "\n",
        "    problem = cp.Problem(objective)\n",
        "    problem.solve()\n",
        "\n",
        "    optimal_beta = beta.value\n",
        "    beta_Xi[j,:]=optimal_beta.reshape(1, -1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  Beta_matrix.append(beta_Xi)\n",
        "  print(i)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xzDUn0zijHbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#suppose we have already got the Beta_matrix, the next task is to output the prediction interval\n",
        "prediction_set = np.zeros((test_mask.sum(),dataset.num_classes))\n",
        "for i in range(test_mask.sum()):\n",
        "  prediction = []\n",
        "  beta_matrix = Beta_matrix[i]\n",
        "  for j in range(dataset.num_classes):\n",
        "    if(test_scores_foreachclass[i,j]<= matrix[n+j].reshape(1,-1)@(beta_matrix[j]) ):\n",
        "     prediction.append(1)\n",
        "\n",
        "    else:\n",
        "     prediction.append(0)\n",
        "\n",
        "  prediction_set[i,:]=np.array(prediction).reshape(1, -1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VEws1Kt8wZAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check empirical covergae(marginal first)\n",
        "numpy_y_Test = y_Test.detach().cpu().numpy()\n",
        "total_correctness = 0\n",
        "for i in range(test_mask.sum()):\n",
        "  for j in range(dataset.num_classes):\n",
        "    if((prediction_set[i,j] == 1 )& (numpy_y_Test[i]==j)):\n",
        "       total_correctness +=1\n",
        "\n",
        "\n",
        "empirical_coverage = total_correctness/(test_mask.sum())\n",
        "print(f\"The empirical coverage is: {empirical_coverage}\")\n",
        "\n",
        "print(f\"The empirical miscoverage is: {1-empirical_coverage}\")\n"
      ],
      "metadata": {
        "id": "IHI_2dv1-PKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_test = matrix[n:n+m]\n",
        "group_coverage_listgroup =[]\n",
        "\n",
        "#Hopefully we have achieved group conditional coverage and lets check over d groups\n",
        "for i in range(d):\n",
        "  total_correct = 0\n",
        "  num_data = 0\n",
        "  for j in range(test_mask.sum()):\n",
        "    if(matrix_test[j,i] == 1):\n",
        "      num_data+=1\n",
        "      for k in range(dataset.num_classes):\n",
        "          if(prediction_set[j,k] == 1 ):\n",
        "            if(numpy_y_Test[j]==k):\n",
        "             total_correct +=1\n",
        "\n",
        "  group_coverage = total_correct/num_data\n",
        "  group_coverage_listgroup.append(group_coverage)\n",
        "\n",
        "\n",
        "for i in range(d):\n",
        "  print(f\"The empirical group coverage for the {i+1 } th group is: {group_coverage_listgroup[i]}\")"
      ],
      "metadata": {
        "id": "FKe_s-f4-PH-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}