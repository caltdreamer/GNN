{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ivlGTD07TSsWeXl33aHlv4t6gEoh4dVY",
      "authorship_tag": "ABX9TyNbnAuA/WX6C/MkCiVOdv+S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caltdreamer/GNN/blob/main/SyntheticData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fdIrcg50AzO"
      },
      "outputs": [],
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install -q torch-geometric\n",
        "!pip install umap-learn\n",
        "!pip install conditionalconformal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/msesia/arc.git"
      ],
      "metadata": {
        "id": "7cxetL3Qjl7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GraphSAGE\n",
        "from torch_geometric.data import NeighborSampler\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.datasets import Planetoid,NELL,CitationFull,CoraFull\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.nn import BatchNorm\n",
        "import arc\n",
        "from arc import models\n",
        "from arc import methods\n",
        "from arc import black_boxes\n",
        "from arc import others\n",
        "from arc import coverage\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os.path as osp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import collections\n",
        "import random\n",
        "import networkx as nx\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from conditionalconformal.synthetic_data import generate_cqr_data, indicator_matrix\n",
        "from conditionalconformal import CondConf\n",
        "from scipy.stats import norm\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "KqxduyO9jezH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Construct Data (X,Y) 1-d case for better illustrations\n",
        "#Please Use this as illustration\n",
        "dimension=1\n",
        "def generate_cqr_data(n_total):\n",
        "\n",
        "\n",
        "    def f(x):\n",
        "\n",
        "        ax = 0*x\n",
        "        for i in range(len(x)):\n",
        "            ax[i] = np.random.poisson(np.sin(x[i])**2+0.1) + 0.03*x[i]*np.random.randn(1)\n",
        "            ax[i] += 10*(np.random.uniform(0,1,1)<0.01)*np.random.randn(1)\n",
        "        return ax.astype(np.float32)\n",
        "    \"\"\"\n",
        "    mu, sigma = 2.5, 1\n",
        "    non_uniform_random_points = np.random.normal(mu, sigma, n_total)\n",
        "    non_uniform_random_points_clipped = np.clip(non_uniform_random_points, 0, 5)\n",
        "    x_all = non_uniform_random_points_clipped\n",
        "    \"\"\"\n",
        "    x_all = np.random.uniform(0, 5.0, size=n_total).astype(np.float32)\n",
        "\n",
        "\n",
        "    y_all = f(x_all)\n",
        "    return x_all,y_all\n",
        "num_nodes = 2000\n",
        "x_all,y_all = generate_cqr_data(num_nodes)\n",
        "x_all = x_all.reshape(num_nodes,1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "UnGjNn_3b4zI",
        "outputId": "8671188f-9f9f-44dd-a8e3-b35a71264bfa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-02cdf8fc28cb>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_all\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mnum_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mx_all\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_cqr_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mx_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-02cdf8fc28cb>\u001b[0m in \u001b[0;36mgenerate_cqr_data\u001b[0;34m(n_total)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mx_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnon_uniform_random_points_clipped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \"\"\"\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mx_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#construct kernel density using gaussian kernel\n",
        "from scipy.stats import gaussian_kde\n",
        "kde = gaussian_kde(x_all.T)"
      ],
      "metadata": {
        "id": "rL7eknAbrAr_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate graph using k nearest neighbors\n",
        "def generate_data_graph(k=5):\n",
        "    X, Y = x_all,y_all\n",
        "    #X = X.reshape(-1, 1)\n",
        "    knn = NearestNeighbors(n_neighbors=k+1).fit(X)\n",
        "    adj_matrix = knn.kneighbors_graph(X, mode=\"connectivity\").toarray()\n",
        "    edge_index = np.argwhere(adj_matrix)\n",
        "    edge_index = torch.tensor(edge_index).t().long()\n",
        "\n",
        "    x_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
        "\n",
        "    data = Data(x=x_tensor, y=y_tensor, edge_index=edge_index)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "5lntUip7jU76"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = generate_data_graph(k=10)\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZmh4nUojWV_",
        "outputId": "2767dafe-4ca0-4ecb-d17c-20a25f494fec"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[2000, 1], edge_index=[2, 22000], y=[2000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "x = data.x.to(device)\n",
        "y = data.y.squeeze().to(device)"
      ],
      "metadata": {
        "id": "Y4W62RDyp3GA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot our feature versus the label\n",
        "x_first_dim = x[:, ].numpy()\n",
        "y_numpy = y.numpy()\n",
        "plt.scatter(x_first_dim[:,0], y_numpy, alpha=0.6, edgecolors=\"w\", linewidth=0.5)\n",
        "plt.xlabel(\"First Dimension of X\")\n",
        "plt.ylabel(\"Label (Y)\")\n",
        "plt.title(\"First Dimension of X vs. Label Y\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OytsBzqGqTqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kde = gaussian_kde(x.numpy().ravel())\n",
        "x_d = np.linspace(x.numpy().min(), x.numpy().max(), 1000)\n",
        "kde_values = kde(x_d)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(x_d, kde_values, label='KDE')\n",
        "plt.scatter(x.numpy(), np.full_like(x.numpy(), -0.01), alpha=0.6, color='red', label='Data points')  # Overlay the data points\n",
        "plt.title('KDE of the First Principal Component')\n",
        "plt.legend()\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Density')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yJjHqfWdQ-Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Randomly choose train and calitest set\n",
        "marginal_condi_shifts = []\n",
        "marginal_condi_kernel = []\n",
        "marginal_split = []\n",
        "ubs_condi_shifts = []\n",
        "lbs_condi_shifts = []\n",
        "ubs_condi_kernel = []\n",
        "lbs_condi_kernel = []\n",
        "ubs_condi_groups = []\n",
        "lbs_condi_groups = []\n",
        "ubs_split =[]\n",
        "lbs_split = []\n",
        "test_set_lists=[]\n",
        "q = []\n",
        "\n",
        "\n",
        "train_percentage = 0.5\n",
        "test_percentage = 0.4\n",
        "\n",
        "for c in range(15):\n",
        "    num_nodes = data.num_nodes\n",
        "    all_node_indices = list(range(num_nodes))\n",
        "    random.shuffle(all_node_indices)\n",
        "    num_train_nodes = int(train_percentage * num_nodes)\n",
        "    num_testandcali_nodes = num_nodes - num_train_nodes\n",
        "    train_node_indices = all_node_indices[:num_train_nodes]\n",
        "    testandcali_node_indices = all_node_indices[num_train_nodes:]\n",
        "\n",
        "    test_node_indices = testandcali_node_indices[:int(test_percentage * num_testandcali_nodes)]\n",
        "    cali_node_indices = testandcali_node_indices[int(test_percentage * num_testandcali_nodes):]\n",
        "\n",
        "\n",
        "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    train_mask[train_node_indices] = True\n",
        "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    train_mask[train_node_indices] = True\n",
        "\n",
        "    testandcali_mask = torch.zeros(num_nodes,dtype=torch.bool)\n",
        "    testandcali_mask[testandcali_node_indices] = True\n",
        "\n",
        "    test_mask = torch.zeros(num_nodes,dtype=torch.bool)\n",
        "    test_mask[test_node_indices] = True\n",
        "\n",
        "\n",
        "    cali_mask = torch.zeros(num_nodes,dtype=torch.bool)\n",
        "    cali_mask[cali_node_indices] = True\n",
        "    test_set_lists.append(test_mask)\n",
        "    #*********************************************\n",
        "\n",
        "    #print(train_mask)\n",
        "    #print(test_mask.sum())\n",
        "    #print(cali_mask.sum())\n",
        "    n_train =train_mask.sum()\n",
        "    n_calib = test_mask.sum()\n",
        "    n_test = cali_mask.sum()\n",
        "\n",
        "\n",
        "    #*********************************************\n",
        "    #Use same Notation as the synthetic data\n",
        "    x_train_final, y_train_final, x_calib, y_calib, x_test, y_test= x[train_mask].detach().cpu().numpy(), y[train_mask].detach().cpu().numpy(), x[cali_mask].detach().cpu().numpy(), y[cali_mask].detach().cpu().numpy(), x[test_mask].detach().cpu().numpy(), y[test_mask].detach().cpu().numpy()\n",
        "\n",
        "    x_all = np.concatenate((x_train_final, x_calib, x_test), axis=0)\n",
        "    y_all = np.concatenate((y_train_final,y_calib,y_test),axis = 0)\n",
        "    #*********************************************\n",
        "    train_loader = NeighborSampler(\n",
        "        data.edge_index, node_idx=(train_mask),\n",
        "        sizes=[3, 3, 3], batch_size=32, shuffle=False,\n",
        "    )\n",
        "    #*********************************************\n",
        "    class SAGERegressor(torch.nn.Module):\n",
        "        def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3):\n",
        "            super(SAGERegressor, self).__init__()\n",
        "\n",
        "            self.num_layers = num_layers\n",
        "            self.convs = torch.nn.ModuleList()\n",
        "            self.batch_norms = torch.nn.ModuleList()\n",
        "            self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
        "            self.batch_norms.append(BatchNorm(hidden_channels))\n",
        "\n",
        "            for i in range(num_layers - 2):\n",
        "                self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "                self.batch_norms.append(BatchNorm(hidden_channels))\n",
        "\n",
        "            self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
        "            self.batch_norms.append(BatchNorm(out_channels))\n",
        "            self.linear1 = torch.nn.Linear(out_channels, hidden_channels)\n",
        "            self.linear2 = torch.nn.Linear(hidden_channels, 1)\n",
        "\n",
        "        def reset_parameters(self):\n",
        "\n",
        "            for conv in self.convs:\n",
        "                conv.reset_parameters()\n",
        "            for bn in self.batch_norms:\n",
        "                bn.reset_parameters()\n",
        "            self.linear1.reset_parameters()\n",
        "            self.linear2.reset_parameters()\n",
        "\n",
        "\n",
        "        def forward(self, x, adjs):\n",
        "            for i, (edge_index, _, size) in enumerate(adjs):\n",
        "                x_target = x[:size[1]]\n",
        "                x = self.convs[i]((x, x_target), edge_index)\n",
        "\n",
        "                x = self.batch_norms[i](x)\n",
        "                if i != self.num_layers - 1:\n",
        "                    x = F.leaky_relu(x, 0.2)\n",
        "                    x = F.dropout(x, p=0.1, training=self.training)\n",
        "\n",
        "            x = F.leaky_relu(self.linear1(x), 0.2)\n",
        "            x = self.linear2(x)\n",
        "\n",
        "            return x\n",
        "    #*********************************************\n",
        "    #regression task\n",
        "    model = SAGERegressor(data.num_features, 2, 1, num_layers=3)\n",
        "    model.reset_parameters()\n",
        "    model = model.to(device)\n",
        "    #print(model)\n",
        "    #*********************************************\n",
        "    model.reset_parameters()\n",
        "    #*********************************************\n",
        "    optimizer = torch.optim.Adam(model.parameters(),lr = 0.005)\n",
        "    def train(epoch):\n",
        "        model.train()\n",
        "        i=0\n",
        "        for batch_size,n_id,adjs in train_loader:\n",
        "\n",
        "            adjs = [adj.to(device) for adj in adjs]\n",
        "            optimizer.zero_grad()\n",
        "            # l1_emb, l2_emb, l3_emb = model(x[n_id], adjs)\n",
        "            l3_emb = model(x[n_id], adjs)\n",
        "            loss = F.mse_loss(l3_emb,y[n_id][:batch_size].unsqueeze(1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            i+=loss\n",
        "\n",
        "        return i\n",
        "    for epoch in range(1,10):\n",
        "        loss = train(epoch)\n",
        "\n",
        "    #*********************************************\n",
        "    Calib_loader = NeighborSampler(data.edge_index,node_idx = cali_mask\n",
        "      , sizes = [3,3,3],batch_size = int((test_mask+cali_mask).sum()),shuffle=False)\n",
        "\n",
        "    Test_loader = NeighborSampler(data.edge_index,node_idx = test_mask,sizes = [3,3,3],batch_size = int((test_mask+cali_mask).sum()),shuffle=False)\n",
        "    #*********************************************\n",
        "    #function the takes graph loader to the third layer embedding and the fn which is\n",
        "    #a matrix in the form [x,predicted y]\n",
        "    def inference_1(subgraph_loader):\n",
        "        model.eval()\n",
        "        for batch_size,n_id,adjs in subgraph_loader:\n",
        "\n",
        "            #emb1,emb2,emb3 = model(x[n_id],adjs)\n",
        "            emb3 = model(x[n_id],adjs)\n",
        "\n",
        "            c = x[n_id][:batch_size]\n",
        "            fn = torch.cat((c, emb3), dim=1)\n",
        "\n",
        "\n",
        "        return emb3,fn\n",
        "    #*********************************************\n",
        "    emb3_Calib,fn_Calib = inference_1(Calib_loader)\n",
        "    emb3_Test,fn_Test =  inference_1(Test_loader)\n",
        "    fn_Calib = fn_Calib.detach().cpu().numpy()\n",
        "    fn_Test = fn_Test.detach().cpu().numpy()\n",
        "    fn = np.vstack((fn_Calib,fn_Test))\n",
        "    prediction_dimension = data.num_features\n",
        "    #print(fn[:,prediction_dimension])\n",
        "\n",
        "    #*********************************************\n",
        "    dimension = data.num_features\n",
        "    #x is input vector and output is predicted label vector\n",
        "    #Will rewrite a new version for efficiency\n",
        "    def get_xhat(x_vector):\n",
        "      if(len(x_vector.shape)==2):\n",
        "            xhat_1= np.ones((x_vector.shape[0], 1))\n",
        "            for j in range(x_vector.shape[0]):\n",
        "              for i in range((cali_mask+test_mask).sum()):\n",
        "                if(np.array_equal(x_vector[j],fn[i,0:dimension])):\n",
        "                  xhat_1[j]=fn[i,dimension]\n",
        "            return xhat_1\n",
        "      else:\n",
        "            for i in range((cali_mask+test_mask).sum()):\n",
        "              if(np.array_equal(x_vector.squeeze(),fn[i,0:dimension])):\n",
        "                return fn[i,dimension]\n",
        "    #*********************************************\n",
        "\n",
        "    #Two sided confidence interval\n",
        "    score_fn = lambda x, y : np.abs(y - get_xhat(x).squeeze())\n",
        "    score_inv_fn = lambda s, x : [get_xhat(x)-s, get_xhat(x) + s]\n",
        "    score_inv_fn_lb = lambda s, x : [get_xhat(x) + s, np.inf]\n",
        "    #*********************************************\n",
        "    def indicator_matrix(scalar_values, disc):\n",
        "        scalar_values = np.array(scalar_values)\n",
        "      # print(scalar_values.shape)\n",
        "        intervals = [(disc[i], disc[i + 1]) for i in range(len(disc) - 1)]\n",
        "\n",
        "        matrix = np.zeros((scalar_values.shape[0], len(intervals)))\n",
        "\n",
        "        for i in range(scalar_values.shape[0]):\n",
        "          value = scalar_values[i,0]\n",
        "          for j,(a,b) in enumerate(intervals):\n",
        "            if (a<= value <b):\n",
        "              matrix[i,j] = 1\n",
        "\n",
        "        return matrix\n",
        "    #*********************************************\n",
        "    #Consider covariate shift of one of the dimension\n",
        "    dimension_interest = 0\n",
        "    #*********************************************\n",
        "    eps = 1\n",
        "    disc = np.arange(0, 5 + eps, eps)\n",
        "    def phi_fn_groups(x_12):\n",
        "      if len(x_12.shape) == 2:\n",
        "        a=x_12[:x_12.shape[0],dimension_interest]\n",
        "        a = a.reshape(-1,1)\n",
        "        return indicator_matrix(a, disc)\n",
        "      else:\n",
        "        a = x_12.reshape(-1,1)\n",
        "        a = a[dimension_interest]\n",
        "        a = a.reshape(-1,1)\n",
        "        return indicator_matrix(a, disc)\n",
        "\n",
        "\n",
        "    eval_locs = [1.5,3.5]\n",
        "    eval_scale = 0.5\n",
        "\n",
        "    other_locs = [0.5,2.5,4.5]\n",
        "    other_scale = 1\n",
        "\n",
        "    def phi_fn_shifts(x_13):\n",
        "        if(len(x_13.shape) == 2):\n",
        "          a = x_13[:x_13.shape[0],dimension_interest]\n",
        "\n",
        "          shifts = [norm.pdf(a, loc=loc, scale=eval_scale).reshape(-1,1)\n",
        "                        for loc in eval_locs]\n",
        "          shifts.extend([norm.pdf(a, loc=loc, scale=other_scale).reshape(-1,1)\n",
        "                        for loc in other_locs])\n",
        "          shifts.append(np.ones((a.shape[0], 1)))\n",
        "          #print(np.concatenate(shifts, axis=1).shape)\n",
        "          return np.concatenate(shifts, axis=1)\n",
        "\n",
        "\n",
        "        else:\n",
        "          a = x_13.reshape(-1,1)\n",
        "          a = a[dimension_interest]\n",
        "          shifts = [norm.pdf(a, loc=loc, scale=eval_scale).reshape(-1,1)\n",
        "                        for loc in eval_locs]\n",
        "          shifts.extend([norm.pdf(a, loc=loc, scale=other_scale).reshape(-1,1)\n",
        "                        for loc in other_locs])\n",
        "          shifts.append(np.ones((a.shape[0], 1)))\n",
        "          #print(np.concatenate(shifts, axis=1).shape)\n",
        "          return np.concatenate(shifts, axis=1)\n",
        "    def phi_n_intercept(x_14):\n",
        "            if(len(x_14.shape)==2):\n",
        "\n",
        "              return np.ones_like(x_14)\n",
        "            else:\n",
        "              return np.ones_like(x_14).reshape(-1,1)\n",
        "\n",
        "\n",
        "    def phi_kernel_estimation(x_15):\n",
        "            if(len(x_15.shape) == 2):\n",
        "              ones_vector = np.ones((x_15.shape[0],1))\n",
        "              new_data_point_std = x_15.reshape(x_15.shape[0],data.num_features)\n",
        "              return np.column_stack((kde(new_data_point_std.T).reshape(x_15.shape[0],1),ones_vector))\n",
        "\n",
        "            else:\n",
        "              ones_vector = np.ones((1,1))\n",
        "              new_data_point_std = x_15.reshape(1,data.num_features)\n",
        "              return np.column_stack((kde(new_data_point_std).reshape(1,1),ones_vector))\n",
        "\n",
        "    #*********************************************\n",
        "    #Set up the problem by shifts\n",
        "\n",
        "    infinite_params = {}\n",
        "    cond_conf = CondConf(score_fn, phi_fn_shifts, infinite_params)\n",
        "    cond_conf.setup_problem(x_calib, y_calib)\n",
        "\n",
        "    n_test = len(x_test)\n",
        "    alpha = 0.05\n",
        "    lbs_shifts = np.zeros((n_test,))\n",
        "    ubs_shifts = np.zeros((n_test,))\n",
        "    i = 0\n",
        "    for x_t in tqdm(x_test):\n",
        "        res = cond_conf.predict(1-alpha, x_t, score_inv_fn)\n",
        "        lbs_shifts[i] = res[0]\n",
        "        ubs_shifts[i] = res[1]\n",
        "        i += 1\n",
        "    #*********************************************\n",
        "    #Set up the problem by kernel\n",
        "    infinite_params = {}\n",
        "    cond_conf = CondConf(score_fn, phi_kernel_estimation, infinite_params)\n",
        "    cond_conf.setup_problem(x_calib, y_calib)\n",
        "\n",
        "    n_test = len(x_test)\n",
        "    alpha = 0.05\n",
        "    lbs_kernel = np.zeros((n_test,))\n",
        "    ubs_kernel = np.zeros((n_test,))\n",
        "    i = 0\n",
        "    for x_t in tqdm(x_test):\n",
        "        res = cond_conf.predict(1-alpha, x_t, score_inv_fn)\n",
        "        ubs_kernel[i] = res[1]\n",
        "        lbs_kernel[i] = res[0]\n",
        "        i += 1\n",
        "    #*********************************************\n",
        "    #Set up the problem by groups\n",
        "\n",
        "    infinite_params = {}\n",
        "    cond_conf = CondConf(score_fn, phi_fn_groups, infinite_params)\n",
        "    cond_conf.setup_problem(x_calib, y_calib)\n",
        "\n",
        "    n_test = len(x_test)\n",
        "    alpha = 0.05\n",
        "    lbs_groups = np.zeros((n_test,))\n",
        "    ubs_groups = np.zeros((n_test,))\n",
        "    i = 0\n",
        "    for x_t in tqdm(x_test):\n",
        "        res = cond_conf.predict(1-alpha, x_t, score_inv_fn)\n",
        "        ubs_groups[i] = res[1]\n",
        "        lbs_groups[i] = res[0]\n",
        "        i += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #*********************************************\n",
        "    def get_marginalcoverage_condi(ubs_1,lbs_1):\n",
        "      j=0\n",
        "      for i in range(len(x_test)):\n",
        "        if((y_test[i]<=ubs_1[i]) & (y_test[i]>=lbs_1[i])):\n",
        "          j+=1\n",
        "\n",
        "      return j/len(x_test)\n",
        "    def get_marginalcoverage_split():\n",
        "      j=0\n",
        "      x_hat = get_xhat(x_test)\n",
        "      for i in range(len(x_test)):\n",
        "        if((y_test[i]>=x_hat[i]-q )& (y_test[i]<= x_hat[i]+q)):\n",
        "          j+=1\n",
        "\n",
        "      return j/len(x_test)\n",
        "    def evaluate_predictions(S, X, y):\n",
        "        marg_coverage = np.mean([y[i] in S[i] for i in range(len(y))])\n",
        "        wsc_coverage = coverage.wsc_unbiased(X, y, S)\n",
        "        print('Marginal coverage:       {:2.3%}'.format(marg_coverage))\n",
        "        print('WS conditional coverage: {:2.3%}'.format(wsc_coverage))\n",
        "\n",
        "    q = np.quantile(np.abs(get_xhat(x_calib) - y_calib.reshape(-1,1)),\n",
        "                    np.ceil((len(x_calib) + 1) * (0.95)) / len(x_calib),)\n",
        "\n",
        "\n",
        "    ubs = (get_xhat(x_test)+q).squeeze()\n",
        "    lbs = (get_xhat(x_test)-q).squeeze()\n",
        "    test_indices = test_mask\n",
        "    intervals = [pd.Interval(left=lb, right=ub, closed='right') for lb, ub in zip(lbs, ubs)]\n",
        "    print(f\"The empirical marginal and WS conditional coverage using split conformal is: \")\n",
        "    print(evaluate_predictions(intervals,x[test_indices].detach().cpu().numpy(),y[test_indices].detach().cpu().numpy()))\n",
        "\n",
        "\n",
        "    ubs = ubs_shifts\n",
        "    lbs = lbs_shifts\n",
        "    test_indices = test_mask\n",
        "    intervals = [pd.Interval(left=lb, right=ub, closed='right') for lb, ub in zip(lbs, ubs)]\n",
        "    print(f\"The empirical marginal and WS conditional coverage using gaussian shifts is: \")\n",
        "    print(evaluate_predictions(intervals,x[test_indices].detach().cpu().numpy(),y[test_indices].detach().cpu().numpy()))\n",
        "    marginal_coverage_split = get_marginalcoverage_split()\n",
        "    marginal_coverage_condi_shifts = get_marginalcoverage_condi(ubs_shifts,lbs_shifts)\n",
        "    marginal_coverage_condi_kernel = get_marginalcoverage_condi(ubs_kernel,lbs_kernel)\n",
        "\n",
        "    ubs = ubs_groups\n",
        "    lbs = lbs_groups\n",
        "    test_indices = test_mask\n",
        "    intervals = [pd.Interval(left=lb, right=ub, closed='right') for lb, ub in zip(lbs, ubs)]\n",
        "    print(f\"The empirical marginal and WS conditional coverage using group method is: \")\n",
        "    print(evaluate_predictions(intervals,x[test_indices].detach().cpu().numpy(),y[test_indices].detach().cpu().numpy()))\n",
        "\n",
        "    ubs = ubs_kernel\n",
        "    lbs = lbs_kernel\n",
        "    test_indices = test_mask\n",
        "    intervals = [pd.Interval(left=lb, right=ub, closed='right') for lb, ub in zip(lbs, ubs)]\n",
        "    print(f\"The empirical marginal and WS conditional coverage using kernel method is: \")\n",
        "    print(evaluate_predictions(intervals,x[test_indices].detach().cpu().numpy(),y[test_indices].detach().cpu().numpy()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #marginal_coverage_split = get_marginalcoverage_split()\n",
        "    #marginal_coverage_condi_shifts = get_marginalcoverage_condi(ubs_shifts,lbs_shifts)\n",
        "    #marginal_coverage_condi_kernel = get_marginalcoverage_condi(ubs_kernel,lbs_kernel)\n",
        "    #marginal_coverage_condi_groups = get_marginalcoverage_condi(ubs_groups,lbs_groups)\n",
        "    #marginal_condi_shifts.append(marginal_coverage_condi_shifts)\n",
        "    #marginal_condi_kernel.append(marginal_coverage_condi_kernel)\n",
        "    #marginal_split.append(marginal_coverage_split)\n",
        "    ubs_condi_shifts.append(ubs_shifts)\n",
        "    lbs_condi_shifts.append(lbs_shifts)\n",
        "    ubs_condi_kernel.append(ubs_kernel)\n",
        "    lbs_condi_kernel.append(lbs_kernel)\n",
        "    ubs_split.append(get_xhat(x_test)+q)\n",
        "    lbs_split.append(get_xhat(x_test)-q)\n",
        "    ubs_condi_groups.append(ubs_groups)\n",
        "    lbs_condi_groups.append(lbs_groups)\n"
      ],
      "metadata": {
        "id": "mCdF1clDpvxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = x[test_set_lists[0]]\n",
        "y_test = y[test_set_lists[0]]\n",
        "ubs = ubs_condi_shifts[0]\n",
        "lbs = lbs_condi_shifts[0]"
      ],
      "metadata": {
        "id": "FRCQ-Fx0Orzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = np.quantile(np.abs(get_xhat(x_calib) - y_calib.reshape(-1,1)),\n",
        "                np.ceil((len(x_calib) + 1) * (0.95)) / len(x_calib),)\n",
        "cp = sns.color_palette()\n",
        "sns.set(font=\"DejaVu Sans\")\n",
        "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
        "fig = plt.figure()\n",
        "fig.set_size_inches(17.5, 6)\n",
        "sort_order = np.argsort(x_test[0:n_test,dimension_interest])\n",
        "x_test_s = x_test[0:n_test,dimension_interest][sort_order]\n",
        "y_test_s = y_test[sort_order]\n",
        "y_test_hat = get_xhat(x_test[sort_order])\n",
        "lb = lbs[sort_order]\n",
        "ub = ubs[sort_order]\n",
        "\n",
        "ax1 = fig.add_subplot(1, 3, 1)\n",
        "ax1.plot(x_test_s, y_test_s, '.', alpha=0.2)\n",
        "ax1.plot(x_test_s, y_test_hat, lw=1, color='k')\n",
        "ax1.plot(x_test_s, y_test_hat + q, color=cp[0], lw=2)\n",
        "ax1.plot(x_test_s, y_test_hat - q, color=cp[0], lw=2)\n",
        "\n",
        "ax1.set_ylim(-5,15)\n",
        "ax1.tick_params(axis='both', which='major', labelsize=14)\n",
        "ax1.set_xlabel(\"$X$\", fontsize=16, labelpad=10)\n",
        "ax1.set_ylabel(\"$Y$\", fontsize=16, labelpad=10)\n",
        "ax1.set_title(\"Split Conformal\", fontsize=18, pad=12)\n",
        "ax1.fill_between(x_test_s.flatten(), y_test_hat.flatten() - q, y_test_hat.flatten() + q,\n",
        "                 color=cp[3], alpha=0.4, label='split prediction interval')\n",
        "\n",
        "\n",
        "ax2 = fig.add_subplot(1, 3, 2, sharex = ax1, sharey = ax1)\n",
        "ax2.plot(x_test_s, y_test_s, '.', alpha=0.2)\n",
        "ax2.plot(x_test_s, y_test_hat, color='k', lw=1)\n",
        "ax2.plot(x_test_s, ub, color=cp[1], lw=2)\n",
        "ax2.plot(x_test_s, lb, color=cp[1], lw=2)\n",
        "ax2.fill_between(x_test_s.flatten(), lb,\n",
        "                 ub,\n",
        "                 color=cp[3], alpha=0.4, label='conditional calibration')\n",
        "ax2.tick_params(axis='both', which='major', direction='out', labelsize=14)\n",
        "ax2.set_xlabel(\"$X$\", fontsize=16, labelpad=10)\n",
        "ax2.set_ylabel(\"$Y$\", fontsize=16, labelpad=10)\n",
        "ax2.set_title(\"Conditional Calibration\", fontsize=18, pad=12)"
      ],
      "metadata": {
        "id": "BLsPEjDPpvut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "def serialize_ndarrays(list_of_ndarrays):\n",
        "    return '\\n\\n'.join('\\n'.join(' '.join(str(cell) for cell in row) for row in arr) if arr.ndim > 1 else ' '.join(str(cell) for cell in arr) for arr in list_of_arrays)\n",
        "ubs_condi_shifts_string = serialize_ndarrays(ubs_condi_shifts)\n",
        "lbs_condi_shifts_string = serialize_ndarrays(lbs_condi_shifts)\n",
        "ubs_condi_groups_string = serialize_ndarrays(ubs_condi_kernel)\n",
        "lbs_condi_groups_string = serialize_ndarrays(lbs_condi_kernel)\n",
        "marginal_condi_shifts_string = serialize_ndarrays(marginal_condi_shifts)\n",
        "marginal_condi_groups_string = serialize_ndarrays(marginal_condi_kernel)\n",
        "marginal_split_string = serialize_ndarrays(marginal_split)\n",
        "ubs_split_string =  serialize_ndarrays(ubs_split)\n",
        "lbs_split_string = serialize_ndarrays(lbs_split)\n",
        "# Path to your Googleserialize_ndarrayserialize_ndarrays(ubs_split)s(ubs_condi_shifts) Drive\n",
        "file_path = '/content/drive/My Drive/synthetic data_10runs.txt'\n",
        "with open(file_path, 'w') as f:\n",
        "    f.write(ubs_condi_shifts_string)\n",
        "    f.write(lbs_condi_shifts_string)\n",
        "    f.write(ubs_condi_groups_string)\n",
        "    f.write(lbs_condi_groups_string)\n",
        "    f.write(marginal_condi_shifts_string)\n",
        "    f.write(marginal_condi_groups_string)\n",
        "    f.write(marginal_split_string)\n",
        "    f.write(ubs_split_string)\n",
        "    f.write(lbs_split_string)"
      ],
      "metadata": {
        "id": "STezX4G3OiSN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}