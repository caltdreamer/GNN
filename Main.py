# -*- coding: utf-8 -*-
"""Main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oK7evrTS6W6VcLiH0Swjw7mR6pHEm1vj
"""

# Install required packages.
import os
import torch
os.environ['TORCH'] = torch.__version__
print(torch.__version__)

!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html
!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html
!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git
!pip install -q torch-geometric
!pip install ogb
!pip install umap-learn
!pip install conditionalconformal

import torch
import Functions
from Functions import split_data,get_xhat,cond_run,get_prediction_sets,phi_n,get_srt,get_pi,load_regression_dataset,load_classification_dataset,evaluate_predictions
from Train import train_regression,train_classification, inference_regression,inference_classification
import Train
from Train import train_classification
from torch.optim import Adam
from Model import SAGERegressor,SAGE
from torch_geometric.datasets import Planetoid, Amazon, Coauthor
import arc
import arc
from arc import models
from arc import methods
from arc import black_boxes
from arc import others
from arc import coverage
from scipy.stats import gaussian_kde
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from torch_geometric.loader import NeighborSampler
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from scipy.stats import gaussian_kde
import numpy as np
from tqdm import tqdm
from conditionalconformal import CondConf
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

def run_experiment(dataset, device, train_percentage=0.3, test_percentage_overcaliandtest=0.5,task = 'regression',rounds=10,alpha = 0.05):
      if(task == 'regression'):

        data = dataset
        x = data.x.to(device)
        y = data.y.squeeze().to(device)
        data_original = data.x
        scaler = StandardScaler()
        data_4d_std = scaler.fit_transform(data_original)
        pca = PCA(n_components=3)
        data_reduced = pca.fit_transform(data_4d_std)
        kde = gaussian_kde(data_reduced.T)
        def phi_kernel_estimation(x_15):
            if(len(x_15.shape) == 2):
              ones_vector = np.ones((x_15.shape[0],1))
              new_data_point_std = x_15.reshape(x_15.shape[0],data.num_features)
              projected_vector = pca.transform(new_data_point_std)
              return np.column_stack((kde(projected_vector.T).reshape(x_15.shape[0],1),ones_vector))

            else:
              ones_vector = np.ones((1,1))
              new_data_point_std = x_15.reshape(1,data.num_features)
              projected_vector = pca.transform(new_data_point_std)

              return np.column_stack((kde(projected_vector).reshape(1,1),ones_vector))

        for n in range(rounds):

            train_mask, test_mask, cali_mask = split_data(data.num_nodes, train_percentage,test_percentage_overcaliandtest)
            test_mask_list.append(test_mask)
            train_loader = NeighborSampler(data.edge_index, node_idx=(train_mask),sizes=[3, 3, 3], batch_size=32, shuffle=False)
            x_train_final, y_train_final, x_calib, y_calib, x_test, y_test= x[train_mask].detach().cpu().numpy(), y[train_mask].detach().cpu().numpy(), x[cali_mask].detach().cpu().numpy(), y[cali_mask].detach().cpu().numpy(), x[test_mask].detach().cpu().numpy(), y[test_mask].detach().cpu().numpy()
            model = SAGERegressor(data.num_features, 3, 1, num_layers=3)
            model.reset_parameters()
            model = model.to(device)
            model.reset_parameters()
            optimizer = torch.optim.Adam(model.parameters(),lr = 0.002)

            for epoch in range(30):
                loss = train_regression(model, data, train_loader, optimizer, device)

            Calib_loader = NeighborSampler(data.edge_index,node_idx = cali_mask, sizes = [3,3,3],batch_size = int((test_mask+cali_mask).sum()),shuffle=False)
            Test_loader = NeighborSampler(data.edge_index,node_idx = test_mask,sizes = [3,3,3],batch_size = int((test_mask+cali_mask).sum()),shuffle=False)

            emb3_Calib,fn_Calib = inference_regression(Calib_loader,model,x)
            emb3_Test,fn_Test = inference_regression(Test_loader,model,x)
            fn_Calib = fn_Calib.detach().cpu().numpy()
            fn_Test = fn_Test.detach().cpu().numpy()
            fn = np.vstack((fn_Calib,fn_Test))
            dimension = data.num_features
            q = np.quantile(np.abs(get_xhat(x_calib,cali_mask,test_mask,dimension,fn) - y_calib.reshape(-1,1)),
                        np.ceil((len(x_calib) + 1) * (1-alpha)) / len(x_calib),)
            score_fn = lambda x, y : np.abs(y - get_xhat(x,cali_mask,test_mask,dimension,fn).squeeze())
            score_inv_fn = lambda s, x : [get_xhat(x,cali_mask,test_mask,dimension,fn)-s, get_xhat(x,cali_mask,test_mask,dimension,fn) + s]
            infinite_params = {}
            cond_conf = CondConf(score_fn, phi_kernel_estimation,infinite_params)
            cond_conf.setup_problem(x_calib, y_calib)
            ubs_kernel,lbs_kernel = cond_run(cond_conf,x_test,score_inv_fn,alpha)
            ubs_split,lbs_split = (get_xhat(x_test,cali_mask,test_mask,dimension,fn)+q).squeeze(),(get_xhat(x_test,cali_mask,test_mask,dimension,fn)-q).squeeze()


            ubs_split_list.append(ubs_split)
            lbs_split_list.append(lbs_split)
            ubs_kernel_list.append(ubs_kernel)
            lbs_kernel_list.append(lbs_kernel)



      else:
        data = dataset[0]
        x = data.x.to(device)
        y = data.y.squeeze().to(device)
        for round in range(rounds):
          train_mask, test_mask, cali_mask = split_data(data.num_nodes, train_percentage,test_percentage_overcaliandtest)
          test_mask_list.append(test_mask)
          train_loader = NeighborSampler(data.edge_index, node_idx=(train_mask),sizes=[3, 3, 3], batch_size=32, shuffle=False)
          x_train_final, y_train_final, x_calib, y_calib, x_test, y_test= x[train_mask].detach().cpu().numpy(), y[train_mask].detach().cpu().numpy(), x[cali_mask].detach().cpu().numpy(), y[cali_mask].detach().cpu().numpy(), x[test_mask].detach().cpu().numpy(), y[test_mask].detach().cpu().numpy()
          model=SAGE(dataset.num_features, 128, dataset.num_classes, num_layers=3)
          model.reset_parameters()
          model = model.to(device)
          model.reset_parameters()
          optimizer = torch.optim.Adam(model.parameters(),lr = 0.002)
          def train(epoch):
            model.train()
            total_loss = total_correct =0

            for batch_size,n_id,adjs in train_loader:
                adjs = [adj.to(device) for adj in adjs]
                optimizer.zero_grad()
                l1_emb, l2_emb, l3_emb = model(x[n_id], adjs)
                out = l3_emb.log_softmax(dim=-1)
                loss = F.nll_loss(out,y[n_id[:batch_size]])
                loss.backward()
                optimizer.step()

                total_loss += float(loss)
                total_correct += int(out.argmax(dim=-1).eq(y[n_id[:batch_size]]).sum())
            loss = total_loss / len(train_loader)
            approx_acc = total_correct / int(train_mask.sum())
            return loss, approx_acc
          for epoch in range(30):
                loss,acc = train(epoch)
          CaliGraph_loader = NeighborSampler(data.edge_index,node_idx = cali_mask,sizes = [3,3,3],batch_size = 100000,shuffle=False)
          TestGraph_loader  = NeighborSampler(data.edge_index,node_idx = test_mask, sizes = [3,3,3],batch_size = 100000,shuffle=False)
          fn_cali,emb3_Cali,y_pred_Cali = inference_classification(CaliGraph_loader,model,x,y,train_mask)
          fn_test,emb3_Test,y_pred_Test = inference_classification(TestGraph_loader,model,x,y,train_mask)
          #Use this one
          prediction_sets_split = get_prediction_sets(alpha,emb3_Cali,y_pred_Cali,emb3_Test,y_pred_Test,int(cali_mask.sum()),int(test_mask.sum()),cali_mask,test_mask,y)
          x_caliandtest = np.vstack((emb3_Cali.detach().cpu().numpy(),emb3_Test.detach().cpu().numpy()))
          fn = np.vstack((fn_cali.detach().cpu().numpy(),fn_test.detach().cpu().numpy()))
          num_features = data.num_features
          n_calib,n_test = int(cali_mask.sum()),int(test_mask.sum())
          num_classes = dataset.num_classes
          matrix_1 = np.zeros((n_calib+n_test, num_classes),dtype=int)
          for i in range(dataset.num_classes):
              matrix_1[:, i] = i
          caliandtest_srt = np.vstack((get_srt(emb3_Cali), get_srt(emb3_Test)))
          caliandtest_pi = np.vstack((get_pi(emb3_Cali),get_pi(emb3_Test)))
          test_scores_foreachclass=np.zeros((n_calib+n_test, dataset.num_classes))
          for i in range(dataset.num_classes):
              test_scores = (np.take_along_axis(caliandtest_srt , caliandtest_pi.argsort(axis=1), axis=1)[
              range(n_calib+n_test), matrix_1[:,i]])
              test_scores_foreachclass[:,i]=test_scores
          def score_fn(x_1, y):
              if x_1.shape[1] != 1:
                  return np.array([test_scores_foreachclass[i, y[i]] for i in range(x_1.shape[0])]).squeeze()
              else:
                  return test_scores_foreachclass[np.where(np.all(x_1 == x_caliandtest, axis=1))[0][0], y]
          def score_inv_fn(s, x_3):
              i = np.where(np.all(x_3.squeeze() == x_caliandtest, axis=1))[0][0]
              return np.take_along_axis(caliandtest_srt[i].reshape(1, dataset.num_classes) <= s, caliandtest_pi[i].reshape(1, dataset.num_classes).argsort(axis=1), axis=1)
          x_calib = x_caliandtest[:n_calib]
          y_calib = y[cali_mask].detach().cpu().numpy()
          x_test = x_caliandtest[-n_test:]
          y_test = y[test_mask].detach().cpu().numpy()
          infinite_params = {}
          cond_conf = CondConf(score_fn, phi_n, infinite_params)
          cond_conf.setup_problem(x_calib, y_calib)
          prediction_sets_test = []
          u=0
          for x_t in tqdm(x_test):
              res = cond_conf.predict(1-alpha, x_t, score_inv_fn)
              prediction_sets_test.append(res)
              u=u+1
          prediction_sets_test = np.vstack(prediction_sets_test)

          prediction_sets_condi_list.append(prediction_sets_test)
          prediction_sets_split_list.append(prediction_sets_split)

#To check the marginal and the WS Conditional Coverage
#Example running regression dataset
ubs_split_list=[]
lbs_split_list=[]
ubs_kernel_list=[]
lbs_kernel_list=[]
test_mask_list=[]

dataset = load_regression_dataset('ChicagoSketch',2176)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
run_experiment(dataset, device, train_percentage=0.4, test_percentage_overcaliandtest=0.5,task = 'regression',rounds=1,alpha = 0.05)
rounds = 1
data_regression = dataset
x = data_regression.x
y = data_regression.y
for round in range(rounds):
    ubs = ubs_kernel_list[round]
    lbs = lbs_kernel_list[round]
    test_indices = test_mask_list[round]
    intervals = [pd.Interval(left=lb, right=ub, closed='right') for lb, ub in zip(lbs, ubs)]
    print(f"The empirical marginal and WS conditional coverage using kernel density estimation is: ")
    print(evaluate_predictions(intervals,x[test_indices].detach().cpu().numpy(),y[test_indices].detach().cpu().numpy()))
    ubs = ubs_split_list[round].squeeze()
    lbs = lbs_split_list[round].squeeze()
    intervals = [pd.Interval(left=lb, right=ub, closed='right') for lb, ub in zip(lbs, ubs)]
    print(f"The empirical marginal and WS conditional coverage using Split conformal is: ")
    print(evaluate_predictions(intervals,x[test_indices].detach().cpu().numpy(),y[test_indices].detach().cpu().numpy()))

#example running classification dataset
prediction_sets_condi_list=[]
prediction_sets_split_list=[]
test_mask_list=[]
dataset = load_classification_dataset('Cora')
data_classification = dataset[0]
x = data_classification.x
y = data_classification.y
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
run_experiment(dataset, device, train_percentage=0.3, test_percentage_overcaliandtest=0.5,task = 'classification',rounds=10,alpha = 0.05)


data = dataset[0]
def evaluate_predictions(S, X, y):
        marg_coverage = np.mean([y[i] in S[i] for i in range(len(y))])
        wsc_coverage = coverage.wsc_unbiased(X, y, S)
        print('Marginal coverage:       {:2.3%}'.format(marg_coverage))
        print('WS conditional coverage: {:2.3%}'.format(wsc_coverage))
def indices_of_nonzero_entries(binary_matrix):
# Using list comprehension to find the indices of non-zero entries for each row
    indices_matrix = [np.flatnonzero(row) for row in binary_matrix]
    return indices_matrix

#Rounds should match the experiments rounds
rounds = 10
for round in range(rounds):
      test_mask = test_mask_list[round]
      prediction_sets_condi = prediction_sets_condi_list[round]
      prediction_sets_split = prediction_sets_split_list[round]


      int_matrix =prediction_sets_condi_list[round].astype(int)
      indices_matrix = indices_of_nonzero_entries(int_matrix)
      print(f"The empirical marginal and WS conditional coverage using conditional conformal is: ")
      print(evaluate_predictions(indices_matrix,x[test_mask].detach().cpu().numpy(),y[test_mask].detach().cpu().numpy()))
      int_matrix =prediction_sets_split_list[round].astype(int)
      indices_matrix = indices_of_nonzero_entries(int_matrix)
      print(f"The empirical marginal and WS conditional coverage using split conformal is: ")
      print(evaluate_predictions(indices_matrix,x[test_mask].detach().cpu().numpy(),y[test_mask].detach().cpu().numpy()))

#Illustration
#Example showing the plot of the first result setting the round =0
round=0
test_mask = test_mask_list[round]
x_test = x[test_mask]
y_test = y[test_mask]
n_test = x_test.shape[0]
dimension_interest = 2
ubs_kernel = ubs_kernel_list[round]
lbs_kernel = ubs_kernel_list[round]
ubs_split = ubs_split_list[round]
lbs_split = lbs_split_list[round]


cp = sns.color_palette()
sns.set(font="DejaVu Sans")
sns.set_style("whitegrid", {'axes.grid' : False})
fig = plt.figure()
fig.set_size_inches(17.5, 6)
sort_order = np.argsort(x_test[0:n_test,dimension_interest])
x_test_s = x_test[0:n_test,dimension_interest][sort_order]
y_test_s = y_test[sort_order]
y_test_hat = ubs_split-((ubs_split[0]-lbs_split[0])/2)
q = (ubs_split[0]-lbs_split[0])/2
lb = lbs[sort_order]
ub = ubs[sort_order]

ax1 = fig.add_subplot(1, 3, 1)
ax1.plot(x_test_s.flatten(), y_test_s.flatten(), '.', alpha=0.2)
ax1.plot(x_test_s.flatten(), y_test_hat.flatten(), lw=1, color='k')
ax1.plot(x_test_s.flatten(), y_test_hat.flatten() + q, color=cp[0], lw=2)
ax1.plot(x_test_s.flatten(), y_test_hat.flatten() - q, color=cp[0], lw=2)
ax1.set_ylim(-5,10)
ax1.tick_params(axis='both', which='major', labelsize=14)
ax1.set_xlabel("$X$", fontsize=16, labelpad=10)
ax1.set_ylabel("$Y$", fontsize=16, labelpad=10)
ax1.set_title("Split Conformal", fontsize=18, pad=12)
ax1.fill_between(x_test_s.flatten(), y_test_hat.flatten() - q, y_test_hat.flatten() + q,
                 color=cp[3], alpha=0.4, label='split prediction interval')
ax2 = fig.add_subplot(1, 3, 2, sharex = ax1, sharey = ax1)
ax2.plot(x_test_s.flatten(), y_test_s.flatten(), '.', alpha=0.2)
ax2.plot(x_test_s.flatten(), y_test_hat.flatten(), color='k', lw=1)
ax2.plot(x_test_s.flatten(), ub, color=cp[1], lw=2)
ax2.plot(x_test_s.flatten(), lb, color=cp[1], lw=2)
ax2.fill_between(x_test_s.flatten(), lb,
                 ub,
                 color=cp[3], alpha=0.4, label='conditional calibration')
ax2.tick_params(axis='both', which='major', direction='out', labelsize=14)
ax2.set_xlabel("$X$", fontsize=16, labelpad=10)
ax2.set_ylabel("$Y$", fontsize=16, labelpad=10)
ax2.set_title("Conditional Calibration", fontsize=18, pad=12)