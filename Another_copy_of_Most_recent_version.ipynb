{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caltdreamer/GNN/blob/main/Another_copy_of_Most_recent_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcX285LF1Ngv"
      },
      "outputs": [],
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install -q torch-geometric\n",
        "!pip install umap-learn\n",
        "!git clone https://github.com/msesia/arc.git\n",
        "!pip install conditionalconformal\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V7BhvS6Ou9Gm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b852606-b982-4b95-9cab-9281800c7391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r /content/conformalized-gnn-master/requirements.txt (line 1)) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r /content/conformalized-gnn-master/requirements.txt (line 2)) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r /content/conformalized-gnn-master/requirements.txt (line 3)) (1.5.3)\n",
            "Collecting wandb (from -r /content/conformalized-gnn-master/requirements.txt (line 4))\n",
            "  Downloading wandb-0.16.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r /content/conformalized-gnn-master/requirements.txt (line 5)) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r /content/conformalized-gnn-master/requirements.txt (line 6)) (1.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r /content/conformalized-gnn-master/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r /content/conformalized-gnn-master/requirements.txt (line 3)) (2023.3.post1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r /content/conformalized-gnn-master/requirements.txt (line 4)) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r /content/conformalized-gnn-master/requirements.txt (line 4))\n",
            "  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r /content/conformalized-gnn-master/requirements.txt (line 4)) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r /content/conformalized-gnn-master/requirements.txt (line 4)) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb->-r /content/conformalized-gnn-master/requirements.txt (line 4))\n",
            "  Downloading sentry_sdk-1.39.2-py2.py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->-r /content/conformalized-gnn-master/requirements.txt (line 4))\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb->-r /content/conformalized-gnn-master/requirements.txt (line 4)) (6.0.1)\n",
            "Collecting setproctitle (from wandb->-r /content/conformalized-gnn-master/requirements.txt (line 4))\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r /content/conformalized-gnn-master/requirements.txt (line 4)) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->-r /content/conformalized-gnn-master/requirements.txt (line 4)) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r /content/conformalized-gnn-master/requirements.txt (line 4)) (3.20.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r /content/conformalized-gnn-master/requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r /content/conformalized-gnn-master/requirements.txt (line 6)) (3.2.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->-r /content/conformalized-gnn-master/requirements.txt (line 4)) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r /content/conformalized-gnn-master/requirements.txt (line 4))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r /content/conformalized-gnn-master/requirements.txt (line 4)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r /content/conformalized-gnn-master/requirements.txt (line 4)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r /content/conformalized-gnn-master/requirements.txt (line 4)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r /content/conformalized-gnn-master/requirements.txt (line 4)) (2023.11.17)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r /content/conformalized-gnn-master/requirements.txt (line 4))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.41 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.39.2 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.2\n",
            "/content/conformalized-gnn-master\n"
          ]
        }
      ],
      "source": [
        "!pip install -r /content/conformalized-gnn-master/requirements.txt\n",
        "%cd /content/conformalized-gnn-master\n",
        "!mkdir -p /content/conformalized-gnn-master/model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GraphSAGE\n",
        "from torch_geometric.data import NeighborSampler\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.datasets import NELL\n",
        "from torch_geometric.datasets import CitationFull\n",
        "from torch_geometric.datasets import CoraFull\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.data import Data\n",
        "from tqdm import tqdm\n",
        "from torch_geometric.nn import SAGEConv, BatchNorm\n",
        "import arc\n",
        "from arc import models\n",
        "from arc import methods\n",
        "from arc import black_boxes\n",
        "from arc import others\n",
        "from arc import coverage\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os.path as osp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import collections\n",
        "import random\n",
        "import networkx as nx\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "from numpy.ma.core import maximum\n",
        "from conditionalconformal.synthetic_data import generate_cqr_data, indicator_matrix\n",
        "from conditionalconformal import CondConf\n",
        "\n",
        "from scipy.stats import norm\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import gaussian_kde\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, SGConv\n",
        "\n"
      ],
      "metadata": {
        "id": "0sPBFV-BCQxc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "def wsc(X, y, S, delta=0.1, M=1000, random_state=2020, verbose=False):\n",
        "    rng = np.random.default_rng(random_state)\n",
        "\n",
        "    def wsc_v(X, y, S, delta, v):\n",
        "        #print(X.shape)\n",
        "        #print(y.shape)\n",
        "        #print(len(S))\n",
        "        n = len(y)\n",
        "        cover = np.array([y[i] in S[i] for i in range(n)])\n",
        "        #print(f'length is {n}')\n",
        "        #print(n)\n",
        "        #print(f'coverage is {cover.sum()}')\n",
        "        z = np.dot(X,v)\n",
        "        # Compute mass\n",
        "        z_order = np.argsort(z)\n",
        "        z_sorted = z[z_order]\n",
        "        cover_ordered = cover[z_order]\n",
        "        ai_max = int(np.round((1.0-delta)*n))\n",
        "        ai_best = 0\n",
        "        bi_best = n-1\n",
        "        cover_min = 1\n",
        "        for ai in np.arange(0, ai_max):\n",
        "            bi_min = np.minimum(ai+int(np.round(delta*n)),n)\n",
        "            coverage = np.cumsum(cover_ordered[ai:n]) / np.arange(1,n-ai+1)\n",
        "            coverage[np.arange(0,bi_min-ai)]=1\n",
        "            bi_star = ai+np.argmin(coverage)\n",
        "            cover_star = coverage[bi_star-ai]\n",
        "            if cover_star < cover_min:\n",
        "                ai_best = ai\n",
        "                bi_best = bi_star\n",
        "                cover_min = cover_star\n",
        "        #print(cover_min)\n",
        "        #print(ai_best)\n",
        "        #print(bi_best)\n",
        "        return cover_min, z_sorted[ai_best], z_sorted[bi_best]\n",
        "\n",
        "    def sample_sphere(n, p):\n",
        "        v = rng.normal(size=(p, n))\n",
        "        v /= np.linalg.norm(v, axis=0)\n",
        "        return v.T\n",
        "    V = sample_sphere(M, p=X.shape[1])\n",
        "\n",
        "    wsc_list = [[]] * M\n",
        "    a_list = [[]] * M\n",
        "    b_list = [[]] * M\n",
        "    #print('wqwqqw')\n",
        "    if verbose:\n",
        "        for m in tqdm(range(M)):\n",
        "            wsc_list[m], a_list[m], b_list[m] = wsc_v(X, y, S, delta, V[m])\n",
        "    else:\n",
        "        #print('iehkwd')\n",
        "        for m in range(M):\n",
        "            #print(m)\n",
        "            #print(X.shape)\n",
        "            #print(y.shape)\n",
        "            wsc_list[m], a_list[m], b_list[m] = wsc_v(X, y, S, delta, V[m])\n",
        "            #print(m)\n",
        "    #print('ikfrk')\n",
        "    idx_star = np.argmin(np.array(wsc_list))\n",
        "    a_star = a_list[idx_star]\n",
        "    b_star = b_list[idx_star]\n",
        "    v_star = V[idx_star]\n",
        "    wsc_star = wsc_list[idx_star]\n",
        "    return wsc_star, v_star, a_star, b_star\n",
        "\n",
        "def wsc_unbiased(X, y, S, delta=0.1, M=1000, test_size=0.75, random_state=2020, verbose=False):\n",
        "    #print('iqhd')\n",
        "    def wsc_vab(X, y, S, v, a, b):\n",
        "        n = len(y)\n",
        "        cover = np.array([y[i] in S[i] for i in range(n)])\n",
        "        z = np.dot(X,v)\n",
        "        idx = np.where((z>=a)*(z<=b))\n",
        "        coverage = np.mean(cover[idx])\n",
        "        return coverage\n",
        "    #print('skjdh')\n",
        "\n",
        "    max_attempts = 5000\n",
        "    for attempt in range(max_attempts):\n",
        "        X_train, X_test, y_train, y_test, S_train, S_test = train_test_split(X, y, S, test_size=0.75, random_state=attempt)\n",
        "        cover = np.array([y_train[i] in S_train[i] for i in range(len(y_train))])\n",
        "        if not all(cover):\n",
        "            break\n",
        "\n",
        "    if all(cover):\n",
        "        print('May cause problem')\n",
        "    #print(len(y_train))\n",
        "    #print(cover)\n",
        "    #print('dasytrdv')\n",
        "    #print(X_train.shape)\n",
        "    #print(X_test.shape)\n",
        "    ##print(y_train.shape)\n",
        "    #print(y_test.shape)\n",
        "    #print(len(S_train))\n",
        "    #print(len(S_test))\n",
        "    #print(S_train)\n",
        "    # Find adversarial parameters\n",
        "    wsc_star, v_star, a_star, b_star = wsc(X_train, y_train, S_train, delta=delta, M=M, random_state=random_state, verbose=verbose)\n",
        "    #print('ewtddvzdas')\n",
        "    #print(v_star)\n",
        "    #print(a_star)\n",
        "    #print(b_star)\n",
        "    # Estimate coverage\n",
        "    coverage = wsc_vab(X_test, y_test, S_test, v_star, a_star, b_star)\n",
        "    return coverage\n"
      ],
      "metadata": {
        "id": "BVaz7Il4wQXI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the results"
      ],
      "metadata": {
        "id": "C3U0Jxai96q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_weighted_average_coverage(coverages, X_test, f):\n",
        "\n",
        "    weighted_coverages = [coverages[i] * f(X_test[i]) for i in range(len(X_test))]\n",
        "\n",
        "\n",
        "    total_weight = sum(f(X_test[i]) for i in range(len(X_test)))\n",
        "\n",
        "\n",
        "    weighted_average_coverage = sum(weighted_coverages) / total_weight\n",
        "\n",
        "    return weighted_average_coverage\n",
        "def create_intervals(min_value, max_value, n):\n",
        "    interval_length = (max_value - min_value) / n\n",
        "\n",
        "    # Create the intervals\n",
        "    intervals = [(min_value + i * interval_length, min_value + (i + 1) * interval_length) for i in range(n)]\n",
        "\n",
        "    return intervals"
      ],
      "metadata": {
        "id": "09mZtskjSXJn"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Regression_GNN_results.txt'\n",
        "\n",
        "with open(file_path, 'rb') as file:\n",
        "    results = pickle.load(file)"
      ],
      "metadata": {
        "id": "FBsAWQ20_mVs"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "dataset_results[dataset_name][f'Iteration_{z}'] = {\n",
        "            'Marginal_Coverage': marg_coverage,\n",
        "            'current_test_features':current_test_features,\n",
        "            'coverage_flag':coverage_flag,\n",
        "            'Average length':average_length,\n",
        "            'current_data_features':current_data_features,\n",
        "            'current_calib_test_mask':current_calib_test_mask,\n",
        "            'current_test_mask_outof_calibtest':current_test_mask_outof_calibtest,\n",
        "            'current_test_labels':current_test_labels}\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-w5eXx1qVYQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "datasets = ['Anaheim','ChicagoSketch','county_election_2016','county_education_2012','county_income_2012','county_unemployment_2012']\n",
        "#dataset_results = {dataset: {'method_split_conformal': {}} for dataset in datasets}\n",
        "num_nodes_dataset = my_dict = {\n",
        "    'Anaheim': 914,\n",
        "    'ChicagoSketch': 2176 ,\n",
        "    'county_election_2016': 3234,\n",
        "    'county_education_2012': 3234,\n",
        "    'county_income_2012': 3234,\n",
        "    'county_unemployment_2012':3234,\n",
        "    'twitch_PTBR':1912\n",
        "}\n",
        "weighted_coverage = {dataset: {'Weighted_coverage_density': []} for dataset in datasets}\n",
        "for dataset in datasets:\n",
        "    print(dataset)\n",
        "    with open(f'/content/conformalized-gnn-master/dataset_regression/{dataset}_edge_list.txt', 'r') as f:\n",
        "      edges = [list(map(int, line.strip().split())) for line in f]\n",
        "    edges_tensor = torch.tensor(edges, dtype=torch.long)\n",
        "    edge_index = edges_tensor.t().contiguous()\n",
        "    features = torch.tensor([list(map(float, line.strip().split())) for line in open(f'/content/conformalized-gnn-master/dataset_regression/{dataset}_features.txt')], dtype=torch.float)\n",
        "    labels = torch.tensor([float(line.strip()) for line in open(f'/content/conformalized-gnn-master/dataset_regression/{dataset}_labels.txt')], dtype=torch.float)\n",
        "    num_nodes = num_nodes_dataset.get(f'{dataset}')\n",
        "    valid_edges_mask = (edge_index >= 0).all(dim=0) & (edge_index < num_nodes).all(dim=0)\n",
        "    filtered_edge_index = edge_index[:, valid_edges_mask]\n",
        "\n",
        "    data = Data(x=features, edge_index=filtered_edge_index, y=labels)\n",
        "    rank = np.linalg.matrix_rank(data.x.detach().cpu().numpy())\n",
        "    print(data)\n",
        "    data_4d = data.x\n",
        "    scaler = StandardScaler()\n",
        "    data_4d_std = scaler.fit_transform(data_4d)\n",
        "    pca = PCA(n_components=1)\n",
        "    data_reduced = pca.fit_transform(data_4d_std)\n",
        "    kde = gaussian_kde(data_reduced.T)\n",
        "\n",
        "    total_coverage = 0\n",
        "    total_run=30\n",
        "    for o in range(total_run):\n",
        "\n",
        "      coverages = results.get(f'{dataset}').get(f'Iteration_{o}').get('coverage_flag')\n",
        "      X_test = results.get(f'{dataset}').get(f'Iteration_{o}').get('current_test_features')\n",
        "      X_test_transformed = pca.transform(X_test)\n",
        "      #print(kde(X_test_transformed[0]))\n",
        "      # Apply KDE to the PCA-transformed test points\n",
        "      weighted_coverages = [coverages[i] * kde(X_test_transformed[i:i+1]) for i in range(len(X_test_transformed))]\n",
        "      total_weight = sum(kde(X_test_transformed[i:i+1]) for i in range(len(X_test_transformed)))\n",
        "\n",
        "      weighted_average_coverage = sum(weighted_coverages) / total_weight\n",
        "      #print(weighted_average_coverage)\n",
        "      total_coverage += weighted_average_coverage\n",
        "    weighted_coverage[dataset]['Weighted_coverage_density'].append(total_coverage/total_run)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSLtmCfSSRwh",
        "outputId": "40b650e3-a0a0-44a4-bc0b-2d8887bc2859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anaheim\n",
            "Data(x=[914, 4], edge_index=[2, 3870], y=[914])\n",
            "ChicagoSketch\n",
            "Data(x=[2176, 4], edge_index=[2, 15099], y=[2176])\n",
            "county_election_2016\n",
            "Data(x=[3234, 6], edge_index=[2, 12715], y=[3234])\n",
            "county_education_2012\n",
            "Data(x=[3234, 6], edge_index=[2, 12715], y=[3234])\n",
            "county_income_2012\n",
            "Data(x=[3234, 6], edge_index=[2, 12715], y=[3234])\n",
            "county_unemployment_2012\n",
            "Data(x=[3234, 6], edge_index=[2, 12715], y=[3234])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "datasets = ['Anaheim','ChicagoSketch','county_election_2016','county_education_2012','county_income_2012','county_unemployment_2012']\n",
        "dataset_results = {dataset: {'method_split_conformal': {}} for dataset in datasets}\n",
        "num_nodes_dataset = my_dict = {\n",
        "    'Anaheim': 914,\n",
        "    'ChicagoSketch': 2176 ,\n",
        "    'county_election_2016': 3234,\n",
        "    'county_education_2012': 3234,\n",
        "    'county_income_2012': 3234,\n",
        "    'county_unemployment_2012':3234,\n",
        "    'twitch_PTBR':1912\n",
        "}\n",
        "\n",
        "weighted_coverage = {dataset: {'Weighted_coverage_density': {}} for dataset in datasets}\n",
        "for dataset in datasets:\n",
        "    print(dataset)\n",
        "    with open(f'/content/conformalized-gnn-master/dataset_regression/{dataset}_edge_list.txt', 'r') as f:\n",
        "      edges = [list(map(int, line.strip().split())) for line in f]\n",
        "    edges_tensor = torch.tensor(edges, dtype=torch.long)\n",
        "    edge_index = edges_tensor.t().contiguous()\n",
        "    features = torch.tensor([list(map(float, line.strip().split())) for line in open(f'/content/conformalized-gnn-master/dataset_regression/{dataset}_features.txt')], dtype=torch.float)\n",
        "    labels = torch.tensor([float(line.strip()) for line in open(f'/content/conformalized-gnn-master/dataset_regression/{dataset}_labels.txt')], dtype=torch.float)\n",
        "    num_nodes = num_nodes_dataset.get(f'{dataset}')\n",
        "    valid_edges_mask = (edge_index >= 0).all(dim=0) & (edge_index < num_nodes).all(dim=0)\n",
        "    filtered_edge_index = edge_index[:, valid_edges_mask]\n",
        "\n",
        "    data = Data(x=features, edge_index=filtered_edge_index, y=labels)\n",
        "    rank = np.linalg.matrix_rank(data.x.detach().cpu().numpy())\n",
        "    print(data)\n",
        "    data_4d = data.x\n",
        "    scaler = StandardScaler()\n",
        "    data_4d_std = scaler.fit_transform(data_4d)\n",
        "    pca = PCA(n_components=1)\n",
        "    data_reduced = pca.fit_transform(data_4d_std)\n",
        "    kde = gaussian_kde(data_reduced.T)\n",
        "\n",
        "    covariates_np = data.x.detach().cpu().numpy()\n",
        "\n",
        "    min_value = np.min(covariates_np[:, 2])\n",
        "    max_value = np.max(covariates_np[:, 2])\n",
        "    def indicator_function(x, interval):\n",
        "        return 1 if interval[0] <= x < interval[1] else 0\n",
        "    intervals = create_intervals(min_value,max_value,10)\n",
        "\n",
        "    interval_coverage = {i: [] for i in range(10)}\n",
        "    for o in range(50):\n",
        "        coverages = results[dataset]['method_split_conformal']['Iteration_{}'.format(o)]['coverage_flag']\n",
        "        X_test = data.x[results[dataset]['method_split_conformal']['Iteration_{}'.format(o)]['test_mask']]\n",
        "\n",
        "        for b in range(10):\n",
        "            weighted_coverages = [coverages[i] * indicator_function(X_test[i:i+1][0][2], intervals[b]) for i in range(len(X_test))]\n",
        "            total_weight = sum(indicator_function(X_test[i:i+1][0][2], intervals[b]) for i in range(len(X_test)))\n",
        "\n",
        "\n",
        "            weighted_average_coverage = sum(weighted_coverages) / total_weight if total_weight != 0 else 0\n",
        "            interval_coverage[b].append(weighted_average_coverage)\n",
        "\n",
        "    for b in range(10):\n",
        "        weighted_coverage[dataset]['Weighted_coverage_density'][intervals[b]] = np.mean(interval_coverage[b])\n",
        "\n"
      ],
      "metadata": {
        "id": "PBqCGmmBhOr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = ['Anaheim','ChicagoSketch','county_election_2016','county_education_2012','county_income_2012','county_unemployment_2012']\n",
        "for data in datasets:\n",
        "    #weighted_cov = weighted_coverage[data]['Weighted_coverage_density']\n",
        "    marginal_cov = np.mean([results[data]['Iteration_{}'.format(i)]['Marginal_Coverage'] for i in range(30)])\n",
        "    conditional_cov = np.mean([results[data]['Iteration_{}'.format(i)]['conditional_coverage'] for i in range(30)])\n",
        "    length  = np.mean([results[data]['Iteration_{}'.format(i)]['Average length'] for i in range(30)])\n",
        "\n",
        "    print(f\"{data}:\\n\"\n",
        "          f\"  - Average length: {length}\\n\"\n",
        "          #f\"  - Weighted coverage: {weighted_cov}\\n\"\n",
        "          f\"  - Marginal Coverage: {marginal_cov}\\n\"\n",
        "          f\"  - Conditional coverage: {conditional_cov}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgsmsWKyZVVR",
        "outputId": "a4517893-5686-46cd-ed39-69d77ded6ebf"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anaheim:\n",
            "  - Average length: 2.831768751144409\n",
            "  - Marginal Coverage: 0.9577413479052824\n",
            "  - Conditional coverage: 0.9448366972826109\n",
            "ChicagoSketch:\n",
            "  - Average length: 2.407207489013672\n",
            "  - Marginal Coverage: 0.9525993883792049\n",
            "  - Conditional coverage: 0.9293686194050382\n",
            "county_election_2016:\n",
            "  - Average length: 1.015498399734497\n",
            "  - Marginal Coverage: 0.944152498712004\n",
            "  - Conditional coverage: 0.9156506211909357\n",
            "county_education_2012:\n",
            "  - Average length: 2.6729772090911865\n",
            "  - Marginal Coverage: 0.9525502318392582\n",
            "  - Conditional coverage: 0.9210911267905411\n",
            "county_income_2012:\n",
            "  - Average length: 2.540382146835327\n",
            "  - Marginal Coverage: 0.9481710458526533\n",
            "  - Conditional coverage: 0.9287074888153933\n",
            "county_unemployment_2012:\n",
            "  - Average length: 3.302426338195801\n",
            "  - Marginal Coverage: 0.9502833590932508\n",
            "  - Conditional coverage: 0.899479029393077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = ['Anaheim','ChicagoSketch','county_election_2016','county_education_2012','county_income_2012','county_unemployment_2012']\n",
        "for data in datasets:\n",
        "    #weighted_cov = weighted_coverage[data]['Weighted_coverage_density']\n",
        "    marginal_cov = np.mean([results[data]['method_split_conformal']['Iteration_{}'.format(i)]['Marginal_Coverage'] for i in range(30)])\n",
        "    conditional_cov = np.mean([results[data]['method_split_conformal']['Iteration_{}'.format(i)]['Conditional_Coverage'] for i in range(30)])\n",
        "    length  = np.mean([results[data]['method_split_conformal']['Iteration_{}'.format(i)]['Average length'] for i in range(30)])\n",
        "\n",
        "    print(f\"{data}:\\n\"\n",
        "          f\"  - Marginal coverage: {marginal_cov}\\n\"\n",
        "          #f\"  - Weighted coverage: {weighted_cov}\\n\"\n",
        "          f\"  - Length: {length}\\n\"\n",
        "          f\"  - Conditional coverage: {conditional_cov}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHmbR1wjGioH",
        "outputId": "159b707f-a78b-4915-c35e-09e46bbac9f6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anaheim:\n",
            "  - Marginal coverage: 0.9553527980535281\n",
            "  - Length: 2.747343723987089\n",
            "  - Conditional coverage: 0.8925062519562598\n",
            "ChicagoSketch:\n",
            "  - Marginal coverage: 0.9519142419601837\n",
            "  - Length: 2.97746501993987\n",
            "  - Conditional coverage: 0.8383933823296285\n",
            "county_election_2016:\n",
            "  - Marginal coverage: 0.9527835051546389\n",
            "  - Length: 1.0393593447154383\n",
            "  - Conditional coverage: 0.8953917418366503\n",
            "county_education_2012:\n",
            "  - Marginal coverage: 0.9507216494845359\n",
            "  - Length: 2.6378783461365076\n",
            "  - Conditional coverage: 0.8326155520793985\n",
            "county_income_2012:\n",
            "  - Marginal coverage: 0.9507903780068727\n",
            "  - Length: 2.7768732245725025\n",
            "  - Conditional coverage: 0.7459627927097983\n",
            "county_unemployment_2012:\n",
            "  - Marginal coverage: 0.9504123711340201\n",
            "  - Length: 3.051837706715993\n",
            "  - Conditional coverage: 0.8614182317642717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = ['Anaheim','ChicagoSketch','county_election_2016','county_education_2012','county_income_2012','county_unemployment_2012']\n",
        "for data in datasets:\n",
        "    #weighted_cov = weighted_coverage[data]['Weighted_coverage_density']\n",
        "    marginal_cov = np.mean([results[data]['method_condition_calibrated']['Iteration_{}'.format(i)]['Marginal_Coverage'] for i in range(30)])\n",
        "    conditional_cov = np.mean([results[data]['method_condition_calibrated']['Iteration_{}'.format(i)]['Conditional_Coverage'] for i in range(30)])\n",
        "    length  = np.mean([results[data]['method_condition_calibrated']['Iteration_{}'.format(i)]['Average length'] for i in range(30)])\n",
        "\n",
        "    print(f\"{data}:\\n\"\n",
        "          f\"  - Marginal coverage: {marginal_cov}\\n\"\n",
        "          #f\"  - Weighted coverage: {weighted_cov}\\n\"\n",
        "          f\"  - Length: {length}\\n\"\n",
        "          f\"  - Conditional coverage: {conditional_cov}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdTg-lWtwHS7",
        "outputId": "5490f050-1d13-42b6-bcc0-f55906d46660"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anaheim:\n",
            "  - Marginal coverage: 0.9535279805352798\n",
            "  - Length: 2.5260525351585885\n",
            "  - Conditional coverage: 0.8537981286110646\n",
            "ChicagoSketch:\n",
            "  - Marginal coverage: 0.9527820316488004\n",
            "  - Length: 2.866116881406764\n",
            "  - Conditional coverage: 0.8761236071673199\n",
            "county_election_2016:\n",
            "  - Marginal coverage: 0.9519243986254293\n",
            "  - Length: 1.0090525463011493\n",
            "  - Conditional coverage: 0.9273379925625475\n",
            "county_education_2012:\n",
            "  - Marginal coverage: 0.952508591065292\n",
            "  - Length: 2.5652335697284907\n",
            "  - Conditional coverage: 0.9176847692416028\n",
            "county_income_2012:\n",
            "  - Marginal coverage: 0.9546391752577319\n",
            "  - Length: 2.6630490465534598\n",
            "  - Conditional coverage: 0.9347123512127646\n",
            "county_unemployment_2012:\n",
            "  - Marginal coverage: 0.9543986254295531\n",
            "  - Length: 3.3552544362778556\n",
            "  - Conditional coverage: 0.884627654021097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the results\n",
        "for dataset in datasets:\n",
        "    print(f\"{dataset}:\")\n",
        "    for interval, avg_coverage in weighted_coverage[dataset]['Weighted_coverage_density'].items():\n",
        "        print(f\"  Interval {interval}: Average Weighted Coverage = {avg_coverage}\")"
      ],
      "metadata": {
        "id": "18hQJJNXli-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTzaCTmx9hO3"
      },
      "outputs": [],
      "source": [
        "dataset_names = datasets = ['Anaheim','ChicagoSketch','county_election_2016','county_education_2012','county_income_2012','county_unemployment_2012','twitch_PTBR']\n",
        "num_nodes_dataset = my_dict = {\n",
        "    'Anaheim': 914,\n",
        "    'ChicagoSketch': 2176 ,\n",
        "    'county_election_2016': 3234,\n",
        "    'county_education_2012': 3234,\n",
        "    'county_income_2012': 3234,\n",
        "    'county_unemployment_2012':3234,\n",
        "    'twitch_PTBR':1912\n",
        "}\n",
        "for dataset in dataset_names:\n",
        "\n",
        "    with open(f'/content/conformalized-gnn-master/dataset_regression/{dataset}_edge_list.txt', 'r') as f:\n",
        "        edges = [list(map(int, line.strip().split())) for line in f]\n",
        "    edges_tensor = torch.tensor(edges, dtype=torch.long)\n",
        "    edge_index = edges_tensor.t().contiguous()\n",
        "    features = torch.tensor([list(map(float, line.strip().split())) for line in open(f'/content/conformalized-gnn-master/dataset_regression/{dataset}_features.txt')], dtype=torch.float)\n",
        "    labels = torch.tensor([float(line.strip()) for line in open(f'/content/conformalized-gnn-master/dataset_regression/{dataset}_labels.txt')], dtype=torch.float)\n",
        "    num_nodes = num_nodes_dataset.get(f'{dataset}')\n",
        "    valid_edges_mask = (edge_index >= 0).all(dim=0) & (edge_index < num_nodes).all(dim=0)\n",
        "    filtered_edge_index = edge_index[:, valid_edges_mask]\n",
        "\n",
        "    data = Data(x=features, edge_index=filtered_edge_index, y=labels)\n",
        "    rank = np.linalg.matrix_rank(data.x.detach().cpu().numpy())\n",
        "    print(f'{dataset} dataset has original feature dimension = {data.num_node_features}')\n",
        "    print(f'{dataset} dataset has feature rank = {rank}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using our methods"
      ],
      "metadata": {
        "id": "Jh-7bu3E-fLh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "1ibCIOu12_UJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2d45d277-a9f1-472f-b8b2-121df3d8de40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anaheim\n",
            "dataset name Anaheim\n",
            "Data(x=[914, 4], edge_index=[2, 3870], y=[914])\n",
            "Not Chicago Sketch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.NeighborSampler' is deprecated, use 'loader.NeighborSampler' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot broadcast dimensions  (276,) (75626,)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-d446b86b63f7>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    437\u001b[0m       \u001b[0minfinite_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0mcond_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCondConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi_kernel_estimation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfinite_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m       \u001b[0mcond_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_problem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_calib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_calib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m       \u001b[0mn_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/conditionalconformal/condconf.py\u001b[0m in \u001b[0;36msetup_problem\u001b[0;34m(self, x_calib, y_calib)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores_calib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_calib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_calib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         self.cvx_problem = setup_cvx_problem(\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_calib\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores_calib\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/conditionalconformal/condconf.py\u001b[0m in \u001b[0;36msetup_cvx_problem\u001b[0;34m(x_calib, scores_calib, phi_calib, infinite_params)\u001b[0m\n\u001b[1;32m    632\u001b[0m         ]\n\u001b[1;32m    633\u001b[0m         prob = cp.Problem(\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMinimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mconstraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cvxpy/atoms/affine/binary_operators.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lh_expr, rh_expr)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlh_expr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrh_expr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mlh_expr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrh_expr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlh_expr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrh_expr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlh_expr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrh_expr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_atom_log_log_convex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cvxpy/atoms/affine/binary_operators.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lh_exp, rh_exp)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlh_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrh_exp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBinaryOperator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlh_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrh_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cvxpy/atoms/atom.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mAtom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast_to_const\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_from_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Atoms must be at most 2D.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cvxpy/atoms/affine/binary_operators.py\u001b[0m in \u001b[0;36mshape_from_args\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \"\"\"The sum of the argument dimensions - 1.\n\u001b[1;32m    279\u001b[0m         \"\"\"\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_psd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cvxpy/utilities/shape.py\u001b[0m in \u001b[0;36msum_shapes\u001b[0;34m(shapes)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Only allow broadcasting for 0D arrays or summation of scalars.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqueezed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqueezed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0;34m\"Cannot broadcast dimensions \"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 len(shapes)*\" %s\" % tuple(shapes))\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot broadcast dimensions  (276,) (75626,)"
          ]
        }
      ],
      "source": [
        "datasets = ['Anaheim','ChicagoSketch','county_election_2016','county_education_2012','county_income_2012','county_unemployment_2012']\n",
        "from tqdm import tqdm\n",
        "dataset_results = {dataset: {'method_split_conformal': {}, 'method_condition_calibrated': {}} for dataset in datasets}\n",
        "num_nodes_dataset = my_dict = {\n",
        "    'Anaheim': 914,\n",
        "    'ChicagoSketch': 2176 ,\n",
        "    'county_election_2016': 3234,\n",
        "    'county_education_2012': 3234,\n",
        "    'county_income_2012': 3234,\n",
        "    'county_unemployment_2012':3234,\n",
        "    'twitch_PTBR':1912\n",
        "}\n",
        "for dataset in datasets:\n",
        "    print(dataset)\n",
        "    with open(f'/content/conformalized-gnn-master/dataset_regression/{dataset}_edge_list.txt', 'r') as f:\n",
        "      edges = [list(map(int, line.strip().split())) for line in f]\n",
        "    edges_tensor = torch.tensor(edges, dtype=torch.long)\n",
        "    edge_index = edges_tensor.t().contiguous()\n",
        "    features = torch.tensor([list(map(float, line.strip().split())) for line in open(f'/content/conformalized-gnn-master/dataset_regression/{dataset}_features.txt')], dtype=torch.float)\n",
        "    labels = torch.tensor([float(line.strip()) for line in open(f'/content/conformalized-gnn-master/dataset_regression/{dataset}_labels.txt')], dtype=torch.float)\n",
        "    num_nodes = num_nodes_dataset.get(f'{dataset}')\n",
        "    valid_edges_mask = (edge_index >= 0).all(dim=0) & (edge_index < num_nodes).all(dim=0)\n",
        "    filtered_edge_index = edge_index[:, valid_edges_mask]\n",
        "\n",
        "    data = Data(x=features, edge_index=filtered_edge_index, y=labels)\n",
        "    rank = np.linalg.matrix_rank(data.x.detach().cpu().numpy())\n",
        "\n",
        "    data_4d = data.x\n",
        "    scaler = StandardScaler()\n",
        "    data_4d_std = scaler.fit_transform(data_4d)\n",
        "    pca = PCA(n_components=1)\n",
        "    data_reduced = pca.fit_transform(data_4d_std)\n",
        "    kde = gaussian_kde(data_reduced.T)\n",
        "    print(f'dataset name {dataset}')\n",
        "    print(data)\n",
        "\n",
        "    def create_kdes(data_x):\n",
        "      kdes = []\n",
        "      if(dataset!='ChicagoSketch'):\n",
        "        print('Not Chicago Sketch')\n",
        "        for i in range(data_x.shape[1]):\n",
        "            kde = gaussian_kde(data_x[:, i])\n",
        "            kdes.append(kde)\n",
        "        return kdes\n",
        "      else:\n",
        "        for i in range(data_x.shape[1]-1):\n",
        "            kde = gaussian_kde(data_x[:,i])\n",
        "            kdes.append(kde)\n",
        "        return kdes\n",
        "\n",
        "    kdes = create_kdes(data.x)\n",
        "\n",
        "\n",
        "    for c in range(30):\n",
        "\n",
        "      device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "      x = data.x.to(device)\n",
        "      y = data.y.squeeze().to(device)\n",
        "      train_percentage = 0.4\n",
        "      test_percentage = 0.5\n",
        "      split_marginal = []\n",
        "      split_wsc_condition = []\n",
        "      split_length = []\n",
        "      condition_calibrated_marginal = []\n",
        "      condition_calibrated_wsc_condition = []\n",
        "      condition_calibrated_length = []\n",
        "\n",
        "\n",
        "      #Randomly choose train and calitest set\n",
        "\n",
        "      num_nodes = data.num_nodes\n",
        "      all_node_indices = list(range(num_nodes))\n",
        "      random.shuffle(all_node_indices)\n",
        "      num_train_nodes = int(train_percentage * num_nodes)\n",
        "      num_testandcali_nodes = num_nodes - num_train_nodes\n",
        "      train_node_indices = all_node_indices[:num_train_nodes]\n",
        "      testandcali_node_indices = all_node_indices[num_train_nodes:]\n",
        "\n",
        "      test_node_indices = testandcali_node_indices[:int(test_percentage * num_testandcali_nodes)]\n",
        "      cali_node_indices = testandcali_node_indices[int(test_percentage * num_testandcali_nodes):]\n",
        "\n",
        "\n",
        "      train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "      train_mask[train_node_indices] = True\n",
        "      train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "      train_mask[train_node_indices] = True\n",
        "\n",
        "      testandcali_mask = torch.zeros(num_nodes,dtype=torch.bool)\n",
        "      testandcali_mask[testandcali_node_indices] = True\n",
        "\n",
        "      test_mask = torch.zeros(num_nodes,dtype=torch.bool)\n",
        "      test_mask[test_node_indices] = True\n",
        "\n",
        "\n",
        "      cali_mask = torch.zeros(num_nodes,dtype=torch.bool)\n",
        "      cali_mask[cali_node_indices] = True\n",
        "      #test_mask_lists.append(test_mask)\n",
        "\n",
        "      #*********************************************\n",
        "      n_train =train_mask.sum()\n",
        "      n_calib = test_mask.sum()\n",
        "      n_test = cali_mask.sum()\n",
        "\n",
        "\n",
        "      #*********************************************\n",
        "      #Use same Notation as the synthetic data\n",
        "      x_train_final, y_train_final, x_calib, y_calib, x_test, y_test= x[train_mask].detach().cpu().numpy(), y[train_mask].detach().cpu().numpy(), x[cali_mask].detach().cpu().numpy(), y[cali_mask].detach().cpu().numpy(), x[test_mask].detach().cpu().numpy(), y[test_mask].detach().cpu().numpy()\n",
        "\n",
        "      x_all = np.concatenate((x_train_final, x_calib, x_test), axis=0)\n",
        "      y_all = np.concatenate((y_train_final,y_calib,y_test),axis = 0)\n",
        "      #*********************************************\n",
        "      train_loader = NeighborSampler(\n",
        "          data.edge_index, node_idx=(train_mask),\n",
        "          sizes=[3, 3, 3], batch_size=32, shuffle=False,\n",
        "      )\n",
        "      #*********************************************\n",
        "      class SAGERegressor(torch.nn.Module):\n",
        "          def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3):\n",
        "              super(SAGERegressor, self).__init__()\n",
        "\n",
        "              self.num_layers = num_layers\n",
        "              self.convs = torch.nn.ModuleList()\n",
        "              self.batch_norms = torch.nn.ModuleList()\n",
        "              self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
        "              self.batch_norms.append(BatchNorm(hidden_channels))\n",
        "\n",
        "              for i in range(num_layers - 2):\n",
        "                  self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "                  self.batch_norms.append(BatchNorm(hidden_channels))\n",
        "\n",
        "              self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
        "              self.batch_norms.append(BatchNorm(out_channels))\n",
        "              self.linear1 = torch.nn.Linear(out_channels, hidden_channels)\n",
        "              self.linear2 = torch.nn.Linear(hidden_channels, 1)\n",
        "\n",
        "          def reset_parameters(self):\n",
        "\n",
        "              for conv in self.convs:\n",
        "                  conv.reset_parameters()\n",
        "              for bn in self.batch_norms:\n",
        "                  bn.reset_parameters()\n",
        "              self.linear1.reset_parameters()\n",
        "              self.linear2.reset_parameters()\n",
        "\n",
        "          def forward(self, x, adjs):\n",
        "              for i, (edge_index, _, size) in enumerate(adjs):\n",
        "                  x_target = x[:size[1]]\n",
        "                  x = self.convs[i]((x, x_target), edge_index)\n",
        "\n",
        "                  x = self.batch_norms[i](x)\n",
        "                  if i != self.num_layers - 1:\n",
        "                      x = F.leaky_relu(x, 0.2)\n",
        "                      x = F.dropout(x, p=0.1, training=self.training)\n",
        "\n",
        "              x = F.leaky_relu(self.linear1(x), 0.2)\n",
        "              x = self.linear2(x)\n",
        "\n",
        "              return x\n",
        "      class GNN(torch.nn.Module):\n",
        "            def __init__(self, in_channels, hidden_channels, out_channels, backbone, heads = 1, aggr = 'sum'):\n",
        "                super().__init__()\n",
        "                if backbone == 'GCN':\n",
        "                    self.conv1 = GCNConv(in_channels, hidden_channels, cached=True,\n",
        "                                    normalize=True)\n",
        "                    self.conv2 = GCNConv(hidden_channels, out_channels, cached=True,\n",
        "                                    normalize=True)\n",
        "                elif backbone == 'GAT':\n",
        "                    self.conv1 = GATConv(in_channels, hidden_channels, heads)\n",
        "                    self.conv2 = GATConv(hidden_channels, out_channels, heads)\n",
        "                elif backbone == 'GraphSAGE':\n",
        "                    self.conv1 = SAGEConv(in_channels, hidden_channels, aggr)\n",
        "                    self.conv2 = SAGEConv(hidden_channels, out_channels, aggr)\n",
        "                elif backbone == 'SGC':\n",
        "                    self.conv1 = SGConv(in_channels, hidden_channels)\n",
        "                    self.conv2 = SGConv(hidden_channels, out_channels)\n",
        "\n",
        "            def forward(self, x, edge_index, edge_weight=None, return_h = False):\n",
        "                x = F.dropout(x, p=0.5, training=self.training)\n",
        "                x = self.conv1(x, edge_index, edge_weight).relu()\n",
        "                if return_h:\n",
        "                    return x\n",
        "                x = F.dropout(x, p=0.5, training=self.training)\n",
        "                x = self.conv2(x, edge_index, edge_weight)\n",
        "                return x\n",
        "\n",
        "      class GNN_Multi_Layer(torch.nn.Module):\n",
        "          def __init__(self, in_channels, hidden_channels, out_channels, backbone, heads = 1, aggr = 'sum', num_layers = 2):\n",
        "              super().__init__()\n",
        "\n",
        "              self.convs = torch.nn.ModuleList()\n",
        "              if num_layers == 1:\n",
        "                  if backbone == 'GCN':\n",
        "                      self.convs.append(GCNConv(in_channels, out_channels, cached=True, normalize=True))\n",
        "                  elif backbone == 'GAT':\n",
        "                      self.convs.append(GATConv(in_channels, out_channels, heads))\n",
        "                  elif backbone == 'GraphSAGE':\n",
        "                      self.convs.append(SAGEConv(in_channels, out_channels, aggr))\n",
        "                  elif backbone == 'SGC':\n",
        "                      self.convs.append(SGConv(in_channels, out_channels))\n",
        "              else:\n",
        "                  if backbone == 'GCN':\n",
        "                      self.convs.append(GCNConv(in_channels, hidden_channels, cached=True, normalize=True))\n",
        "                  elif backbone == 'GAT':\n",
        "                      self.convs.append(GATConv(in_channels, hidden_channels, heads))\n",
        "                  elif backbone == 'GraphSAGE':\n",
        "                      self.convs.append(SAGEConv(in_channels, hidden_channels, aggr))\n",
        "                  elif backbone == 'SGC':\n",
        "                      self.convs.append(SGConv(in_channels, hidden_channels))\n",
        "                  for _ in range(num_layers-2):\n",
        "                      if backbone == 'GCN':\n",
        "                          self.convs.append(GCNConv(hidden_channels, hidden_channels, cached=True, normalize=True))\n",
        "                      elif backbone == 'GAT':\n",
        "                          self.convs.append(GATConv(hidden_channels, hidden_channels, heads))\n",
        "                      elif backbone == 'GraphSAGE':\n",
        "                          self.convs.append(SAGEConv(hidden_channels, hidden_channels, aggr))\n",
        "                      elif backbone == 'SGC':\n",
        "                          self.convs.append(SGConv(hidden_channels, hidden_channels))\n",
        "                  if backbone == 'GCN':\n",
        "                      self.convs.append(GCNConv(hidden_channels, out_channels, cached=True, normalize=True))\n",
        "                  elif backbone == 'GAT':\n",
        "                      self.convs.append(GATConv(hidden_channels, out_channels, heads))\n",
        "                  elif backbone == 'GraphSAGE':\n",
        "                      self.convs.append(SAGEConv(hidden_channels, out_channels, aggr))\n",
        "                  elif backbone == 'SGC':\n",
        "                      self.convs.append(SGConv(hidden_channels, out_channels))\n",
        "          def reset_parameters(self):\n",
        "\n",
        "              for conv in self.convs:\n",
        "                  conv.reset_parameters()\n",
        "\n",
        "          def forward(self, x, adjs):\n",
        "                  for idx, (edge_index, _, size) in enumerate(adjs):\n",
        "                      if idx >= len(self.convs):\n",
        "                          break\n",
        "\n",
        "                      x_target = x[:size[1]]\n",
        "                      x = self.convs[idx]((x, x_target), edge_index)\n",
        "\n",
        "                      if idx != len(self.convs) - 1:\n",
        "                          x = F.relu(x)\n",
        "                          x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "                  return x\n",
        "      #*********************************************\n",
        "      #regression task\n",
        "      #model = SAGERegressor(data.num_features, 3, 1, num_layers=3)\n",
        "      model = GNN_Multi_Layer(in_channels = data.num_features,hidden_channels = 16,out_channels = 1,backbone = 'GraphSAGE')\n",
        "      model.reset_parameters()\n",
        "      model = model.to(device)\n",
        "      #print(model)\n",
        "      #*********************************************\n",
        "      model.reset_parameters()\n",
        "      #*********************************************\n",
        "      optimizer = torch.optim.Adam(model.parameters(),lr = 0.01)\n",
        "      def train(epoch):\n",
        "          model.train()\n",
        "          total_loss = 0\n",
        "          for batch_size, n_id, adjs in train_loader:\n",
        "              adjs = [adj.to(device) for adj in adjs]\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "\n",
        "              out = model(x[n_id], adjs)\n",
        "              out = out[:batch_size, :]\n",
        "\n",
        "              # The target tensor should match the batch size\n",
        "              target = y[n_id[:batch_size]]\n",
        "\n",
        "              loss = F.mse_loss(out, target.unsqueeze(1))\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              total_loss += loss.item()\n",
        "          return total_loss\n",
        "\n",
        "\n",
        "\n",
        "      for epoch in range(1,50):\n",
        "          loss = train(epoch)\n",
        "      #*********************************************\n",
        "      Calib_loader = NeighborSampler(data.edge_index,node_idx = cali_mask\n",
        "        , sizes = [3,3,3],batch_size = int((test_mask+cali_mask).sum()),shuffle=False)\n",
        "\n",
        "      Test_loader = NeighborSampler(data.edge_index,node_idx = test_mask,sizes = [3,3,3],batch_size = int((test_mask+cali_mask).sum()),shuffle=False)\n",
        "      #*********************************************\n",
        "      #function the takes graph loader to the third layer embedding and the fn which is\n",
        "      #a matrix in the form [x,predicted y]\n",
        "      def inference_1(subgraph_loader):\n",
        "          model.eval()\n",
        "          all_c = []\n",
        "          all_emb3 = []\n",
        "\n",
        "          for batch_size, n_id, adjs in subgraph_loader:\n",
        "              adjs = [adj.to(device) for adj in adjs]\n",
        "\n",
        "              emb3 = model(x[n_id], adjs)[:batch_size]\n",
        "\n",
        "              c = x[n_id][:batch_size]\n",
        "\n",
        "              all_c.append(c)\n",
        "              all_emb3.append(emb3)\n",
        "\n",
        "          # Concatenate all the batches together\n",
        "          all_c = torch.cat(all_c, dim=0)\n",
        "          all_emb3 = torch.cat(all_emb3, dim=0)\n",
        "\n",
        "          fn = torch.cat((all_c, all_emb3), dim=1)\n",
        "\n",
        "          return fn\n",
        "\n",
        "      #*********************************************\n",
        "\n",
        "      fn_Calib = inference_1(Calib_loader)\n",
        "      fn_Test = inference_1(Test_loader)\n",
        "      fn_Calib = fn_Calib.detach().cpu().numpy()\n",
        "      fn_Test = fn_Test.detach().cpu().numpy()\n",
        "      fn = np.vstack((fn_Calib,fn_Test))\n",
        "      prediction_dimension = data.num_features\n",
        "      #*********************************************\n",
        "      dimension = data.num_features\n",
        "      #x is input vectors with each row representing one data and output is predicted label vector\n",
        "      def get_xhat(x_vector):\n",
        "        if(x_vector.shape[1]!=1):\n",
        "              xhat_1= np.ones((x_vector.shape[0], 1))\n",
        "              for j in range(x_vector.shape[0]):\n",
        "                for i in range((cali_mask+test_mask).sum()):\n",
        "                  if(np.array_equal(x_vector[j],fn[i,0:dimension])):\n",
        "                    xhat_1[j]=fn[i,dimension]\n",
        "              return xhat_1\n",
        "        else:\n",
        "              for i in range((cali_mask+test_mask).sum()):\n",
        "                if(np.array_equal(x_vector.squeeze(),fn[i,0:dimension])):\n",
        "                  return fn[i,dimension]\n",
        "      #*********************************************\n",
        "      #Two sided confidence interval\n",
        "      #score_fn = lambda x, y : np.abs(y - get_xhat(x).squeeze())\n",
        "      score_fn = lambda x, y : (y - get_xhat(x)).squeeze()\n",
        "      score_inv_fn = lambda s, x : [get_xhat(x)-s, get_xhat(x) + s]\n",
        "      score_inv_fn_lb = lambda s, x : [get_xhat(x) + s, np.inf]\n",
        "      score_inv_fn_ub = lambda s, x : [-np.inf, get_xhat(x) - s]\n",
        "      #*********************************************\n",
        "      def indicator_matrix(scalar_values, disc):\n",
        "          scalar_values = np.array(scalar_values)\n",
        "          intervals = [(disc[i], disc[i + 1]) for i in range(len(disc) - 1)]\n",
        "          matrix = np.zeros((scalar_values.shape[0], len(intervals)))\n",
        "          for i in range(scalar_values.shape[0]):\n",
        "            value = scalar_values[i,0]\n",
        "            for j,(a,b) in enumerate(intervals):\n",
        "              if (a<= value <b):\n",
        "                matrix[i,j] = 1\n",
        "\n",
        "          return matrix\n",
        "      #*********************************************\n",
        "      #Consider covariate shift of one of the dimension\n",
        "      dimension_interest = 2\n",
        "      #*********************************************\n",
        "      minimum = data.x[:,dimension_interest].min()\n",
        "      maximum = data.x[:,dimension_interest].max()\n",
        "      #disc depends on the minimum and maximum of the second dimension\n",
        "      #We next consider multiple types of covariate shifts and the last one is without any shifts\n",
        "      disc = np.arange(math.ceil(minimum)-1, math.ceil(maximum),0.5)\n",
        "      def phi_fn_groups(x_12):\n",
        "        if len(x_12.shape) == 2:\n",
        "          a=x_12[:x_12.shape[0],dimension_interest]\n",
        "          a = a.reshape(-1,1)\n",
        "          return indicator_matrix(a, disc)\n",
        "        else:\n",
        "          a = x_12.reshape(-1,1)\n",
        "          a = a[dimension_interest]\n",
        "          a = a.reshape(-1,1)\n",
        "          return indicator_matrix(a, disc)\n",
        "\n",
        "      #The gaussian density tilts with mean and variance\n",
        "      eval_locs = [-1.5, -0.5]\n",
        "      eval_scale = 0.2\n",
        "\n",
        "      other_locs = [0, 1,1.5]\n",
        "      other_scale = 1\n",
        "\n",
        "      def phi_fn_shifts(x_13):\n",
        "          if(len(x_13.shape) == 2):\n",
        "            a = x_13[:x_13.shape[0],dimension_interest]\n",
        "\n",
        "            shifts = [norm.pdf(a, loc=loc, scale=eval_scale).reshape(-1,1)\n",
        "                          for loc in eval_locs]\n",
        "            shifts.extend([norm.pdf(a, loc=loc, scale=other_scale).reshape(-1,1)\n",
        "                          for loc in other_locs])\n",
        "            shifts.append(np.ones((a.shape[0], 1)))\n",
        "            return np.concatenate(shifts, axis=1)\n",
        "\n",
        "          else:\n",
        "            a = x_13.reshape(-1,1)\n",
        "            a = a[dimension_interest]\n",
        "            shifts = [norm.pdf(a, loc=loc, scale=eval_scale).reshape(-1,1)\n",
        "                          for loc in eval_locs]\n",
        "            shifts.extend([norm.pdf(a, loc=loc, scale=other_scale).reshape(-1,1)\n",
        "                          for loc in other_locs])\n",
        "            shifts.append(np.ones((a.shape[0], 1)))\n",
        "            return np.concatenate(shifts, axis=1)\n",
        "      def intercept(x_14):\n",
        "        if(len(x_14.shape)==2):\n",
        "          return x_14\n",
        "        else:\n",
        "          return x_14.reshape(1,data.num_features)\n",
        "      \"\"\"\n",
        "      def phi_kernel_estimation(x_15):\n",
        "        if(len(x_15.shape) == 2):\n",
        "          print('2')\n",
        "          ones_vector = np.ones((x_15.shape[0],1))\n",
        "          new_data_point_std = x_15.reshape(x_15.shape[0],data.num_features)\n",
        "          projected_vector = pca.transform(new_data_point_std)\n",
        "          return np.column_stack((kde(projected_vector.T).reshape(x_15.shape[0],1),ones_vector))\n",
        "\n",
        "        else:\n",
        "          print('1')\n",
        "          ones_vector = np.ones((1,1))\n",
        "          new_data_point_std = x_15.reshape(1,data.num_features)\n",
        "          projected_vector = pca.transform(new_data_point_std)\n",
        "\n",
        "          return np.column_stack((kde(projected_vector).reshape(1,1),ones_vector))\n",
        "      \"\"\"\n",
        "\n",
        "      def phi_kernel_estimation(x_15):\n",
        "\n",
        "          if len(x_15.shape) == 2:  # Multiple data points\n",
        "              kde_estimations = [kde(x_15[:, i]).reshape(-1, 1) for i, kde in enumerate(kdes)]\n",
        "              ones_vector = np.ones((x_15.shape[0], 1))\n",
        "              return np.hstack(kde_estimations + [ones_vector])\n",
        "          else:  # Single data point\n",
        "              kde_estimations = [kde([x_15[i]]).reshape(1, 1) for i, kde in enumerate(kdes)]\n",
        "              ones_vector = np.array([[1]])\n",
        "              return np.hstack(kde_estimations + [ones_vector])\n",
        "\n",
        "\n",
        "\n",
        "      #*********************************************\n",
        "      #Set up the problem by kernel density estimation\n",
        "      infinite_params = {}\n",
        "      cond_conf = CondConf(score_fn, phi_kernel_estimation, infinite_params)\n",
        "      cond_conf.setup_problem(x_calib, y_calib)\n",
        "\n",
        "      n_test = len(x_test)\n",
        "      alpha = 0.05\n",
        "      lbs_kernel = np.zeros((n_test,))\n",
        "      ubs_kernel= np.zeros((n_test,))\n",
        "      i = 0\n",
        "      for x_t in tqdm(x_test):\n",
        "          \"\"\"\n",
        "          res = cond_conf.predict(1-alpha, x_t, score_inv_fn)\n",
        "          lbs_kernel[i] = res[0]\n",
        "          ubs_kernel[i] = res[1]\n",
        "          \"\"\"\n",
        "          res = cond_conf.predict(alpha / 2, x_t, score_inv_fn_lb, exact=True, randomize=True)\n",
        "          lbs_kernel[i] = res[0]\n",
        "          res = cond_conf.predict(1 - alpha / 2, x_t, score_inv_fn_ub, exact=True, randomize=True)\n",
        "          ubs_kernel[i] = res[1]\n",
        "          i += 1\n",
        "      #*********************************************\n",
        "      #Set up the problem by groups\n",
        "      \"\"\"\n",
        "      infinite_params = {}\n",
        "      #cond_conf = CondConf(score_fn, phi_fn_shifts, infinite_params)\n",
        "      #cond_conf.setup_problem(x_calib, y_calib)\n",
        "\n",
        "      n_test = len(x_test)\n",
        "      alpha = 0.05\n",
        "      lbs_groups = np.zeros((n_test,))\n",
        "      ubs_groups = np.zeros((n_test,))\n",
        "      i = 0\n",
        "      for x_t in tqdm(x_test):\n",
        "          res = cond_conf.predict(1-alpha, x_t, score_inv_fn)\n",
        "          lbs_groups[i] = res[0]\n",
        "          ubs_groups[i] = res[1]\n",
        "        i += 1\n",
        "      \"\"\"\n",
        "\n",
        "      def get_marginalcoverage_condi(ubs_1,lbs_1):\n",
        "        j=0\n",
        "        for i in range(len(x_test)):\n",
        "          if((y_test[i]<=ubs_1[i]) & (y_test[i]>=lbs_1[i])):\n",
        "            j+=1\n",
        "\n",
        "        return j/len(x_test)\n",
        "      def get_marginalcoverage_split():\n",
        "        j=0\n",
        "        x_hat = get_xhat(x_test)\n",
        "        for i in range(len(x_test)):\n",
        "          if((y_test[i]>=x_hat[i]-q )& (y_test[i]<= x_hat[i]+q)):\n",
        "            j+=1\n",
        "\n",
        "        return j/len(x_test)\n",
        "      def evaluate_predictions(S, X, y):\n",
        "          marg_coverage = np.mean([y[i] in S[i] for i in range(len(y))])\n",
        "          wsc_coverage = wsc_unbiased(X, y, S,delta=0.1,M=1000)\n",
        "          #print('Marginal coverage:       {:2.3%}'.format(marg_coverage))\n",
        "          #print('WS conditional coverage: {:2.3%}'.format(wsc_coverage))\n",
        "          return wsc_coverage\n",
        "      q = np.quantile(np.abs(get_xhat(x_calib) - y_calib.reshape(-1,1)),\n",
        "                      np.ceil((len(x_calib) + 1) * (0.95)) / len(x_calib),)\n",
        "      \"\"\"\n",
        "      marginal_coverage_split = get_marginalcoverage_split()\n",
        "      marginal_coverage_condi_kernel = get_marginalcoverage_condi(ubs_kernel,lbs_kernel)\n",
        "      marginal_condi.append(marginal_coverage_condi_kernel)\n",
        "      marginal_split.append(marginal_coverage_split)\n",
        "      ubs_condi_kernel.append(ubs_kernel)\n",
        "      lbs_condi_kernel.append(lbs_kernel)\n",
        "      ubs_split.append(get_xhat(x_test)+q)\n",
        "      lbs_split.append(get_xhat(x_test)-q)\n",
        "      \"\"\"\n",
        "      #Check the marginal and WS conditional coverage\n",
        "      ubs = (get_xhat(x_test)+q).squeeze()\n",
        "      lbs =(get_xhat(x_test)-q).squeeze()\n",
        "      test_indices = test_mask\n",
        "      intervals = [pd.Interval(left=lb, right=ub, closed='right') for lb, ub in zip(lbs, ubs)]\n",
        "      print(f\"The empirical marginal and WS conditional coverage using split conformal is: \")\n",
        "      print(evaluate_predictions(intervals,x[test_indices].detach().cpu().numpy(),y[test_indices].detach().cpu().numpy()))\n",
        "      marg_coverage = np.mean([y[test_indices].detach().cpu().numpy()[i] in intervals[i] for i in range(len(y[test_indices].detach().cpu().numpy()))])\n",
        "      wsc_coverage = evaluate_predictions(intervals,x[test_indices].detach().cpu().numpy(),y[test_indices].detach().cpu().numpy())\n",
        "      split_wsc_condition.append(evaluate_predictions(intervals,x[test_indices].detach().cpu().numpy(),y[test_indices].detach().cpu().numpy()))\n",
        "      split_length.append(2*q)\n",
        "      current_test_features = x[test_mask].detach().cpu().numpy()\n",
        "      coverage_flag = [y[test_indices].detach().cpu().numpy()[i] in intervals[i] for i in range(len(y[test_indices].detach().cpu().numpy()))]\n",
        "\n",
        "      dataset_results[dataset].get('method_split_conformal')[f'Iteration_{c}'] = {\n",
        "          'Marginal_Coverage': marg_coverage,\n",
        "          'Conditional_Coverage': wsc_coverage,\n",
        "          'test_mask':test_mask,\n",
        "          'coverage_flag':coverage_flag,\n",
        "          'Average length':2*q,\n",
        "          'ubs':ubs,\n",
        "          'lbs':lbs}\n",
        "\n",
        "      ubs = ubs_kernel\n",
        "      lbs = lbs_kernel\n",
        "      test_indices = test_mask\n",
        "      intervals = [pd.Interval(left=lb, right=ub, closed='right') for lb, ub in zip(lbs, ubs)]\n",
        "      #print(f\"The empirical marginal and WS conditional coverage using kernel density is: \")\n",
        "      print(marginal_coverage)\n",
        "      print(evaluate_predictions(intervals,x[test_indices].detach().cpu().numpy(),y[test_indices].detach().cpu().numpy()))\n",
        "      #condition_calibrated_marginal.append(get_marginalcoverage_condi(ubs,lbs))\n",
        "      #condition_calibrated_wsc_condition.append(evaluate_predictions(intervals,x[test_indices].detach().cpu().numpy(),y[test_indices].detach().cpu().numpy()))\n",
        "      #condition_calibrated_length.append(ubs-lbs)\n",
        "      marginal_coverage = np.mean([y[test_indices].detach().cpu().numpy()[i] in intervals[i] for i in range(len(y[test_indices].detach().cpu().numpy()))])\n",
        "      wsc_coverage = evaluate_predictions(intervals,x[test_indices].detach().cpu().numpy(),y[test_indices].detach().cpu().numpy())\n",
        "      coverage_flag = [y[test_indices].detach().cpu().numpy()[i] in intervals[i] for i in range(len(y[test_indices].detach().cpu().numpy()))]\n",
        "      average_length = np.mean(ubs-lbs)\n",
        "      #print(wsc_coverage)\n",
        "      #print(average_length)\n",
        "      dataset_results[dataset].get('method_condition_calibrated')[f'Iteration_{c}'] = {\n",
        "          'Marginal_Coverage': marginal_coverage ,\n",
        "          'Conditional_Coverage': wsc_coverage,\n",
        "          'test_mask':test_mask,\n",
        "          'coverage_flag':coverage_flag,\n",
        "          'Average length':average_length,\n",
        "          'ubs':ubs,\n",
        "          'lbs':lbs}\n",
        "      #print(c)\n",
        "import pickle\n",
        "file_path = '/content/drive/MyDrive/Regression_SplitwithCondi_experiment_results.txt'\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump(dataset_results, file)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run their regression model and collect the results"
      ],
      "metadata": {
        "id": "Z-BzngmJ-llt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score_fn(x_calib, y_calib).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83f722CIQbSz",
        "outputId": "758665be-815b-42d1-dfe9-dcd5f28961cf"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(275, 275)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phi_kernel_estimation(x_calib).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6utOZwBQlP_",
        "outputId": "49404d09-7d86-42e1-b4f5-8fa5e3c2059b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(275, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_D7v0UiJfx2K"
      },
      "outputs": [],
      "source": [
        "datasets = ['Anaheim','ChicagoSketch','county_election_2016','county_education_2012','county_income_2012','county_unemployment_2012','twitch_PTBR']\n",
        "dataset_results = {dataset: {} for dataset in datasets}\n",
        "for dataset_name in datasets:\n",
        "      marginal_coverage_all = []\n",
        "      conditional_coverage_all = []\n",
        "      for z in range(50):\n",
        "          !python train.py --model GraphSAGE \\\n",
        "                      --dataset {dataset_name} \\\n",
        "                      --device cuda \\\n",
        "                      --alpha 0.05 \\\n",
        "                      --num_runs 1 \\\n",
        "                      --conf_correct_model QR \\\n",
        "                      --data_seed {z+100}\n",
        "\n",
        "          import pickle\n",
        "          file_path = '/content/conformalized-gnn-master/data_splits.pkl'\n",
        "          with open(file_path, 'rb') as file:\n",
        "            data_splits = pickle.load(file)\n",
        "          file_path = f'/content/conformalized-gnn-master/pred/{dataset_name}_GraphSAGE_QR_alpha_0.05.pkl'\n",
        "          with open(file_path, 'rb') as file:\n",
        "            results1 = pickle.load(file)\n",
        "          file_path = '/content/conformalized-gnn-master/labelsandfeatures.pkl'\n",
        "          with open(file_path, 'rb') as file:\n",
        "            data_labels = pickle.load(file)\n",
        "\n",
        "          #print(data_labels.get('labels')[~results1[0].get('conf_gnn').get('Raw')[4][0]])\n",
        "          current_data_labels = data_labels.get('labels')\n",
        "          current_data_features = data_labels.get('features')\n",
        "\n",
        "          current_calib_test_mask = data_labels.get('calib_test_mask')\n",
        "          #print(current_calib_test_mask.shape)\n",
        "          current_test_mask_outof_calibtest = ~current_calib_test_mask\n",
        "          current_test_features = current_data_features[current_calib_test_mask][~results1[0].get('gnn').get('CQR')[4][0]]\n",
        "          current_test_labels = current_data_labels[~results1[0].get('gnn').get('CQR')[4][0]]\n",
        "          current_lbs = results1[0].get('gnn').get('CQR')[2][0][0]\n",
        "          current_ubs = results1[0].get('gnn').get('CQR')[2][0][1]\n",
        "          average_length = np.mean(current_ubs - current_lbs)\n",
        "          intervals = [pd.Interval(left=lb, right=ub, closed='right') for lb, ub in zip(current_lbs, current_ubs)]\n",
        "          marg_coverage = np.mean([current_test_labels[i] in intervals[i] for i in range(len(current_test_labels))])\n",
        "          coverage_flag = [current_test_labels[i] in intervals[i] for i in range(len(current_test_labels))]\n",
        "          #wsc_coverage = coverage.wsc_unbiased(current_test_features, current_test_labels, intervals,delta=0.05,M=5000)\n",
        "          dataset_results[dataset_name][f'Iteration_{z}'] = {\n",
        "            'Marginal_Coverage': marg_coverage,\n",
        "            'current_test_features':current_test_features,\n",
        "            'coverage_flag':coverage_flag,\n",
        "            'Average length':average_length,\n",
        "            'current_data_features':current_data_features,\n",
        "            'current_calib_test_mask':current_calib_test_mask,\n",
        "            'current_test_mask_outof_calibtest':current_test_mask_outof_calibtest,\n",
        "            'current_test_labels':current_test_labels}\n",
        "\n",
        "\n",
        "# Save the entire dictionary to a pickle file\n",
        "file_path = '/content/drive/MyDrive/Regression_GNN_Multi_results.txt'\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump(dataset_results, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run their classification model and collect the results"
      ],
      "metadata": {
        "id": "T1mUMFt4-tb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "datasets = ['Cora_ML_CF','DBLP_CF','CiteSeer_CF','PubMed_CF','Amazon-Computers','Amazon-Photo','Coauthor-CS','Coauthor-Physics']\n",
        "dataset_results = {dataset: {} for dataset in datasets}\n",
        "for dataset_name in datasets:\n",
        "      marginal_coverage_all = []\n",
        "      conditional_coverage_all = []\n",
        "      for z in range(20):\n",
        "          !python train.py --model GraphSAGE \\\n",
        "                      --dataset {dataset_name} \\\n",
        "                      --device cpu \\\n",
        "                      --alpha 0.05 \\\n",
        "                      --num_runs 1 \\\n",
        "                      --conf_correct_model Calibrate \\\n",
        "                      --calibrator TS \\\n",
        "                      --data_seed {z+200}\n",
        "\n",
        "          import pickle\n",
        "          file_path = '/content/conformalized-gnn-master/data_splits.pkl'\n",
        "          with open(file_path, 'rb') as file:\n",
        "            data_splits = pickle.load(file)\n",
        "          file_path = f'/content/conformalized-gnn-master/pred/{dataset_name}_GraphSAGE_TS_alpha_0.05.pkl'\n",
        "          with open(file_path, 'rb') as file:\n",
        "            results1 = pickle.load(file)\n",
        "          file_path = '/content/conformalized-gnn-master/labelsandfeatures.pkl'\n",
        "          with open(file_path, 'rb') as file:\n",
        "            data_labels = pickle.load(file)\n",
        "          import numpy as np\n",
        "          #print(data_labels.get('labels')[~results1[0].get('conf_gnn').get('Raw')[4][0]])\n",
        "          current_data_labels = data_labels.get('labels')\n",
        "          current_data_features = data_labels.get('features')\n",
        "          current_calib_test_mask = data_labels.get('calib_test_mask')\n",
        "          #print(current_calib_test_mask.shape)\n",
        "          current_test_mask_outof_calibtest = ~current_calib_test_mask\n",
        "          current_test_features = current_data_features[current_calib_test_mask][~results1[0].get('gnn').get('APS')[4][0]]\n",
        "          current_test_labels = current_data_labels[~results1[0].get('gnn').get('APS')[4][0]]\n",
        "          current_predictions = results1[0].get('gnn').get('APS')[2][0]\n",
        "          true_indices_per_row = [list(np.where(row)[0]) for row in current_predictions]\n",
        "          avg_interval_length = np.mean([len(row) for row in true_indices_per_row])\n",
        "          marg_coverage = np.mean([current_test_labels[i] in true_indices_per_row[i] for i in range(len(current_test_labels))])\n",
        "          coverage_flag = [current_test_labels[i] in true_indices_per_row[i] for i in range(len(current_test_labels))]\n",
        "          #print([current_test_labels[i] in true_indices_per_row[i] for i in range(len(current_test_labels))])\n",
        "          wsc_coverage = coverage.wsc_unbiased(current_test_features, current_test_labels, true_indices_per_row,delta=0.1,M=1000)\n",
        "          dataset_results[dataset_name][f'Iteration_{z}'] = {\n",
        "            'Marginal_Coverage': marg_coverage,\n",
        "            'Conditional_Coverage':wsc_coverage,\n",
        "            'Average_Interval_Length': avg_interval_length,\n",
        "            'current_test_features':current_test_features,\n",
        "            'coverage_flag':coverage_flag,\n",
        "            'current_data_features':current_data_features,\n",
        "            'current_calib_test_mask':current_calib_test_mask,\n",
        "            'current_test_mask_outof_calibtest':current_test_mask_outof_calibtest}\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Classification_GNN_results.txt'\n",
        "import pickle\n",
        "# Save the final results to a file\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump(dataset_results, file)\n",
        "\n"
      ],
      "metadata": {
        "id": "dBIg8tocWo8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/Regression_CFGNN_results.txt'\n",
        "with open(file_path, 'rb') as file:\n",
        "    loaded_results = pickle.load(file)\n",
        "\n",
        "for dataset_name in dataset_results:\n",
        "    marginal_coverage_sum = 0\n",
        "    conditional_coverage_sum = 0\n",
        "    count_valid_conditional = 0\n",
        "\n",
        "    for i in range(30):\n",
        "        iteration_key = f'Iteration_{i}'\n",
        "        marginal_coverage_sum += dataset_results[dataset_name][iteration_key]['Marginal_Coverage']\n",
        "\n",
        "        conditional_coverage = dataset_results[dataset_name][iteration_key]['Conditional_Coverage']\n",
        "        if not np.isnan(conditional_coverage):  # Check if conditional coverage is not nan\n",
        "            conditional_coverage_sum += conditional_coverage\n",
        "            count_valid_conditional += 1\n",
        "\n",
        "    avg_marginal_coverage = marginal_coverage_sum / 30\n",
        "    avg_conditional_coverage = conditional_coverage_sum / count_valid_conditional if count_valid_conditional > 0 else 'nan'\n",
        "\n",
        "    print(f'{dataset_name}:')\n",
        "    print(f'  Average Marginal Coverage: {avg_marginal_coverage}')\n",
        "    print(f'  Average Conditional Coverage: {avg_conditional_coverage}\\n')"
      ],
      "metadata": {
        "id": "O9YiCkSeFYw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "possible plot to see coverage"
      ],
      "metadata": {
        "id": "LHH1eCcl_Fg9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_vLnnJAex58"
      },
      "outputs": [],
      "source": [
        "#Plot the first result of the dimension of interest of covariates\n",
        "x_test = x[test_mask_lists[0]]\n",
        "y_test = y[test_mask_lists[0]]\n",
        "ubs = ubs_condi_kernel[4]\n",
        "lbs = lbs_condi_kernel[4]\n",
        "dimension_interest = 1\n",
        "\n",
        "cp = sns.color_palette()\n",
        "sns.set(font=\"DejaVu Sans\")\n",
        "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
        "fig = plt.figure()\n",
        "fig.set_size_inches(17.5, 6)\n",
        "sort_order = np.argsort(x_test[0:n_test,dimension_interest])\n",
        "x_test_s = x_test[0:n_test,dimension_interest][sort_order]\n",
        "y_test_s = y_test[sort_order]\n",
        "y_test_hat = get_xhat(x_test[sort_order])\n",
        "lb = lbs[sort_order]\n",
        "ub = ubs[sort_order]\n",
        "\n",
        "ax1 = fig.add_subplot(1, 3, 1)\n",
        "ax1.plot(x_test_s, y_test_s, '.', alpha=0.2)\n",
        "ax1.plot(x_test_s, y_test_hat, lw=1, color='k')\n",
        "ax1.plot(x_test_s, y_test_hat + q, color=cp[0], lw=2)\n",
        "ax1.plot(x_test_s, y_test_hat - q, color=cp[0], lw=2)\n",
        "\n",
        "ax1.set_ylim(-5,10)\n",
        "ax1.tick_params(axis='both', which='major', labelsize=14)\n",
        "ax1.set_xlabel(\"$X$\", fontsize=16, labelpad=10)\n",
        "ax1.set_ylabel(\"$Y$\", fontsize=16, labelpad=10)\n",
        "ax1.set_title(\"Split Conformal\", fontsize=18, pad=12)\n",
        "ax1.fill_between(x_test_s.flatten(), y_test_hat.flatten() - q, y_test_hat.flatten() + q,\n",
        "                 color=cp[3], alpha=0.4, label='split prediction interval')\n",
        "\n",
        "\n",
        "ax2 = fig.add_subplot(1, 3, 2, sharex = ax1, sharey = ax1)\n",
        "ax2.plot(x_test_s, y_test_s, '.', alpha=0.2)\n",
        "ax2.plot(x_test_s, y_test_hat, color='k', lw=1)\n",
        "ax2.plot(x_test_s, ub, color=cp[1], lw=2)\n",
        "ax2.plot(x_test_s, lb, color=cp[1], lw=2)\n",
        "ax2.fill_between(x_test_s.flatten(), lb,\n",
        "                 ub,\n",
        "                 color=cp[3], alpha=0.4, label='conditional calibration')\n",
        "ax2.tick_params(axis='both', which='major', direction='out', labelsize=14)\n",
        "ax2.set_xlabel(\"$X$\", fontsize=16, labelpad=10)\n",
        "ax2.set_ylabel(\"$Y$\", fontsize=16, labelpad=10)\n",
        "ax2.set_title(\"Conditional Calibration\", fontsize=18, pad=12)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1CTFhngUMO_SD_uyLw2orgHrOtUVwZN_z",
      "authorship_tag": "ABX9TyOf0W4yBo7nHSxxoTq2m9yh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}