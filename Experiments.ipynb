{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM93nwQKLMyD+L8bAFXlj/J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caltdreamer/GNN/blob/main/Experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install -q torch-geometric\n",
        "!pip install umap-learn\n",
        "!git clone https://github.com/msesia/arc.git\n",
        "!pip install conditionalconformal\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2qXkTICMVWuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/conformalized-gnn-master/requirements.txt\n",
        "%cd /content/conformalized-gnn-master\n",
        "!mkdir -p /content/conformalized-gnn-master/model"
      ],
      "metadata": {
        "id": "lbMg_WLnVaoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "def wsc(X, y, S, delta=0.1, M=1000, random_state=2020, verbose=False):\n",
        "    rng = np.random.default_rng(random_state)\n",
        "\n",
        "    def wsc_v(X, y, S, delta, v):\n",
        "        #print(X.shape)\n",
        "        #print(y.shape)\n",
        "        #print(len(S))\n",
        "        n = len(y)\n",
        "        cover = np.array([y[i] in S[i] for i in range(n)])\n",
        "        #print(f'length is {n}')\n",
        "        #print(n)\n",
        "        #print(f'coverage is {cover.sum()}')\n",
        "        z = np.dot(X,v)\n",
        "        # Compute mass\n",
        "        z_order = np.argsort(z)\n",
        "        z_sorted = z[z_order]\n",
        "        cover_ordered = cover[z_order]\n",
        "        ai_max = int(np.round((1.0-delta)*n))\n",
        "        ai_best = 0\n",
        "        bi_best = n-1\n",
        "        cover_min = 1\n",
        "        for ai in np.arange(0, ai_max):\n",
        "            bi_min = np.minimum(ai+int(np.round(delta*n)),n)\n",
        "            coverage = np.cumsum(cover_ordered[ai:n]) / np.arange(1,n-ai+1)\n",
        "            coverage[np.arange(0,bi_min-ai)]=1\n",
        "            bi_star = ai+np.argmin(coverage)\n",
        "            cover_star = coverage[bi_star-ai]\n",
        "            if cover_star < cover_min:\n",
        "                ai_best = ai\n",
        "                bi_best = bi_star\n",
        "                cover_min = cover_star\n",
        "        #print(cover_min)\n",
        "        #print(ai_best)\n",
        "        #print(bi_best)\n",
        "        return cover_min, z_sorted[ai_best], z_sorted[bi_best]\n",
        "\n",
        "    def sample_sphere(n, p):\n",
        "        v = rng.normal(size=(p, n))\n",
        "        v /= np.linalg.norm(v, axis=0)\n",
        "        return v.T\n",
        "    V = sample_sphere(M, p=X.shape[1])\n",
        "\n",
        "    wsc_list = [[]] * M\n",
        "    a_list = [[]] * M\n",
        "    b_list = [[]] * M\n",
        "    #print('wqwqqw')\n",
        "    if verbose:\n",
        "        for m in tqdm(range(M)):\n",
        "            wsc_list[m], a_list[m], b_list[m] = wsc_v(X, y, S, delta, V[m])\n",
        "    else:\n",
        "        #print('iehkwd')\n",
        "        for m in range(M):\n",
        "            #print(m)\n",
        "            #print(X.shape)\n",
        "            #print(y.shape)\n",
        "            wsc_list[m], a_list[m], b_list[m] = wsc_v(X, y, S, delta, V[m])\n",
        "            #print(m)\n",
        "    #print('ikfrk')\n",
        "    idx_star = np.argmin(np.array(wsc_list))\n",
        "    a_star = a_list[idx_star]\n",
        "    b_star = b_list[idx_star]\n",
        "    v_star = V[idx_star]\n",
        "    wsc_star = wsc_list[idx_star]\n",
        "    return wsc_star, v_star, a_star, b_star\n",
        "\n",
        "def wsc_unbiased(X, y, S, delta=0.1, M=1000, test_size=0.75, random_state=2020, verbose=False):\n",
        "    #print('iqhd')\n",
        "    def wsc_vab(X, y, S, v, a, b):\n",
        "        n = len(y)\n",
        "        cover = np.array([y[i] in S[i] for i in range(n)])\n",
        "        z = np.dot(X,v)\n",
        "        idx = np.where((z>=a)*(z<=b))\n",
        "        coverage = np.mean(cover[idx])\n",
        "        return coverage\n",
        "    #print('skjdh')\n",
        "\n",
        "    max_attempts = 5000\n",
        "    for attempt in range(max_attempts):\n",
        "        X_train, X_test, y_train, y_test, S_train, S_test = train_test_split(X, y, S, test_size=0.75, random_state=attempt)\n",
        "        cover = np.array([y_train[i] in S_train[i] for i in range(len(y_train))])\n",
        "        if not all(cover):\n",
        "            break\n",
        "\n",
        "    if all(cover):\n",
        "        print('May cause problem')\n",
        "    #print(len(y_train))\n",
        "    #print(cover)\n",
        "    #print('dasytrdv')\n",
        "    #print(X_train.shape)\n",
        "    #print(X_test.shape)\n",
        "    ##print(y_train.shape)\n",
        "    #print(y_test.shape)\n",
        "    #print(len(S_train))\n",
        "    #print(len(S_test))\n",
        "    #print(S_train)\n",
        "    # Find adversarial parameters\n",
        "    wsc_star, v_star, a_star, b_star = wsc(X_train, y_train, S_train, delta=delta, M=M, random_state=random_state, verbose=verbose)\n",
        "    #print('ewtddvzdas')\n",
        "    #print(v_star)\n",
        "    #print(a_star)\n",
        "    #print(b_star)\n",
        "    # Estimate coverage\n",
        "    coverage = wsc_vab(X_test, y_test, S_test, v_star, a_star, b_star)\n",
        "    return coverage\n"
      ],
      "metadata": {
        "id": "vpv9EI1xVcqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHc0pY-MVVYt"
      },
      "outputs": [],
      "source": [
        "datasets = ['Anaheim','ChicagoSketch','county_election_2016','county_education_2012','county_income_2012','county_unemployment_2012']\n",
        "dataset_results = {dataset: {} for dataset in datasets}\n",
        "for dataset_name in datasets:\n",
        "      marginal_coverage_all = []\n",
        "      conditional_coverage_all = []\n",
        "      for z in range(10):\n",
        "          !python train.py --model GraphSAGE \\\n",
        "                      --dataset {dataset_name} \\\n",
        "                      --device cpu \\\n",
        "                      --alpha 0.1 \\\n",
        "                      --num_runs 1 \\\n",
        "                      --conf_correct_model QR \\\n",
        "                      --data_seed {z+500}\n",
        "          import pickle\n",
        "          file_path = '/content/conformalized-gnn-master/data_splits.pkl'\n",
        "          with open(file_path, 'rb') as file:\n",
        "            data_splits = pickle.load(file)\n",
        "          file_path = f'/content/conformalized-gnn-master/pred/{dataset_name}_GraphSAGE_QR.pkl'\n",
        "          with open(file_path, 'rb') as file:\n",
        "            results1 = pickle.load(file)\n",
        "          file_path = '/content/conformalized-gnn-master/labelsandfeatures.pkl'\n",
        "          with open(file_path, 'rb') as file:\n",
        "            data_labels = pickle.load(file)\n",
        "\n",
        "          #print(data_labels.get('labels')[~results1[0].get('conf_gnn').get('Raw')[4][0]])\n",
        "          current_data_labels = data_labels.get('labels')\n",
        "          current_data_features = data_labels.get('features')\n",
        "          current_calib_test_mask = data_labels.get('calib_test_mask')\n",
        "          #print(current_calib_test_mask.shape)\n",
        "          current_test_mask_outof_calibtest = ~current_calib_test_mask\n",
        "          current_test_features = current_data_features[current_calib_test_mask][~results1[0].get('conf_gnn').get('Raw')[4][0]]\n",
        "          current_test_labels = current_data_labels[~results1[0].get('conf_gnn').get('Raw')[4][0]]\n",
        "          current_lbs = results1[0].get('conf_gnn').get('Raw')[2][0][0]\n",
        "          current_ubs = results1[0].get('conf_gnn').get('Raw')[2][0][1]\n",
        "\n",
        "          average_length = np.mean(current_ubs - current_lbs)\n",
        "          intervals = [pd.Interval(left=lb, right=ub, closed='right') for lb, ub in zip(current_lbs, current_ubs)]\n",
        "          marg_coverage = np.mean([current_test_labels[i] in intervals[i] for i in range(len(current_test_labels))])\n",
        "          print(marg_coverage)\n",
        "          coverage_flag = [current_test_labels[i] in intervals[i] for i in range(len(current_test_labels))]\n",
        "          wsc_coverage = wsc_unbiased(current_test_features, current_test_labels, intervals,delta=0.1,M=1000)\n",
        "          print(wsc_coverage)\n",
        "          dataset_results[dataset_name][f'Iteration_{z}'] = {\n",
        "            'Marginal_Coverage': marg_coverage,\n",
        "            'current_test_features':current_test_features,\n",
        "            'coverage_flag':coverage_flag,\n",
        "            'Average length':average_length,\n",
        "            'current_data_features':current_data_features,\n",
        "            'current_calib_test_mask':current_calib_test_mask,\n",
        "            'current_test_mask_outof_calibtest':current_test_mask_outof_calibtest,\n",
        "            'current_test_labels':current_test_labels}\n",
        "          dataset_results_marginal[dataset_name].append(marg_coverage)\n",
        "          dataset_results_conditional[dataset_name].append(wsc_coverage)\n",
        "\n",
        "\n",
        "# Save the entire dictionary to a pickle file\n",
        "file_path = '/content/drive/MyDrive/Regression_CFGNN_results.txt'\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump(dataset_results, file)"
      ]
    }
  ]
}