# -*- coding: utf-8 -*-
"""functions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LJBoayFSOsPNPiT2-WGRAEfZdqwHMA7H
"""

import torch
import torch.nn.functional as F
from torch_geometric.nn import GraphSAGE
from torch_geometric.data import NeighborSampler
from torch_geometric.nn import SAGEConv
from torch_geometric.datasets import Planetoid
from torch_geometric.datasets import NELL
from torch_geometric.datasets import CitationFull
from torch_geometric.datasets import CoraFull
from torch_geometric.data import DataLoader
from torch_geometric.data import Data
from tqdm import tqdm
from torch_geometric.nn import SAGEConv, BatchNorm
import torch.optim as optim
import numpy as np
import os.path as osp
import pandas as pd
import numpy as np
import collections
import random
import networkx as nx
import seaborn as sns
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import math
from numpy.ma.core import maximum
from conditionalconformal.synthetic_data import generate_cqr_data, indicator_matrix
from conditionalconformal import CondConf
import numpy as np
from sklearn.model_selection import train_test_split
from tqdm import tqdm
from scipy.stats import norm
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
from scipy.stats import gaussian_kde
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn.functional as F
import torch.nn as nn
import copy
import numpy as np
from torch_geometric.nn import GCNConv, GATConv, SAGEConv, SGConv
import numpy as np
from tqdm import tqdm
from conditionalconformal import CondConf



def run_condition(cal_features, cal_labels, test_features, test_labels, prediction_cal, prediction_test, data_num_features, data_num_classes, alpha):
        emb3_Cali = prediction_cal
        cal_smx = emb3_Cali
        cal_pi = cal_smx.argsort(1)[:, ::-1]
        cal_srt = np.take_along_axis(cal_smx,cal_pi,axis=1).cumsum(axis=1)

        test_smx = prediction_test
        test_pi = test_smx.argsort(1)[:, ::-1]
        test_srt = np.take_along_axis(test_smx,test_pi,axis=1).cumsum(axis=1)
        num_features = data_num_features
        n_calib = len(prediction_cal)
        n_test = len(prediction_test)

        num_classes = data_num_classes
        matrix_1 = np.zeros((n_calib+n_test, num_classes),dtype=int)

        for i in range(data_num_classes):
            matrix_1[:, i] = i

        caliandtest_srt = np.vstack((cal_srt, test_srt))
        caliandtest_pi = np.vstack((cal_pi,test_pi))
        test_scores_foreachclass=np.zeros((n_calib+n_test,num_classes))

        for i in range(num_classes):
            test_scores = (np.take_along_axis(caliandtest_srt , caliandtest_pi.argsort(axis=1), axis=1)[
            range(n_calib+n_test), matrix_1[:,i]])
            test_scores_foreachclass[:,i]=test_scores
        x_caliandtest = np.vstack((cal_smx,test_smx))
        x_calib = cal_smx
        y_calib = cal_labels.squeeze()
        x_test = test_smx
        y_test = test_labels.squeeze()

        def score_fn(x_1,y):
          
              scores = np.ones((x_1.shape[0],1))
              for i in range(x_1.shape[0]):
                scores[i]=test_scores_foreachclass[i,y[i]]
              return scores.squeeze()
        def phi_n(x_2):
            if len(x_2.shape) == 2:
                one_column = np.ones((x_2.shape[0],1))
                return np.hstack((x_2,one_column))
            else:
                return np.append(x_2,1)
        def phi_n(x_2):
            return x_2     

        def score_inv_fn(s,x_3):
              return np.take_along_axis(caliandtest_srt[p].reshape(1,-1) <= s,caliandtest_pi[p].reshape(1,-1).argsort(axis=1),axis=1)
        infinite_params = {}
        cond_conf = CondConf(score_fn=score_fn,Phi_fn=phi_n,quantile_fn=None)
        cond_conf.setup_problem(x_calib, y_calib)
        prediction_sets_test=[]
        p=n_calib
        for x_t in tqdm(x_test):
            res = cond_conf.predict(1-alpha, x_t, score_inv_fn)
            prediction_sets_test.append(res)
            p = p+1

        return np.vstack(prediction_sets_test)

def wsc(X, y, S, delta=0.1, M=1000, random_state=2020, verbose=False):
    rng = np.random.default_rng(random_state)

    def wsc_v(X, y, S, delta, v):
        n = len(y)
        cover = np.array([y[i] in S[i] for i in range(n)])
        z = np.dot(X,v)
        # Compute mass
        z_order = np.argsort(z)
        z_sorted = z[z_order]
        cover_ordered = cover[z_order]
        ai_max = int(np.round((1.0-delta)*n))
        ai_best = 0
        bi_best = n-1
        cover_min = 1
        for ai in np.arange(0, ai_max):
            bi_min = np.minimum(ai+int(np.round(delta*n)),n)
            coverage = np.cumsum(cover_ordered[ai:n]) / np.arange(1,n-ai+1)
            coverage[np.arange(0,bi_min-ai)]=1
            bi_star = ai+np.argmin(coverage)
            cover_star = coverage[bi_star-ai]
            if cover_star < cover_min:
                ai_best = ai
                bi_best = bi_star
                cover_min = cover_star
        return cover_min, z_sorted[ai_best], z_sorted[bi_best]

    def sample_sphere(n, p):
        v = rng.normal(size=(p, n))
        v /= np.linalg.norm(v, axis=0)
        return v.T
    V = sample_sphere(M, p=X.shape[1])

    wsc_list = [[]] * M
    a_list = [[]] * M
    b_list = [[]] * M
    if verbose:
        for m in tqdm(range(M)):
            wsc_list[m], a_list[m], b_list[m] = wsc_v(X, y, S, delta, V[m])
    else:
        for m in range(M):
            wsc_list[m], a_list[m], b_list[m] = wsc_v(X, y, S, delta, V[m])

    idx_star = np.argmin(np.array(wsc_list))
    a_star = a_list[idx_star]
    b_star = b_list[idx_star]
    v_star = V[idx_star]
    wsc_star = wsc_list[idx_star]
    return wsc_star, v_star, a_star, b_star

def wsc_unbiased(X, y, S, delta=0.1, M=1000, test_size=0.75, random_state=2020, verbose=False):
    def wsc_vab(X, y, S, v, a, b):
        n = len(y)
        cover = np.array([y[i] in S[i] for i in range(n)])
        z = np.dot(X,v)
        idx = np.where((z>=a)*(z<=b))
        coverage = np.mean(cover[idx])
        return coverage

    max_attempts = 5000
    for attempt in range(max_attempts):
        X_train, X_test, y_train, y_test, S_train, S_test = train_test_split(X, y, S, test_size=0.75, random_state=attempt)
        cover = np.array([y_train[i] in S_train[i] for i in range(len(y_train))])
        if not all(cover):
            break

    if all(cover):
        print('May cause problem')

    # Find adversarial parameters
    wsc_star, v_star, a_star, b_star = wsc(X_train, y_train, S_train, delta=delta, M=M, random_state=random_state, verbose=verbose)

    # Estimate coverage
    coverage = wsc_vab(X_test, y_test, S_test, v_star, a_star, b_star)
    return coverage

def evaluate_predictions(S, X, y):
    marg_coverage = np.mean([y[i] in S[i] for i in range(len(y))])
    wsc_coverage = wsc_unbiased(X, y, S)
    return marg_coverage,wsc_coverage
def indices_of_nonzero_entries(binary_matrix):
    # Using list comprehension to find the indices of non-zero entries for each row
    indices_matrix = [np.flatnonzero(row) for row in binary_matrix]
    return indices_matrix
