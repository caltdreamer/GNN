# -*- coding: utf-8 -*-
"""Main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kLAgPrS3tDBYSAa37FO2ZT6DKXmd4O-j
"""

# Main.py
import argparse
import os.path as osp
import random
import math
import collections
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import LinearRegression
from scipy.stats import norm, gaussian_kde
import networkx as nx

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch_geometric.nn import GraphSAGE, SAGEConv, BatchNorm, GCNConv, GATConv, SGConv
from torch_geometric.data import Data, DataLoader, NeighborSampler
from torch_geometric.datasets import Planetoid, NELL, CitationFull, CoraFull
from tqdm import tqdm
from conditionalconformal import CondConf
from conditionalconformal.synthetic_data import generate_cqr_data, indicator_matrix
from functions import run_condition, wsc_unbiased, evaluate_predictions, indices_of_nonzero_entries
import pickle
import argparse
import os
import subprocess
import pickle
def main(args):
    datasets = [args.dataset]
    alphas = [args.alpha]
    for alpha in alphas:
        for dataset_name in datasets:
          dataset_results = {dataset: {'GNN': {}} for dataset in datasets}
          
          for z in range(args.num_runs):
              subprocess.run(['python', 'train.py',
                                '--model', args.model,
                                '--dataset', dataset_name,
                                '--device', args.device,
                                '--alpha', str(alpha),
                                '--num_runs', str(1),
                                '--optimal',
                                '--data_seed', str(z + args.seed),
                                '--epochs', str(args.epochs)])
              """
              !python train.py --model args.model \
                  --dataset {dataset_name} \
                  --device args.device \
                  --alpha {alpha} \
                  --num_runs args.num_runs \
                  --optimal\
                  --data_seed {z+args.seed}
              """

              file_path = f'./pred/optimal_{dataset_name}_{args.model}_conftr_calib_holdout_confgnn_alpha_{alpha}.pkl'
              with open(file_path, 'rb') as file:
                  results1 = pickle.load(file)

              file_path = './data_splits.pkl'
              with open(file_path, 'rb') as file:
                data_splits = pickle.load(file)

              file_path = './GNN_prediction.pkl'
              with open(file_path, 'rb') as file:
                data_prediction = pickle.load(file)

              file_path = './run_condition_variables_CFGNN_2.pkl'
              with open(file_path, 'rb') as file:
                  variables_retrieved = pickle.load(file)

              cal_features = variables_retrieved['cal_features']
              cal_labels = variables_retrieved['cal_labels']
              test_features = variables_retrieved['test_features']
              test_labels = variables_retrieved['test_labels']
              prediction = variables_retrieved['prediction']
              prediction_cal = prediction[variables_retrieved['idx']]
              prediction_test = prediction[~variables_retrieved['idx']]
              prediction_cc= run_condition(cal_features,cal_labels,test_features,test_labels,prediction_cal,prediction_test,variables_retrieved['num_features'],prediction.shape[1],alpha)
              int_matrix =prediction_cc.astype(int)
              indices_matrix = indices_of_nonzero_entries(int_matrix)
              marg_coverage_cc,wsc_coverage_cc = evaluate_predictions(indices_matrix,test_features,test_labels)
              length_cc = np.sum(int_matrix)/int_matrix.shape[0]
              coverage_indicator_cc = [test_labels[i] in indices_matrix[i] for i in range(len(test_labels))]
              categories = np.unique(test_labels)
              coverage_counts_for_each_category_cc = {}
              total_counts_for_each_category_cc = {}
              for category in categories:
                category_indices = np.where(test_labels == category)[0]

                category_coverage = np.array(coverage_indicator_cc)[category_indices]

                covered_count = np.sum(category_coverage)
                total_count = len(category_coverage)

                coverage_counts_for_each_category_cc[category] = covered_count
                total_counts_for_each_category_cc[category] = total_count
                coverage_for_each_category_cc = {}
              for category in categories:
                  category_indices = np.where(test_labels == category)[0]

                  category_coverage = np.array(coverage_indicator_cc)[category_indices]

              marg_coverage = results1[0].get('conf_gnn').get('APS')[0][0]
              prediction_sets_CF = results1[0].get('conf_gnn').get('APS')[2]
              indices_matrix_CF = indices_of_nonzero_entries(prediction_sets_CF.astype(int))

              _,wsc_coverage_CF = evaluate_predictions(indices_matrix_CF,results1[0].get('conf_gnn').get('APS')[4],results1[0].get('conf_gnn').get('APS')[3])
              length_CF = np.sum(prediction_sets_CF.astype(int))/prediction_sets_CF.astype(int).shape[0]
              coverage_indicator_CF = [results1[0].get('conf_gnn').get('APS')[3][i] in  indices_matrix_CF[i] for i in range(len(results1[0].get('conf_gnn').get('APS')[3]))]
              categories = np.unique(results1[0].get('conf_gnn').get('APS')[3])
              categories = np.unique(results1[0].get('conf_gnn').get('APS')[3])

              coverage_counts_for_each_category_CF = {}
              total_counts_for_each_category_CF= {}

              for category in categories:
                  category_indices = np.where(results1[0].get('conf_gnn').get('APS')[3] == category)[0]

                  category_coverage = np.array(coverage_indicator_CF)[category_indices]

                  covered_count = np.sum(category_coverage)
                  total_count = len(category_coverage)

                  coverage_counts_for_each_category_CF[category] = covered_count
                  total_counts_for_each_category_CF[category] = total_count
              coverage_for_each_category = {}
              for category in categories:
                  category_indices = np.where(results1[0].get('conf_gnn').get('APS')[3] == category)[0]

                  category_coverage = np.array(coverage_indicator_CF)[category_indices]

                  coverage_for_each_category[category] = np.mean(category_coverage)

              dataset_results[dataset_name]['GNN'][f'Iteration_{z}'] = {
              'Marginal_Coverage':  marg_coverage,
              'Marginal_Coverage_cc':marg_coverage_cc,
              'current_test_features_CFGNN':results1[0].get('conf_gnn').get('APS')[4],
              'current_test_features_cc':test_features,
              'conditional_coverage':wsc_coverage_CF,
              'conditional_coverage_cc':wsc_coverage_cc,
              'Average length':length_CF,
              'Average Length_cc':length_cc,
              'coverage_flag':coverage_indicator_CF,
              'coverage_flag_cc':coverage_indicator_cc,
              'coverage_counts_CF':coverage_counts_for_each_category_CF,
              'total_counts_CF':total_counts_for_each_category_CF,
              'coverage_counts_cc':coverage_counts_for_each_category_cc,
              'total_counts_cc':total_counts_for_each_category_cc,
              'Prediction_sets_CF':prediction_sets_CF,
              'Prediction_sets_cc':prediction_cc}
              
          file_path = f'./results/Classification_{dataset_name}_{alpha}_{z}.txt'
          with open(file_path, 'wb') as file:
              pickle.dump(dataset_results, file)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train and evaluate models.")
    parser.add_argument('--dataset', type=str, required=True, help='Dataset name')
    parser.add_argument('--alpha', type=float, default=0.05, help='Alpha value for confidence interval')
    parser.add_argument('--seed', type=int, default=100, help='Random Seeds')
    parser.add_argument('--device', type=str, default='cuda')
    parser.add_argument('--num_runs', type=int, default=1, help='Numbers of runs of experiments')
    parser.add_argument('--model', type=str, default='GCN', choices = ['GAT', 'GCN', 'GraphSAGE', 'SGC'])
    parser.add_argument('--epochs', type=int, default=500, help='epochs')
    # Add any other arguments your script accepts

    args = parser.parse_args()
    main(args)
